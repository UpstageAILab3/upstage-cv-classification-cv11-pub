{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{"id":"OliaDaX_lwou"},"source":["# **ğŸ“„ Document type classification baseline code**\n","> ë¬¸ì„œ íƒ€ì… ë¶„ë¥˜ ëŒ€íšŒì— ì˜¤ì‹  ì—¬ëŸ¬ë¶„ í™˜ì˜í•©ë‹ˆë‹¤! ğŸ‰     \n","> ì•„ë˜ baselineì—ì„œëŠ” ResNet ëª¨ë¸ì„ ë¡œë“œí•˜ì—¬, ëª¨ë¸ì„ í•™ìŠµ ë° ì˜ˆì¸¡ íŒŒì¼ ìƒì„±í•˜ëŠ” í”„ë¡œì„¸ìŠ¤ì— ëŒ€í•´ ì•Œì•„ë³´ê² ìŠµë‹ˆë‹¤.\n","\n","## Contents\n","- Prepare Environments\n","- Import Library & Define Functions\n","- Hyper-parameters\n","- Load Data\n","- Train Model\n","- Inference & Save File\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import zipfile\n","import os\n","\n","# ì••ì¶• íŒŒì¼ ê²½ë¡œì™€ ì••ì¶• í•´ì œí•  ë””ë ‰í† ë¦¬ ì„¤ì •\n","zip_file_path = '/data/ephemeral/home/datasets_fin/train_wdnx4.zip'\n","extract_dir = '/data/ephemeral/home/datasets_fin/train_wdnx4'\n","\n","# ì••ì¶• íŒŒì¼ ì—´ê¸°\n","with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n","    # ëª¨ë“  íŒŒì¼ ì••ì¶• í•´ì œ\n","    zip_ref.extractall(extract_dir)\n","\n","print(f'Files extracted to {extract_dir}')\n"]},{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":9396,"status":"ok","timestamp":1700314592802,"user":{"displayName":"Ynot(ì†¡ì›í˜¸)","userId":"16271863862696372773"},"user_tz":-540},"id":"3BaoIkv5Xwa0"},"outputs":[],"source":["import os\n","import time\n","import random\n","\n","import timm\n","import torch\n","import albumentations as A\n","import pandas as pd\n","import numpy as np\n","import torch.nn as nn\n","from albumentations.pytorch import ToTensorV2\n","from torch.optim import Adam, AdamW\n","from torchvision import transforms\n","from torch.utils.data import Dataset, DataLoader\n","from PIL import Image\n","from tqdm import tqdm\n","from sklearn.metrics import accuracy_score, f1_score"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["import json\n","import wandb\n","from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay # sklearn ë‚´ confusion matrix ê³„ì‚° í•¨ìˆ˜\n","import matplotlib.pyplot as plt # ì‹œê°í™”ë¥¼ ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬\n","import torchvision.transforms as T # ì´ë¯¸ì§€ ë³€í™˜ì„ ìœ„í•œ ëª¨ë“ˆ\n","from torch.optim.lr_scheduler import ReduceLROnPlateau\n","from PIL import Image, ImageEnhance, ImageOps\n","import numpy as np\n","import cv2\n","import albumentations as A\n","from albumentations.pytorch import ToTensorV2\n","from torch.cuda.amp import autocast, GradScaler\n","from sklearn.metrics import accuracy_score, f1_score\n"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["# ì‹œë“œë¥¼ ê³ ì •í•©ë‹ˆë‹¤.\n","SEED = 42\n","os.environ['PYTHONHASHSEED'] = str(SEED)\n","random.seed(SEED)\n","np.random.seed(SEED)\n","torch.manual_seed(SEED)\n","torch.cuda.manual_seed(SEED)\n","torch.cuda.manual_seed_all(SEED)\n","torch.backends.cudnn.benchmark = True"]},{"cell_type":"code","execution_count":58,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["NanumGothic\n"]}],"source":["from matplotlib import font_manager, rc\n","import matplotlib.pyplot as plt\n","\n","font_path = '/usr/share/fonts/truetype/nanum/NanumGothic.ttf'\n","font_name = font_manager.FontProperties(fname=font_path).get_name()\n","print(font_name)\n","plt.rc('font', family=font_name)"]},{"cell_type":"markdown","metadata":{},"source":["# 0.97 submission ì œì¶œ íŒŒì¼ì„ ê°€ì§€ê³  testì…‹ ë¶„í¬ í™•ì¸: trainê³¼ ë™ì¼í•¨"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import pandas as pd\n","\n","# CSV íŒŒì¼ ë¡œë“œ\n","meta = pd.read_csv(r'/data/ephemeral/home/datasets_fin/meta.csv')\n","data = pd.read_csv(r'/data/ephemeral/home/notebook/EASY_pred_ocr_0806_9514_34714.csv')\n","merged_data = pd.merge(data, meta, on='target', how='left')\n","\n","\n","# 'target' ì»¬ëŸ¼ì˜ ê°’ì— ëŒ€í•œ ë¹ˆë„ìˆ˜ ê³„ì‚°\n","class_counts = merged_data[['target','class_name_kr']].value_counts().sort_index()\n","\n","# ë§‰ëŒ€ ê·¸ë˜í”„ë¡œ ì‹œê°í™”\n","plt.figure(figsize=(10, 6))\n","class_counts.plot(kind='bar', color='skyblue')\n","plt.title('Class Distribution in Testset Data')\n","plt.xlabel('Class Label')\n","plt.ylabel('Frequency')\n","plt.xticks(rotation=90)  # í´ë˜ìŠ¤ ë ˆì´ë¸”ì´ ìˆ˜í‰ìœ¼ë¡œ í‘œì‹œë˜ë„ë¡ ì„¤ì •\n","plt.grid(axis='y', linestyle='--', alpha=0.7)\n","plt.show()\n"]},{"cell_type":"markdown","metadata":{},"source":["# ì´ë¯¸ì§€ í¬ê¸° í™•ì¸"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import matplotlib.pyplot as plt\n","from collections import Counter\n","import os\n","from PIL import Image\n","\n","# ì´ë¯¸ì§€ í¬ê¸° ì¶”ì¶œ í•¨ìˆ˜\n","def check_image_sizes(directory):\n","    sizes = []\n","    for filename in os.listdir(directory):\n","        if filename.endswith(('.jpg', '.jpeg', '.png')):\n","            filepath = os.path.join(directory, filename)\n","            with Image.open(filepath) as img:\n","                sizes.append(img.size)\n","    return sizes\n","\n","# ì´ë¯¸ì§€ í¬ê¸°ë³„ ë¹ˆë„ ê³„ì‚°\n","directory = '/data/ephemeral/home/datasets_fin/combined_train/train'\n","sizes = check_image_sizes(directory)\n","size_counts = Counter(sizes)\n","\n","# í¬ê¸°ì™€ ë¹ˆë„ ë°ì´í„° ë¶„ë¦¬\n","sizes, counts = zip(*size_counts.items())\n","\n","# ì‹œê°í™”\n","plt.figure(figsize=(10, 6))\n","plt.barh(range(len(counts)), counts, tick_label=[f'{w}x{h}' for w, h in sizes])\n","plt.xlabel('Number of Images')\n","plt.ylabel('Image Size (width x height)')\n","plt.title('Image Size Distribution')\n","plt.show()\n","\n","\n","directory2 = '/data/ephemeral/home/datasets_fin/test'\n","sizes2 = check_image_sizes(directory2)\n","size_counts2 = Counter(sizes2)\n","\n","# í¬ê¸°ì™€ ë¹ˆë„ ë°ì´í„° ë¶„ë¦¬\n","sizes2, counts2 = zip(*size_counts2.items())\n","\n","# ì‹œê°í™”\n","plt.figure(figsize=(10, 6))\n","plt.barh(range(len(counts2)), counts2, tick_label=[f'{w}x{h}' for w, h in sizes2])\n","plt.xlabel('Number of Images')\n","plt.ylabel('Image Size (width x height)')\n","plt.title('Image Size Distribution')\n","plt.show()\n"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"Wjom43UvoXcx"},"source":["## 3. Hyper-parameters\n","* í•™ìŠµ ë° ì¶”ë¡ ì— í•„ìš”í•œ í•˜ì´í¼íŒŒë¼ë¯¸í„°ë“¤ì„ ì •ì˜í•©ë‹ˆë‹¤."]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mhelloyoonjae\u001b[0m (\u001b[33mhelloyoonjae-\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"]},{"data":{"text/html":["wandb version 0.17.6 is available!  To upgrade, please run:\n"," $ pip install wandb --upgrade"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Tracking run with wandb version 0.17.5"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Run data is saved locally in <code>/data/ephemeral/home/notebook/wandb/run-20240810_221436-sxshmb5m</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Syncing run <strong><a href='https://wandb.ai/helloyoonjae-/helloyoonjae-/runs/sxshmb5m' target=\"_blank\">fastcampus_cv11</a></strong> to <a href='https://wandb.ai/helloyoonjae-/helloyoonjae-' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View project at <a href='https://wandb.ai/helloyoonjae-/helloyoonjae-' target=\"_blank\">https://wandb.ai/helloyoonjae-/helloyoonjae-</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View run at <a href='https://wandb.ai/helloyoonjae-/helloyoonjae-/runs/sxshmb5m' target=\"_blank\">https://wandb.ai/helloyoonjae-/helloyoonjae-/runs/sxshmb5m</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/helloyoonjae-/helloyoonjae-/runs/sxshmb5m?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"],"text/plain":["<wandb.sdk.wandb_run.Run at 0x7f4dff5955a0>"]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["wandb.init(project = 'helloyoonjae-', name = 'fastcampus_cv11')"]},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":436,"status":"ok","timestamp":1700315112439,"user":{"displayName":"Ynot(ì†¡ì›í˜¸)","userId":"16271863862696372773"},"user_tz":-540},"id":"KByfAeRmXwYk"},"outputs":[],"source":["# device\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","\n","# data config\n","data_path = '/data/ephemeral/home/datasets_fin/combined_train/train'\n","\n","# model config\n","model_name = 'tf_efficientnet_b5.ns_jft_in1k'#'tiny_vit_21m_384.dist_in22k_ft_in1k'#'efficientnetv2_rw_m.agc_in1k' #'vit_base_patch16_224.augreg_in1k' #'efficientnet_b3.ra2_in1k'#'densenet121.ra_in1k'# #'resnet101' #'resnet34' # 'resnet50' 'efficientnet-b0', ...\n","\n","# B0: 224 x 224\n","# B1: 240 x 240\n","# B2: 260 x 260\n","# B3: 300 x 300\n","# B4: 380 x 380\n","# B5: 456 x 456\n","# B6: 528 x 528\n","# B7: 600 x 600\n","\n","# training config\n","pre_img_size = 600\n","img_size = 456 #224 #256\n","LR = 5e-4\n","EPOCHS = 5\n","BATCH_SIZE = 8\n","num_workers = 4\n","early_stopping_patience = 5  # Early Stopping ì„¤ì •\n","augment_ratio = 200"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"amum-FlIojc6"},"source":["## 4. Load Data\n","* í•™ìŠµ, í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ê³¼ ë¡œë”ë¥¼ ì •ì˜í•©ë‹ˆë‹¤."]},{"cell_type":"code","execution_count":7,"metadata":{"executionInfo":{"elapsed":255,"status":"ok","timestamp":1700315066028,"user":{"displayName":"Ynot(ì†¡ì›í˜¸)","userId":"16271863862696372773"},"user_tz":-540},"id":"kTECBJfVTbdl"},"outputs":[],"source":["# one epoch í•™ìŠµì„ ìœ„í•œ í•¨ìˆ˜ì…ë‹ˆë‹¤.\n","def train_one_epoch(loader, model, optimizer, loss_fn, device):\n","    model.train()\n","    train_loss = 0\n","    preds_list = []\n","    targets_list = []\n","\n","    pbar = tqdm(loader)\n","    for image, targets in pbar:\n","        image = image.to(device)\n","        targets = targets.to(device)\n","\n","        model.zero_grad(set_to_none=True)\n","\n","        preds = model(image)\n","        loss = loss_fn(preds, targets)\n","        loss.backward()\n","        optimizer.step()\n","\n","        train_loss += loss.item()\n","        preds_list.extend(preds.argmax(dim=1).detach().cpu().numpy())\n","        targets_list.extend(targets.detach().cpu().numpy())\n","\n","        pbar.set_description(f\"Loss: {loss.item():.4f}\")\n","\n","    train_loss /= len(loader)\n","    train_acc = accuracy_score(targets_list, preds_list)\n","    train_f1 = f1_score(targets_list, preds_list, average='macro')\n","\n","    ret = {\n","        \"train_loss\": train_loss,\n","        \"train_acc\": train_acc,\n","        \"train_f1\": train_f1,\n","    }\n","\n","    return ret"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[],"source":["# ë°ì´í„°ì…‹ í´ë˜ìŠ¤ë¥¼ ì •ì˜í•©ë‹ˆë‹¤.\n","class ImageDataset(Dataset):\n","    def __init__(self, csv, path, transform=None, oversample=False, augment_ratio=1):\n","        self.df = pd.read_csv(csv)\n","        self.path = path\n","        self.transform = transform\n","        self.oversample = oversample\n","        self.augment_ratio = augment_ratio\n","\n","        # í´ë˜ìŠ¤ê°„ ë¶ˆê· í˜• í•´ì†Œë¥¼ ìœ„í•œ ìƒ˜í”Œ ì¦ì‹\n","        if self.oversample:\n","            # ê° í´ë˜ìŠ¤ë³„ë¡œ ë°ì´í„° ìˆ˜ ê³„ì‚°\n","            class_counts = np.bincount(self.df.values[:, 1].astype(int))\n","\n","            # ê° í´ë˜ìŠ¤ë³„ë¡œ ì¦ì‹í•  íšŸìˆ˜ ì„¤ì • (ì´ ì˜ˆì œì—ì„œëŠ” ìµœëŒ€ ë°ì´í„° ìˆ˜ì— ë§ì¶¤)\n","            max_class_count = max(class_counts)\n","            oversample_factors = [max_class_count // count for count in class_counts]\n","            # Class 3, 7 ê°€ì¤‘ì¹˜ 2ë¡œ ë³€ê²½\n","            # oversample_factors[3] = 2\n","            # oversample_factors[7] = 2 \n","            # oversample_factors[14] = 3 \n","\n","            # ê° í´ë˜ìŠ¤ë³„ë¡œ ë°ì´í„°ë¥¼ ì¦ì‹í•œ ìƒˆë¡œìš´ ë°ì´í„° í”„ë ˆì„ ìƒì„±\n","            oversampled_data = [self.df.values[self.df.values[:, 1] == cls].repeat(factor, axis=0) for cls, factor in enumerate(oversample_factors)]\n","            oversampled_data = np.vstack(oversampled_data)\n","\n","            self.df = pd.DataFrame(oversampled_data, columns=self.df.columns)\n","\n","    def __len__(self):\n","        return len(self.df) * self.augment_ratio\n","\n","    def __getitem__(self, idx):\n","        real_idx = idx % len(self.df)\n","        name, target = self.df.iloc[real_idx]\n","        img = np.array(Image.open(os.path.join(self.path, name)).convert(\"RGB\"))\n","        \n","        if self.transform:\n","            img = self.transform(image=img)['image']\n","        return img, target\n","    \n","    \n","# meta.csv íŒŒì¼ ì½ê¸°\n","meta_data = pd.read_csv('/data/ephemeral/home/datasets_fin/meta.csv')\n","label_to_class_name = dict(zip(meta_data['target'], meta_data['class_name']))"]},{"cell_type":"markdown","metadata":{},"source":["# ì „ì²˜ë¦¬"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[],"source":["import numpy as np\n","from PIL import Image\n","import albumentations as A\n","\n","class Crop(A.ImageOnlyTransform):\n","    def __init__(self, crop_position='top', always_apply=False, p=1.0):\n","        super(Crop, self).__init__(always_apply, p)\n","        self.crop_position = crop_position\n","\n","    def apply(self, img, **params):\n","        # ì´ë¯¸ì§€ë¥¼ PIL ì´ë¯¸ì§€ë¡œ ë³€í™˜ (Albumentationsì€ NumPy ë°°ì—´ì„ ì‚¬ìš©)\n","        if isinstance(img, np.ndarray):\n","            img = Image.fromarray(img)\n","\n","        width, height = img.size\n","\n","        if self.crop_position == 'top':\n","            img = img.crop((0, 0, width, height // 2))\n","        elif self.crop_position == 'bottom':\n","            img = img.crop((0, height // 2, width, height))\n","        else:\n","            raise ValueError(\"crop_position must be 'top' or 'bottom'\")\n","\n","        # PIL ì´ë¯¸ì§€ë¥¼ ë‹¤ì‹œ NumPy ë°°ì—´ë¡œ ë³€í™˜\n","        img = np.array(img)\n","        return img\n"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[],"source":["class Adjust(A.ImageOnlyTransform):\n","    def __init__(self, always_apply=False, p=1.0):\n","        super(Adjust, self).__init__(always_apply, p)\n","\n","    def apply(self, img, **params):\n","        \n","        # ì´ë¯¸ì§€ë¥¼ PIL ì´ë¯¸ì§€ë¡œ ë³€í™˜ (Albumentationsì€ NumPy ë°°ì—´ì„ ì‚¬ìš©)\n","        if isinstance(img, np.ndarray):\n","            # img = Image.fromarray(img)\n","            img = Image.fromarray(img.astype(np.uint8))\n","        # PIL ì´ë¯¸ì§€ë¥¼ ë‹¤ì‹œ NumPy ë°°ì—´ë¡œ ë³€í™˜\n","        img = np.array(img)               \n","                \n","        gray_image = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n","        average_brightness = np.mean(gray_image)\n","        # print(average_brightness)\n","        if average_brightness < 160:\n","            img_pil = Image.fromarray(img)\n","            # Adjust brightness\n","            enhancer = ImageEnhance.Brightness(img_pil)\n","            img_pil = enhancer.enhance(1.5)\n","            \n","            # Adjust contrast\n","            enhancer = ImageEnhance.Contrast(img_pil)\n","            img_pil = enhancer.enhance(2.0)\n","            \n","            img = np.array(img_pil)\n","        \n","        return img"]},{"cell_type":"markdown","metadata":{},"source":["# ì •ê·œí™” ìˆ˜ì¹˜ë¥¼ ê¸°ì¡´ê²ƒì„ ëŒ€ì²´í•˜ì—¬ trainë°ì´í„°ì— ëŒ€í•´ ì„¤ì •í•˜ê¸°: íš¨ê³¼ê°€ ì—†ì–´ì„œ ì•ˆí•˜ëŠ” ê²ƒìœ¼ë¡œ í•¨"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import numpy as np\n","import torch\n","from torch.utils.data import DataLoader\n","from torchvision.datasets import ImageFolder\n","import albumentations as A\n","from albumentations.pytorch import ToTensorV2\n","from tqdm import tqdm\n","\n","# Albumentations transform ì •ì˜\n","pre_img_size = pre_img_size  # ì‚¬ì „ ì •ì˜ëœ ì´ë¯¸ì§€ í¬ê¸° (í•„ìš”ì— ë”°ë¼ ìˆ˜ì •)\n","img_size = pre_img_size\n","augment_ratio = augment_ratio\n","\n","transform = A.Compose([\n","    A.LongestMaxSize(max_size=pre_img_size, always_apply=True),\n","    A.PadIfNeeded(min_height=pre_img_size, min_width=pre_img_size, border_mode=0, value=(255, 255, 255)),\n","    A.Resize(height=img_size, width=img_size),\n","    \n","    A.OneOf([\n","        A.GaussNoise(var_limit=(10.0, 800.0), p=0.75),\n","        A.GaussianBlur(blur_limit=(1, 7), p=0.5)\n","    ], p=0.75),\n","    # A.RandomRotate90(p=0.5),\n","    A.HorizontalFlip(p=0.75),\n","    A.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2, p=0.5),\n","    A.ShiftScaleRotate(shift_limit=0.1, scale_limit=0.1, rotate_limit=30, p=0.25),\n","    # RotateBy30(p=0.75),\n","    A.CoarseDropout(max_holes=6, max_height=32, max_width=32, p=0.5),\n","    A.ElasticTransform(alpha=1, sigma=30, alpha_affine=30, p=0.5),\n","    A.HueSaturationValue(hue_shift_limit=20, sat_shift_limit=30, val_shift_limit=20, p=0.5),\n","    A.RandomBrightnessContrast(brightness_limit=0.2, contrast_limit=0.2, p=0.1),\n","    # A.Rotate(limit=30, p=0.75),\n","    A.CLAHE(clip_limit=2.0, tile_grid_size=(8, 8), p=0.5),\n","    A.MotionBlur(blur_limit=5, p=0.5),\n","    A.OpticalDistortion(p=0.5),\n","    A.Transpose(p=0.5),\n","    # A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n","    ToTensorV2(),\n","])\n","\n","class CustomDataset(ImageFolder):\n","    def __init__(self, root, transform=None, augment_ratio=1):\n","        super().__init__(root, transform=None)\n","        self.custom_transform = transform\n","        self.augment_ratio = augment_ratio\n","\n","    def __getitem__(self, index):\n","        path, target = self.samples[index % len(self.samples)]\n","        sample = self.loader(path).convert(\"RGB\")  # PIL ì´ë¯¸ì§€ë¡œ ë³€í™˜\n","        if self.custom_transform is not None:\n","            sample = np.array(sample)  # AlbumentationsëŠ” NumPy ë°°ì—´ì„ ì‚¬ìš©\n","            sample = self.custom_transform(image=sample)[\"image\"]  # ë³€í™˜ ì ìš©\n","        return sample, target\n","    \n","    def __len__(self):\n","        return len(self.samples) * self.augment_ratio\n","\n","# ë°ì´í„°ì…‹ ë£¨íŠ¸ ê²½ë¡œ ì„¤ì • (í•„ìš”ì— ë”°ë¼ ìˆ˜ì •)\n","dataset_root = '/data/ephemeral/home/datasets_fin/combined_train/'\n","\n","# ë°ì´í„°ì…‹ ë¡œë“œ\n","dataset = CustomDataset(root=dataset_root, transform=transform)\n","dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n","\n","# ì±„ë„ë³„ í‰ê· ê³¼ í‘œì¤€ í¸ì°¨ ê³„ì‚°\n","mean = 0.0\n","std = 0.0\n","nb_samples = 0.0\n","\n","for data, _ in tqdm(dataloader, desc=\"Calculating mean and std\", unit=\"batch\"):\n","    data = data.float()/ 255.0  # float íƒ€ì…ìœ¼ë¡œ ë³€í™˜í•˜ê³  0-1 ë²”ìœ„ë¡œ ìŠ¤ì¼€ì¼ë§\n","    batch_samples = data.size(0)\n","    data = data.view(batch_samples, data.size(1), -1)\n","    mean += data.mean(2).sum(0)\n","    std += data.std(2).sum(0)\n","    nb_samples += batch_samples\n","\n","mean /= nb_samples\n","std /= nb_samples\n","\n","print(f'Mean: {mean}')\n","print(f'Std: {std}')\n","\n","# ê³„ì‚°ëœ ê°’ì„ ì‚¬ìš©í•˜ì—¬ ì •ê·œí™”\n","custom_transform = transforms.Compose([\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean=mean, std=std),\n","])\n","\n","# ì •ê·œí™”ëœ ë°ì´í„°ì…‹ ë¡œë“œ\n","custom_dataset = CustomDataset(root=dataset_root, transform=custom_transform)\n"]},{"cell_type":"code","execution_count":11,"metadata":{"executionInfo":{"elapsed":1,"status":"ok","timestamp":1700315112439,"user":{"displayName":"Ynot(ì†¡ì›í˜¸)","userId":"16271863862696372773"},"user_tz":-540},"id":"llh5C7ZKoq2S"},"outputs":[],"source":["# https://demo.albumentations.ai/\n","import albumentations as A\n","from albumentations.pytorch import ToTensorV2\n","\n","# horizontal_flip, vertical_flip, double_flip, transpose ë“± ë³€í™˜ ì •ì˜\n","horizontal_flip = A.HorizontalFlip(p=1)\n","vertical_flip = A.VerticalFlip(p=1)\n","double_flip = A.Compose([\n","    A.HorizontalFlip(p=1),\n","    A.VerticalFlip(p=1),\n","])\n","transpose = A.Transpose(p=1)\n","transpose_hflip = A.Compose([\n","    A.Transpose(p=1), \n","    A.HorizontalFlip(p=1),\n","])\n","transpose_vflip = A.Compose([\n","    A.Transpose(p=1),\n","    A.VerticalFlip(p=1),\n","])\n","transpose_dflip = A.Compose([\n","    A.Transpose(p=1),  \n","    A.HorizontalFlip(p=1),\n","    A.VerticalFlip(p=1),\n","])\n","\n","# Augmentationì„ ìœ„í•œ transform ì½”ë“œ\n","trn_transform = A.Compose([\n","    A.LongestMaxSize(max_size=pre_img_size, always_apply=True),\n","    A.PadIfNeeded(min_height=pre_img_size, min_width=pre_img_size, border_mode=0, value=(255, 255, 255)),\n","    A.OneOf([\n","        A.GaussNoise(var_limit=(10.0, 800.0), p=1),\n","        A.GaussianBlur(blur_limit=(1, 7), p=1),\n","        A.MotionBlur(blur_limit=(3, 7), p=1),\n","        A.MedianBlur(blur_limit=3, p=1)\n","    ], p=1),\n","    A.ShiftScaleRotate(shift_limit=0.1, scale_limit=0.1, rotate_limit=30, p=0.25),\n","    A.Rotate(limit=(0, 360), p=0.75),\n","    A.GridDistortion(always_apply=False, p=0.75, num_steps=6, distort_limit=(-0.3, 0.3), interpolation=0, border_mode=0, value=(0, 0, 0), mask_value=None, normalized=False),\n","    A.OneOf([\n","        horizontal_flip,\n","        vertical_flip,\n","        double_flip,\n","        transpose,\n","        transpose_hflip,\n","        transpose_vflip,\n","        transpose_dflip\n","    ], p=1.0),\n","    A.OneOf([\n","        A.OpticalDistortion(always_apply=False, p=1.0, distort_limit=(-0.3, 0.3), shift_limit=(-0.05, 0.09), interpolation=0, border_mode=0, value=(0, 0, 0), mask_value=None),\n","        A.MultiplicativeNoise(always_apply=False, p=1.0, multiplier=(0.93, 2.22), per_channel=True, elementwise=True),\n","        A.ISONoise(always_apply=False, p=1.0, intensity=(0.38, 1.0), color_shift=(0.18, 0.47)),\n","        A.RandomBrightnessContrast(always_apply=False, p=1.0, brightness_limit=(0.19, 0.62), contrast_limit=(-0.02, 0.62), brightness_by_max=True)\n","    ], p=0.75),\n","    \n","    A.Resize(height=img_size, width=img_size),\n","    A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n","    ToTensorV2(),\n","])\n","\n","# test image ë³€í™˜ì„ ìœ„í•œ transform ì½”ë“œ\n","tst_transform = A.Compose([\n","    #Adjust(always_apply=True),\n","    A.LongestMaxSize(max_size=pre_img_size, always_apply=True),\n","    A.PadIfNeeded(min_height=pre_img_size, min_width=pre_img_size, border_mode=0, value=(255, 255, 255)),\n","    A.Resize(height=img_size, width=img_size),\n","    A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n","    ToTensorV2(),\n","])\n"]},{"cell_type":"code","execution_count":18,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Original training data count: 1666\n","Augmented training data count: 333200\n","Test data count: 3140\n"]}],"source":["# ë°ì´í„°ì…‹ ë° ë°ì´í„° ë¡œë” ì •ì˜\n","trn_dataset = ImageDataset(\n","    \"/data/ephemeral/home/datasets_fin/train_labelupdate.csv\",\n","    \"/data/ephemeral/home/datasets_fin/combined_train/train\",\n","    transform=trn_transform,\n","    oversample=True,\n","    augment_ratio=augment_ratio\n",")\n","\n","\n","tst_dataset = ImageDataset(\n","    \"/data/ephemeral/home/datasets_fin/sample_submission.csv\",\n","    \"/data/ephemeral/home/datasets_fin/wdnx4/wdnx4\",\n","    transform=tst_transform,\n","    oversample=False,\n","    augment_ratio=1\n",")\n","\n","\n","ori_traindata_num = int(len(trn_dataset)/augment_ratio)\n","print(f\"Original training data count: {ori_traindata_num}\")\n","print(f\"Augmented training data count: {len(trn_dataset)}\")\n","print(f\"Test data count: {len(tst_dataset)}\")\n","# wandb config ì—…ë°ì´íŠ¸\n","wandb.config.update({\n","    \"Trn_data\": len(trn_dataset)\n","})"]},{"cell_type":"markdown","metadata":{},"source":["# ê¸°ë³¸ valid"]},{"cell_type":"code","execution_count":206,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Train dataset ê°œìˆ˜: 266560\n","Validation dataset ê°œìˆ˜: 66640\n","Test dataset ê°œìˆ˜: 3140\n"]}],"source":["# ë°ì´í„° ì…‹ì„ í•™ìŠµ ë°ì´í„° ì…‹ê³¼ ê²€ì¦ ë°ì´í„° ì…‹ìœ¼ë¡œ ë¶„ë¦¬\n","import copy\n","# validation config\n","VALID_RATIO = 0.8\n","\n","total_size = len(trn_dataset)\n","train_num, valid_num = int(total_size * VALID_RATIO), total_size - int(total_size * VALID_RATIO)\n","\n","# train - valid set ë‚˜ëˆ„ê¸°\n","generator = torch.Generator().manual_seed(SEED)\n","train_dataset, valid_dataset = torch.utils.data.random_split(trn_dataset, [train_num, valid_num], generator = generator)\n","\n","valid_data = copy.deepcopy(valid_dataset)\n","valid_data.dataset.transform = tst_transform\n","\n","print(f'Train dataset ê°œìˆ˜: {len(train_dataset)}')\n","print(f'Validation dataset ê°œìˆ˜: {len(valid_dataset)}')\n","print(f'Test dataset ê°œìˆ˜: {len(tst_dataset)}')"]},{"cell_type":"markdown","metadata":{},"source":["# stratified valid"]},{"cell_type":"code","execution_count":19,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Train dataset ê°œìˆ˜: 251200\n","Validation dataset ê°œìˆ˜: 62800\n","Test dataset ê°œìˆ˜: 3140\n"]}],"source":["import pandas as pd\n","from sklearn.model_selection import StratifiedShuffleSplit\n","import torch\n","import numpy as np\n","import copy\n","\n","VALID_RATIO = 0.8\n","\n","# ì›ë³¸ CSV íŒŒì¼ ì½ê¸°\n","original_df = pd.read_csv(\"/data/ephemeral/home/datasets_fin/train_labelupdate.csv\")\n","\n","# ì›ë³¸ ë°ì´í„°ì˜ ë¼ë²¨ ì¶”ì¶œ\n","labels = original_df['target'].values\n","\n","# StratifiedShuffleSplit ì‚¬ìš©í•˜ì—¬ ì›ë³¸ ë°ì´í„°ì…‹ì„ í•™ìŠµ ë° ê²€ì¦ìœ¼ë¡œ ë¶„í• \n","sss = StratifiedShuffleSplit(n_splits=1, test_size=1-VALID_RATIO, random_state=SEED)\n","train_index, valid_index = next(sss.split(np.zeros(len(labels)), labels))\n","\n","# í•™ìŠµ ë° ê²€ì¦ ë°ì´í„°ì…‹ì„ ì›ë³¸ ë°ì´í„°í”„ë ˆì„ì—ì„œ ë¶„ë¦¬\n","train_df = original_df.iloc[train_index]\n","valid_df = original_df.iloc[valid_index]\n","\n","# ì¦ê°•ëœ ë°ì´í„°ì…‹ì„ ì›ë˜ ë°ì´í„°ì…‹ì— ëŒ€í•œ ì¸ë±ìŠ¤ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë¶„ë¦¬\n","# train_dfì™€ valid_dfì˜ ì¸ë±ìŠ¤ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì¦ê°•ëœ ë°ì´í„°ì…‹ì— ì ìš©\n","train_indices = np.hstack([np.arange(i * augment_ratio, (i + 1) * augment_ratio) for i in train_index])\n","valid_indices = np.hstack([np.arange(i * augment_ratio, (i + 1) * augment_ratio) for i in valid_index])\n","\n","train_dataset = torch.utils.data.Subset(trn_dataset, train_indices)\n","valid_dataset = torch.utils.data.Subset(trn_dataset, valid_indices)\n","\n","# ê²€ì¦ ë°ì´í„°ì— tst_transform ì ìš©\n","valid_data = copy.deepcopy(valid_dataset)\n","valid_data.dataset.transform = tst_transform\n","\n","print(f'Train dataset ê°œìˆ˜: {len(train_dataset)}')\n","print(f'Validation dataset ê°œìˆ˜: {len(valid_dataset)}')\n","print(f'Test dataset ê°œìˆ˜: {len(tst_dataset)}')\n"]},{"cell_type":"code","execution_count":20,"metadata":{"executionInfo":{"elapsed":1,"status":"ok","timestamp":1700315112808,"user":{"displayName":"Ynot(ì†¡ì›í˜¸)","userId":"16271863862696372773"},"user_tz":-540},"id":"_sO03fWaQj1h"},"outputs":[],"source":["# DataLoader ì •ì˜\n","trn_loader = DataLoader(\n","    trn_dataset,\n","    batch_size=BATCH_SIZE,\n","    shuffle=True,\n","    num_workers=num_workers,\n","    pin_memory=True,\n","    drop_last=False\n",")\n","tst_loader = DataLoader(\n","    tst_dataset,\n","    batch_size=BATCH_SIZE,\n","    shuffle=False,\n","    num_workers=0,\n","    pin_memory=True\n",")\n","valid_loader = DataLoader(\n","    valid_dataset, \n","    batch_size = BATCH_SIZE, \n","    shuffle = False,\n","    num_workers=0,\n","    pin_memory=True\n","    )\n"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"Nmm5h3J-pXNV"},"source":["## 5. Train Model\n","* ëª¨ë¸ì„ ë¡œë“œí•˜ê³ , í•™ìŠµì„ ì§„í–‰í•©ë‹ˆë‹¤."]},{"cell_type":"markdown","metadata":{},"source":["# timm ëª¨ë¸"]},{"cell_type":"code","execution_count":21,"metadata":{"executionInfo":{"elapsed":870,"status":"ok","timestamp":1700315114067,"user":{"displayName":"Ynot(ì†¡ì›í˜¸)","userId":"16271863862696372773"},"user_tz":-540},"id":"FbBgFPsLT-CO"},"outputs":[],"source":["torch.cuda.empty_cache()\n","\n","# ëª¨ë¸ì„ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤.\n","model = timm.create_model(\n","    model_name,\n","    pretrained=True,\n","    num_classes=17,\n","    drop_rate=0.2\n",").to(device)\n","\n","# ì†ì‹¤ í•¨ìˆ˜ë¥¼ ì •ì˜í•©ë‹ˆë‹¤.\n","loss_fn = nn.CrossEntropyLoss()\n","\n","# ì˜µí‹°ë§ˆì´ì €ë¥¼ ì •ì˜í•©ë‹ˆë‹¤.\n","optimizer = AdamW(model.parameters(), lr=LR)\n","\n","# Learning Rate Schedulerë¥¼ ì •ì˜í•©ë‹ˆë‹¤.\n","scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=3, verbose=True)\n","\n","# Early Stoppingì„ ìœ„í•œ ë³€ìˆ˜ ì´ˆê¸°í™”\n","best_loss = float('inf')\n","early_stopping_counter = 0"]},{"cell_type":"code","execution_count":22,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8778,"status":"ok","timestamp":1700315122843,"user":{"displayName":"Ynot(ì†¡ì›í˜¸)","userId":"16271863862696372773"},"user_tz":-540},"id":"OvIVcSRgUPtS","outputId":"88230bf2-976f-45f6-b3b7-1a2d0ad00548"},"outputs":[],"source":["# í•œ ì—í­(epoch) ë™ì•ˆ ëª¨ë¸ì„ í•™ìŠµì‹œí‚¤ëŠ” í•¨ìˆ˜ì…ë‹ˆë‹¤.\n","def train_one_epoch(loader, model, optimizer, loss_fn, device, log_interval=1000):\n","    model.train()\n","    train_loss = 0\n","    preds_list = []\n","    targets_list = []\n","\n","    pbar = tqdm(loader)\n","    for batch_idx, (image, targets) in enumerate(pbar):\n","        if image is None or targets is None:\n","            continue\n","        \n","        image = image.to(device)\n","        targets = targets.to(device)\n","\n","        optimizer.zero_grad(set_to_none=True)\n","\n","        preds = model(image)\n","        loss = loss_fn(preds, targets)\n","        loss.backward()\n","        optimizer.step()\n","\n","        train_loss += loss.item()\n","        preds_list.extend(preds.argmax(dim=1).detach().cpu().numpy())\n","        targets_list.extend(targets.detach().cpu().numpy())\n","\n","        pbar.set_description(f\"Loss: {loss.item():.4f}\")\n","\n","        # ì§€ì •ëœ ê°„ê²©ìœ¼ë¡œë§Œ wandbì— ë¡œê·¸ë¥¼ ë‚¨ê¹€\n","        if batch_idx % log_interval == 0:\n","            wandb.log({\n","                \"batch_loss\": loss.item(),\n","                \"cumulative_loss\": train_loss / (batch_idx + 1)\n","            })\n","\n","    train_loss /= len(loader)\n","    train_acc = accuracy_score(targets_list, preds_list)\n","    train_f1 = f1_score(targets_list, preds_list, average='macro')\n","\n","    ret = {\n","        \"train_loss\": train_loss,\n","        \"train_acc\": train_acc,\n","        \"train_f1\": train_f1,\n","    }\n","\n","    # ì—í­ ë‹¨ìœ„ë¡œ wandbì— í•™ìŠµ ê³¼ì • ë¡œê·¸\n","    wandb.log({\"train_loss\": train_loss, \"train_acc\": train_acc, \"train_f1\": train_f1})\n","\n","    return ret"]},{"cell_type":"code","execution_count":24,"metadata":{},"outputs":[],"source":["import torch\n","from sklearn.metrics import accuracy_score, f1_score\n","from tqdm import tqdm\n","\n","def training(model, dataloader, criterion, optimizer, device, epoch, num_epochs):\n","    model.train()\n","    train_loss = 0\n","    preds_list = []\n","    targets_list = []\n","\n","    pbar = tqdm(dataloader)\n","    for images, labels in pbar:\n","        images = images.to(device)\n","        labels = labels.to(device)\n","\n","        model.zero_grad(set_to_none=True)\n","\n","        preds = model(images)\n","        loss = criterion(preds, labels)\n","        loss.backward()\n","        optimizer.step()\n","\n","        train_loss += loss.item()\n","        preds_list.extend(preds.argmax(dim=1).detach().cpu().numpy())\n","        targets_list.extend(labels.detach().cpu().numpy())\n","\n","        pbar.set_description(f\"Epoch [{epoch+1}/{num_epochs}] - Train Loss: {loss.item()}\")\n","        \n","    train_loss /= len(dataloader)\n","    train_acc = accuracy_score(targets_list, preds_list)    \n","    train_f1 = f1_score(targets_list, preds_list, average='macro')\n","\n","    return model, train_loss, train_acc, train_f1\n","\n","def evaluation(model, dataloader, criterion, device, epoch, num_epochs):\n","    model.eval()  # ëª¨ë¸ì„ í‰ê°€ ëª¨ë“œë¡œ ì„¤ì •\n","    valid_loss = 0.0\n","    preds_list = []\n","    targets_list = []\n","\n","    with torch.no_grad():  # modelì˜ ì—…ë°ì´íŠ¸ ë§‰ê¸°\n","        tbar = tqdm(dataloader)\n","        for images, labels in tbar:\n","            images = images.to(device)\n","            labels = labels.to(device)\n","\n","            preds = model(images)\n","            loss = criterion(preds, labels)\n","\n","            valid_loss += loss.item()\n","            preds_list.extend(preds.argmax(dim=1).detach().cpu().numpy())\n","            targets_list.extend(labels.detach().cpu().numpy())\n","\n","            tbar.set_description(f\"Epoch [{epoch+1}/{num_epochs}] - Valid Loss: {loss.item()}\")\n","\n","    valid_loss = valid_loss / len(dataloader)\n","    valid_acc = accuracy_score(targets_list, preds_list)  \n","    valid_f1 = f1_score(targets_list, preds_list, average='macro')\n","\n","    return valid_loss, valid_acc, valid_f1\n","\n","def training_loop(model, train_dataloader, valid_dataloader, criterion, optimizer, device, num_epochs, early_stopping_patience, model_name, run):\n","    best_valid_loss = float('inf')\n","    early_stop_counter = 0\n","    valid_max_accuracy = -1\n","    best_model = None\n","\n","    for epoch in range(num_epochs):\n","        model, train_loss, train_acc, train_f1 = training(model, train_dataloader, criterion, optimizer, device, epoch, num_epochs)\n","        valid_loss, valid_acc, valid_f1 = evaluation(model, valid_dataloader, criterion, device, epoch, num_epochs)\n","\n","        monitoring_value = {'train_loss': train_loss, 'train_accuracy': train_acc, 'train_f1': train_f1, \n","                            'valid_loss': valid_loss, 'valid_accuracy': valid_acc, 'valid_f1': valid_f1}\n","        \n","        run.log(monitoring_value, step=epoch)\n","        \n","        print(f'''Epoch [{epoch + 1}/{num_epochs}] Finished\n","        Train Loss: {train_loss}, Train Accuracy: {train_acc}, Train F1: {train_f1}\n","        Valid Loss: {valid_loss}, Valid Accuracy: {valid_acc}, Valid F1: {valid_f1}''')\n","\n","        # Save the model after every epoch\n","        epoch_model_name = f\"{model_name}_epoch_{epoch + 1}.pt\"\n","        torch.save(model.state_dict(), f\"./{epoch_model_name}\")\n","        print(f'Model saved as {epoch_model_name}')\n","\n","        if valid_acc > valid_max_accuracy:\n","            valid_max_accuracy = valid_acc\n","\n","        if valid_loss < best_valid_loss:\n","            best_valid_loss = valid_loss\n","            best_model = model\n","            early_stop_counter = 0\n","        else:\n","            early_stop_counter += 1\n","\n","        if early_stop_counter >= early_stopping_patience:\n","            print(\"Early stopping\")\n","            break\n","\n","    return best_model, valid_max_accuracy"]},{"cell_type":"code","execution_count":25,"metadata":{},"outputs":[{"data":{"text/html":["Finishing last run (ID:sxshmb5m) before initializing another..."],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"9ba86deeedbe424caaa29cf697009310","version_major":2,"version_minor":0},"text/plain":["VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<style>\n","    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n","    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n","    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n","    </style>\n","<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>batch_loss</td><td>â–‡â–ˆâ–‚â–ƒâ–â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–</td></tr><tr><td>cumulative_loss</td><td>â–‡â–ˆâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–</td></tr><tr><td>train_acc</td><td>â–</td></tr><tr><td>train_f1</td><td>â–</td></tr><tr><td>train_loss</td><td>â–</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>batch_loss</td><td>0.00015</td></tr><tr><td>cumulative_loss</td><td>0.04455</td></tr><tr><td>train_acc</td><td>0.95854</td></tr><tr><td>train_f1</td><td>0.95869</td></tr><tr><td>train_loss</td><td>0.11829</td></tr></table><br/></div></div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View run <strong style=\"color:#cdcd00\">fastcampus_cv11</strong> at: <a href='https://wandb.ai/helloyoonjae-/helloyoonjae-/runs/sxshmb5m' target=\"_blank\">https://wandb.ai/helloyoonjae-/helloyoonjae-/runs/sxshmb5m</a><br/> View project at: <a href='https://wandb.ai/helloyoonjae-/helloyoonjae-' target=\"_blank\">https://wandb.ai/helloyoonjae-/helloyoonjae-</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Find logs at: <code>./wandb/run-20240810_221436-sxshmb5m/logs</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Successfully finished last run (ID:sxshmb5m). Initializing new run:<br/>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"13cfc7b6c7b54465839eb558e16cf131","version_major":2,"version_minor":0},"text/plain":["VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.01111227994163831, max=1.0)â€¦"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["wandb version 0.17.6 is available!  To upgrade, please run:\n"," $ pip install wandb --upgrade"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Tracking run with wandb version 0.17.5"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Run data is saved locally in <code>/data/ephemeral/home/notebook/wandb/run-20240811_101801-rjx6b26g</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Syncing run <strong><a href='https://wandb.ai/helloyoonjae-/helloyoonjae-/runs/rjx6b26g' target=\"_blank\">fastcampus_cv11</a></strong> to <a href='https://wandb.ai/helloyoonjae-/helloyoonjae-' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View project at <a href='https://wandb.ai/helloyoonjae-/helloyoonjae-' target=\"_blank\">https://wandb.ai/helloyoonjae-/helloyoonjae-</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View run at <a href='https://wandb.ai/helloyoonjae-/helloyoonjae-/runs/rjx6b26g' target=\"_blank\">https://wandb.ai/helloyoonjae-/helloyoonjae-/runs/rjx6b26g</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: logging graph, to disable use `wandb.watch(log_graph=False)`\n","Epoch [1/5] - Train Loss: 0.003721291199326515: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 41650/41650 [2:03:21<00:00,  5.63it/s]    \n","Epoch [1/5] - Valid Loss: 0.06029784306883812: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7850/7850 [33:20<00:00,  3.92it/s]   \n"]},{"name":"stdout","output_type":"stream","text":["Epoch [1/5] Finished\n","        Train Loss: 0.036345140690544425, Train Accuracy: 0.9877971188475391, Train F1: 0.9878459099534824\n","        Valid Loss: 0.033467487800746654, Valid Accuracy: 0.9897770700636943, Valid F1: 0.9897963070793717\n","Model saved as tf_efficientnet_b5.ns_jft_in1k_epoch_1.pt\n"]},{"name":"stderr","output_type":"stream","text":["Epoch [2/5] - Train Loss: 0.00010965180263156071: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 41650/41650 [2:02:35<00:00,  5.66it/s]  \n","Epoch [2/5] - Valid Loss: 0.5089395642280579: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7850/7850 [32:53<00:00,  3.98it/s]    \n"]},{"name":"stdout","output_type":"stream","text":["Epoch [2/5] Finished\n","        Train Loss: 0.027852874407122686, Train Accuracy: 0.9907082833133253, Train F1: 0.9907223764386129\n","        Valid Loss: 0.04782558159980957, Valid Accuracy: 0.988328025477707, Valid F1: 0.9885298437305631\n","Model saved as tf_efficientnet_b5.ns_jft_in1k_epoch_2.pt\n"]},{"name":"stderr","output_type":"stream","text":["Epoch [3/5] - Train Loss: 7.375933819275815e-06: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 41650/41650 [2:04:45<00:00,  5.56it/s]   \n","Epoch [3/5] - Valid Loss: 0.0006924738408997655: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7850/7850 [32:51<00:00,  3.98it/s] \n"]},{"name":"stdout","output_type":"stream","text":["Epoch [3/5] Finished\n","        Train Loss: 0.02459358372356609, Train Accuracy: 0.9919327731092437, Train F1: 0.9919484721322268\n","        Valid Loss: 0.015319352624171256, Valid Accuracy: 0.9950796178343949, Valid F1: 0.9949040383399208\n","Model saved as tf_efficientnet_b5.ns_jft_in1k_epoch_3.pt\n"]},{"name":"stderr","output_type":"stream","text":["Epoch [4/5] - Train Loss: 0.03986269608139992: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 41650/41650 [2:05:31<00:00,  5.53it/s]     \n","Epoch [4/5] - Valid Loss: 0.049288392066955566: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7850/7850 [32:53<00:00,  3.98it/s]  \n"]},{"name":"stdout","output_type":"stream","text":["Epoch [4/5] Finished\n","        Train Loss: 0.023005343230493396, Train Accuracy: 0.9926110444177672, Train F1: 0.992610674497882\n","        Valid Loss: 0.01393400842562387, Valid Accuracy: 0.9957165605095541, Valid F1: 0.9957402208435003\n","Model saved as tf_efficientnet_b5.ns_jft_in1k_epoch_4.pt\n"]},{"name":"stderr","output_type":"stream","text":["Epoch [5/5] - Train Loss: 7.610771717736498e-05: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 41650/41650 [2:02:23<00:00,  5.67it/s]   \n","Epoch [5/5] - Valid Loss: 8.300280023831874e-05: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7850/7850 [32:44<00:00,  4.00it/s] \n"]},{"name":"stdout","output_type":"stream","text":["Epoch [5/5] Finished\n","        Train Loss: 0.020965055139964755, Train Accuracy: 0.9930792316926771, Train F1: 0.9930848339978455\n","        Valid Loss: 0.01111240493865205, Valid Accuracy: 0.9964171974522293, Valid F1: 0.9963570469635774\n","Model saved as tf_efficientnet_b5.ns_jft_in1k_epoch_5.pt\n","Valid Max Accuracy: 0.9964171974522293\n"]}],"source":["# wandbì— ëª¨ë¸ì˜ weight & bias, graident ì‹œê°í™”\n","run = wandb.init(project = 'helloyoonjae-', name = 'fastcampus_cv11')\n","run.watch(model, loss_fn, log = 'all', log_graph = True)\n","\n","model, valid_max_accuracy = training_loop(model, trn_loader, valid_loader, loss_fn, optimizer, device, EPOCHS, early_stopping_patience, model_name, run)\n","print(f'Valid Max Accuracy: {valid_max_accuracy}')"]},{"cell_type":"markdown","metadata":{},"source":["# custom attention ëª¨ë¸"]},{"cell_type":"code","execution_count":18,"metadata":{},"outputs":[],"source":["# ê¸°ë³¸ì½”ë“œ\n","import torch\n","import torch.nn as nn\n","import timm\n","import torch.nn.functional as F\n","\n","class AttentionModule(nn.Module):\n","    def __init__(self, in_features, out_features):\n","        super(AttentionModule, self).__init__()\n","        self.attention = nn.Sequential(\n","            nn.Linear(in_features, out_features),\n","            nn.Sigmoid()\n","        )\n","\n","    def forward(self, x):\n","        attention_weights = self.attention(x)\n","        return x * attention_weights\n","    \n","\n","class CustomEfficientNetB7(nn.Module):\n","    def __init__(self, num_classes, attention_size=1792):\n","        super(CustomEfficientNetB7, self).__init__()\n","        self.base_model = timm.create_model('efficientnet_b7', pretrained=True)\n","        \n","        # Remove the existing classifier\n","        self.base_model.reset_classifier(0, '')\n","\n","        # Add SE attention mechanism\n","        self.attention = AttentionModule(attention_size, attention_size)\n","\n","        # Add dropout layer with 0.2 ratio\n","        self.dropout = nn.Dropout(p=0.2)\n","        # drop_out ë¹„ìœ¨ 20%\n","\n","        # New classifier with attention\n","        self.classifier = nn.Linear(attention_size, num_classes)\n","        \n","    def forward(self, x):\n","        x = self.base_model(x)\n","        \n","        # Global average pooling\n","        x = x.mean([2, 3])\n","\n","        # Apply attention mechanism\n","        x = self.attention(x)\n","\n","        # Apply dropout with 0.2 ratio\n","        x = self.dropout(x)\n","\n","        # Final classification\n","        x = self.classifier(x)\n","\n","        return x\n"]},{"cell_type":"code","execution_count":19,"metadata":{},"outputs":[],"source":["# ê°œì„ í•œ ì½”ë“œ\n","import torch\n","import torch.nn as nn\n","import timm\n","import torch.nn.functional as F\n","\n","class AttentionModule(nn.Module):\n","    def __init__(self, in_features, out_features):\n","        super(AttentionModule, self).__init__()\n","        self.attention = nn.Sequential(\n","            nn.Linear(in_features, out_features),\n","            # nn.Sigmoid()\n","            nn.Softmax(dim=1)\n","        )\n","\n","    def forward(self, x):\n","        attention_weights = self.attention(x)\n","        return x * attention_weights\n","    \n","\n","class CustomEfficientNetB7(nn.Module):\n","    def __init__(self, num_classes, attention_size=1792):\n","        super(CustomEfficientNetB7, self).__init__()\n","        self.base_model = timm.create_model('efficientnet_b7', pretrained=True)\n","        \n","        # Remove the existing classifier\n","        self.base_model.reset_classifier(0, '')\n","\n","        # Add SE attention mechanism\n","        self.attention = AttentionModule(attention_size, attention_size)\n","\n","        # Add dropout layer with 0.2 ratio\n","        self.dropout = nn.Dropout(p=0.2)\n","        \n","        # Batch normalization after dropout\n","        self.batch_norm = nn.BatchNorm1d(attention_size)\n","\n","        # New classifier with attention\n","        self.classifier = nn.Linear(attention_size, num_classes)\n","        \n","    def forward(self, x):\n","        x = self.base_model(x)\n","        \n","        # Global average pooling\n","        x = F.adaptive_avg_pool2d(x, 1).view(x.size(0), -1)  # Adaptive pooling\n","\n","        # Apply attention mechanism\n","        attention_output = self.attention(x)\n","        \n","        # Residual Connection\n","        x = x + attention_output\n","\n","        # Apply dropout with 0.2 ratio\n","        x = self.dropout(x)\n","\n","        # Apply BatchNorm\n","        x = self.batch_norm(x)\n","\n","        # Final classification\n","        x = self.classifier(x)\n","\n","        return x\n"]},{"cell_type":"code","execution_count":20,"metadata":{},"outputs":[],"source":["import torch\n","from sklearn.metrics import accuracy_score, f1_score\n","from tqdm import tqdm\n","\n","def training(model, dataloader, criterion, optimizer, device, epoch, num_epochs):\n","    model.train()\n","    train_loss = 0\n","    preds_list = []\n","    targets_list = []\n","\n","    pbar = tqdm(dataloader)\n","    for images, labels in pbar:\n","        images = images.to(device)\n","        labels = labels.to(device)\n","\n","        model.zero_grad(set_to_none=True)\n","\n","        preds = model(images)\n","        loss = criterion(preds, labels)\n","        loss.backward()\n","        optimizer.step()\n","\n","        train_loss += loss.item()\n","        preds_list.extend(preds.argmax(dim=1).detach().cpu().numpy())\n","        targets_list.extend(labels.detach().cpu().numpy())\n","\n","        pbar.set_description(f\"Epoch [{epoch+1}/{num_epochs}] - Train Loss: {loss.item()}\")\n","        \n","    train_loss /= len(dataloader)\n","    train_acc = accuracy_score(targets_list, preds_list)    \n","    train_f1 = f1_score(targets_list, preds_list, average='macro')\n","\n","    return model, train_loss, train_acc, train_f1\n","\n","def evaluation(model, dataloader, criterion, device, epoch, num_epochs):\n","    model.eval()  # ëª¨ë¸ì„ í‰ê°€ ëª¨ë“œë¡œ ì„¤ì •\n","    valid_loss = 0.0\n","    preds_list = []\n","    targets_list = []\n","\n","    with torch.no_grad():  # modelì˜ ì—…ë°ì´íŠ¸ ë§‰ê¸°\n","        tbar = tqdm(dataloader)\n","        for images, labels in tbar:\n","            images = images.to(device)\n","            labels = labels.to(device)\n","\n","            preds = model(images)\n","            loss = criterion(preds, labels)\n","\n","            valid_loss += loss.item()\n","            preds_list.extend(preds.argmax(dim=1).detach().cpu().numpy())\n","            targets_list.extend(labels.detach().cpu().numpy())\n","\n","            tbar.set_description(f\"Epoch [{epoch+1}/{num_epochs}] - Valid Loss: {loss.item()}\")\n","\n","    valid_loss = valid_loss / len(dataloader)\n","    valid_acc = accuracy_score(targets_list, preds_list)  \n","    valid_f1 = f1_score(targets_list, preds_list, average='macro')\n","\n","    return valid_loss, valid_acc, valid_f1\n","\n","def training_loop(model, train_dataloader, valid_dataloader, criterion, optimizer, device, num_epochs, early_stopping_patience, model_name, run):\n","    best_valid_loss = float('inf')\n","    early_stop_counter = 0\n","    valid_max_accuracy = -1\n","    best_model = None\n","\n","    for epoch in range(num_epochs):\n","        model, train_loss, train_acc, train_f1 = training(model, train_dataloader, criterion, optimizer, device, epoch, num_epochs)\n","        valid_loss, valid_acc, valid_f1 = evaluation(model, valid_dataloader, criterion, device, epoch, num_epochs)\n","\n","        monitoring_value = {'train_loss': train_loss, 'train_accuracy': train_acc, 'train_f1': train_f1, \n","                            'valid_loss': valid_loss, 'valid_accuracy': valid_acc, 'valid_f1': valid_f1}\n","        \n","        run.log(monitoring_value, step=epoch)\n","        \n","        print(f'''Epoch [{epoch + 1}/{num_epochs}] Finished\n","        Train Loss: {train_loss}, Train Accuracy: {train_acc}, Train F1: {train_f1}\n","        Valid Loss: {valid_loss}, Valid Accuracy: {valid_acc}, Valid F1: {valid_f1}''')\n","\n","        # Save the model after every epoch\n","        epoch_model_name = f\"{model_name}_epoch_{epoch + 1}.pt\"\n","        torch.save(model.state_dict(), f\"./{epoch_model_name}\")\n","        print(f'Model saved as {epoch_model_name}')\n","\n","        if valid_acc > valid_max_accuracy:\n","            valid_max_accuracy = valid_acc\n","\n","        if valid_loss < best_valid_loss:\n","            best_valid_loss = valid_loss\n","            best_model = model\n","            early_stop_counter = 0\n","        else:\n","            early_stop_counter += 1\n","\n","        if early_stop_counter >= early_stopping_patience:\n","            print(\"Early stopping\")\n","            break\n","\n","    return best_model, valid_max_accuracy"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# ëª¨ë¸ ìƒì„±\n","torch.cuda.empty_cache()\n","\n","num_classes = 17\n","\n","model = CustomEfficientNetB7(num_classes).to(device)\n","loss_fn = nn.CrossEntropyLoss()\n","optimizer = AdamW(model.parameters(), lr=LR)"]},{"cell_type":"code","execution_count":20,"metadata":{},"outputs":[{"data":{"text/html":["Finishing last run (ID:x4ps850u) before initializing another..."],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"36e8a02111b44299afc19b659fcd48b0","version_major":2,"version_minor":0},"text/plain":["VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View run <strong style=\"color:#cdcd00\">fastcampus_cv11</strong> at: <a href='https://wandb.ai/helloyoonjae-/helloyoonjae-/runs/x4ps850u' target=\"_blank\">https://wandb.ai/helloyoonjae-/helloyoonjae-/runs/x4ps850u</a><br/> View project at: <a href='https://wandb.ai/helloyoonjae-/helloyoonjae-' target=\"_blank\">https://wandb.ai/helloyoonjae-/helloyoonjae-</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Find logs at: <code>./wandb/run-20240809_233826-x4ps850u/logs</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Successfully finished last run (ID:x4ps850u). Initializing new run:<br/>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"3fc13bd15c0d4dc4a3236a278f24905c","version_major":2,"version_minor":0},"text/plain":["VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011112341657280922, max=1.0â€¦"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["wandb version 0.17.6 is available!  To upgrade, please run:\n"," $ pip install wandb --upgrade"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Tracking run with wandb version 0.17.5"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Run data is saved locally in <code>/data/ephemeral/home/notebook/wandb/run-20240809_233923-t3hy1n02</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Syncing run <strong><a href='https://wandb.ai/helloyoonjae-/helloyoonjae-/runs/t3hy1n02' target=\"_blank\">fastcampus_cv11</a></strong> to <a href='https://wandb.ai/helloyoonjae-/helloyoonjae-' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View project at <a href='https://wandb.ai/helloyoonjae-/helloyoonjae-' target=\"_blank\">https://wandb.ai/helloyoonjae-/helloyoonjae-</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View run at <a href='https://wandb.ai/helloyoonjae-/helloyoonjae-/runs/t3hy1n02' target=\"_blank\">https://wandb.ai/helloyoonjae-/helloyoonjae-/runs/t3hy1n02</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: logging graph, to disable use `wandb.watch(log_graph=False)`\n","Epoch [1/5] - Train Loss: 0.0043776435777544975: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 41650/41650 [1:04:03<00:00, 10.84it/s] \n","Epoch [1/5] - Valid Loss: 0.022932710126042366: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7850/7850 [40:19<00:00,  3.24it/s]  \n"]},{"name":"stdout","output_type":"stream","text":["Epoch [1/5] Finished\n","        Train Loss: 0.1506204385015825, Train Accuracy: 0.95046818727491, Train F1: 0.9505705695440249\n","        Valid Loss: 0.050231976082785325, Valid Accuracy: 0.9847611464968152, Valid F1: 0.9849932413814098\n","Model saved as efficientnet_b4_epoch_1.pt\n"]},{"name":"stderr","output_type":"stream","text":["Epoch [2/5] - Train Loss: 0.008261856622993946: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 41650/41650 [1:03:53<00:00, 10.86it/s]  \n","Epoch [2/5] - Valid Loss: 0.0:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 4656/7850 [24:00<17:49,  2.99it/s]                   "]}],"source":["# wandbì— ëª¨ë¸ì˜ weight & bias, graident ì‹œê°í™”\n","run = wandb.init(project = 'helloyoonjae-', name = 'fastcampus_cv11')\n","run.watch(model, loss_fn, log = 'all', log_graph = True)\n","\n","model, valid_max_accuracy = training_loop(model, trn_loader, valid_loader, loss_fn, optimizer, device, EPOCHS, early_stopping_patience, model_name, run)\n","print(f'Valid Max Accuracy: {valid_max_accuracy}')"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"lkwxRXoBpbaX"},"source":["# 6. Inference & Save File\n","* í…ŒìŠ¤íŠ¸ ì´ë¯¸ì§€ì— ëŒ€í•œ ì¶”ë¡ ì„ ì§„í–‰í•˜ê³ , ê²°ê³¼ íŒŒì¼ì„ ì €ì¥í•©ë‹ˆë‹¤."]},{"cell_type":"markdown","metadata":{},"source":["# pt íŒŒì¼ ë¶ˆëŸ¬ì˜¤ê¸°"]},{"cell_type":"code","execution_count":19,"metadata":{},"outputs":[{"data":{"text/plain":["<All keys matched successfully>"]},"execution_count":19,"metadata":{},"output_type":"execute_result"}],"source":["# ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±\n","model_name = 'tf_efficientnet_b5.ns_jft_in1k'#'tiny_vit_21m_384.dist_in22k_ft_in1k'#'efficientnetv2_rw_m.agc_in1k' #'vit_base_patch16_224.augreg_in1k' #'efficientnet_b3.ra2_in1k'#'densenet121.ra_in1k'# #'resnet101' #'resnet34' # 'resnet50' 'efficientnet-b0', ...\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","model = timm.create_model(\n","    model_name,\n","    pretrained=True,\n","    num_classes=17,\n","    drop_rate=0.2\n",").to(device)\n","\n","# ê°€ì¤‘ì¹˜ ë¡œë“œ\n","model.load_state_dict(torch.load(r'/data/ephemeral/home/notebook/tf_efficientnet_b5.ns_jft_in1k_epoch_3.pt'))\n"]},{"cell_type":"markdown","metadata":{},"source":["# TTA (Test-Time Augmentation) "]},{"cell_type":"code","execution_count":54,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["  0%|          | 0/393 [00:00<?, ?it/s]"]},{"name":"stderr","output_type":"stream","text":["100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 393/393 [03:02<00:00,  2.15it/s]"]},{"name":"stdout","output_type":"stream","text":["                     ID  target\n","0  0008fdb22ddce0ce.jpg       2\n","1  00091bffdffd83de.jpg      12\n","2  00396fbc1f6cc21d.jpg       5\n","3  00471f8038d9c4b6.jpg      12\n","4  00901f504008d884.jpg       2\n"]},{"name":"stderr","output_type":"stream","text":["\n"]}],"source":["import numpy as np\n","import torch\n","from torch.utils.data import DataLoader, Dataset\n","import albumentations as A\n","from albumentations.pytorch import ToTensorV2\n","from tqdm import tqdm\n","from PIL import Image, ImageEnhance\n","import pandas as pd\n","import os\n","import cv2\n","from torch.nn.functional import softmax\n","\n","# Albumentations transform ì •ì˜\n","pre_img_size = 512  # ì‚¬ì „ ì •ì˜ëœ ì´ë¯¸ì§€ í¬ê¸° (í•„ìš”ì— ë”°ë¼ ìˆ˜ì •)\n","\n","transform = A.Compose([\n","    A.LongestMaxSize(max_size=pre_img_size, always_apply=True),\n","    A.PadIfNeeded(min_height=pre_img_size, min_width=pre_img_size, border_mode=0, value=(255, 255, 255)),\n","    # A.Resize(height=pre_img_size, width=pre_img_size),  # ëª¨ë“  ì´ë¯¸ì§€ë¥¼ ê°™ì€ í¬ê¸°ë¡œ ë¦¬ì‚¬ì´ì§•\n","    #A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]), ì—¬ê¸°ì„œ normalizeë„£ìœ¼ë©´ ì™„ì „íˆ ì˜ëª»ë¨ í›„ì— ë˜ í•˜ê¸° ë•Œë¬¸\n","    ToTensorV2(),\n","])\n","\n","\n","class ImageDataset2(Dataset):\n","    def __init__(self, csv_file, path, transform=None):\n","        self.df = pd.read_csv(csv_file)\n","        self.path = path\n","        self.transform = transform\n","\n","    def __len__(self):\n","        return len(self.df)\n","\n","    def __getitem__(self, idx):\n","        real_idx = idx % len(self.df)\n","        name, target = self.df.iloc[real_idx]\n","        img_path = os.path.join(self.path, name)\n","        \n","        try:\n","            img = np.array(Image.open(img_path).convert(\"RGB\"))\n","        except Exception as e:\n","            print(f\"Error loading image {img_path}: {e}\")\n","            img = np.zeros((pre_img_size, pre_img_size, 3), dtype=np.uint8)  # ë¹ˆ ì´ë¯¸ì§€ë¡œ ëŒ€ì²´\n","        \n","        if self.transform:\n","            img = self.transform(image=img)['image']\n","        return img, target\n","\n","# TTA ë³€í™˜ ì •ì˜\n","tta_transforms = [\n","    A.Compose([\n","        # A.GaussNoise(var_limit=(10.0, 800.0), p=0.75),\n","        # A.GaussianBlur(blur_limit=(1, 7), p=0.5),\n","        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n","        ToTensorV2()\n","    ]),\n","    A.Compose([\n","        # A.GaussNoise(var_limit=(10.0, 800.0), p=0.75),\n","        # A.GaussianBlur(blur_limit=(1, 7), p=0.5),\n","        A.HorizontalFlip(p=1.0),\n","        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n","        ToTensorV2()\n","    ]),\n","    A.Compose([\n","        # A.GaussNoise(var_limit=(10.0, 800.0), p=0.75),\n","        # A.GaussianBlur(blur_limit=(1, 7), p=0.5),\n","        A.VerticalFlip(p=1.0),\n","        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n","        ToTensorV2()\n","    ]),\n","    A.Compose([\n","        # A.GaussNoise(var_limit=(10.0, 800.0), p=0.75),\n","        # A.GaussianBlur(blur_limit=(1, 7), p=0.5),\n","        A.Transpose(p=1.0),\n","        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n","        ToTensorV2()\n","    ]),\n","    A.Compose([\n","        A.Transpose(p=1.0),\n","        A.HorizontalFlip(p=1.0),\n","        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n","        ToTensorV2()\n","    ]),\n","    A.Compose([\n","        # A.GaussNoise(var_limit=(10.0, 800.0), p=0.75),\n","        # A.GaussianBlur(blur_limit=(1, 7), p=0.5),\n","        A.Transpose(p=1.0),\n","        A.VerticalFlip(p=1.0),\n","        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n","        ToTensorV2()\n","    ]),\n","    A.Compose([\n","        # A.GaussNoise(var_limit=(10.0, 800.0), p=0.75),\n","        # A.GaussianBlur(blur_limit=(1, 7), p=0.5),\n","        A.Transpose(p=1.0),\n","        A.HorizontalFlip(p=1.0),\n","        A.VerticalFlip(p=1.0),\n","        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n","        ToTensorV2()\n","    ]),\n","    # A.Compose([\n","    #     A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n","    #     ToTensorV2()\n","    # ])  # ì›ë³¸ ì´ë¯¸ì§€\n","]\n","\n","tta_dataset = ImageDataset2(\n","    \"/data/ephemeral/home/datasets_fin/sample_submission.csv\",\n","    \"/data/ephemeral/home/datasets_fin/test/\",\n","    transform=transform\n",")\n","\n","tta_loader = DataLoader(\n","    tta_dataset,\n","    batch_size=BATCH_SIZE,\n","    shuffle=False,\n","    num_workers=0,\n","    pin_memory=True\n",")\n","\n","# TTA inference function\n","def tta_inference(loader, model, device, tta_transforms):\n","    model.eval()\n","    all_outputs = []\n","    \n","    weights = [0.1, 0.1, 0.1, 0.1, 0.2, 0.2, 0.2]  # ê° ë³€í™˜ì— ëŒ€í•œ ê°€ì¤‘ì¹˜ ì„¤ì •\n","    \n","    for images, _ in tqdm(loader):\n","        images = images.to(device).float()  # ì´ë¯¸ì§€ í…ì„œë¥¼ float í˜•ì‹ìœ¼ë¡œ ë³€í™˜\n","        batch_outputs = torch.zeros(images.size(0), 17).to(device)  # 17ì€ í´ë˜ìŠ¤ ìˆ˜\n","        \n","        for weight, tta_transform in zip(weights, tta_transforms):\n","            tta_images = []\n","            for image in images:\n","                tta_image = tta_transform(image=image.permute(1, 2, 0).cpu().numpy().astype(np.float32))['image']\n","                tta_images.append(tta_image.to(device).float())\n","            tta_images = torch.stack(tta_images)\n","            with torch.no_grad():\n","                preds = model(tta_images)\n","                batch_outputs += weight * preds  # ê°€ì¤‘ì¹˜ ì ìš©\n","        \n","        # TTA í‰ê·  ë‚´ê¸° (Soft Voting)\n","        batch_outputs /= len(tta_transforms)\n","        \n","        # ìµœì¢… ì˜ˆì¸¡ ê°’ì„ ë¦¬ìŠ¤íŠ¸ì— ì €ì¥\n","        all_outputs.append(batch_outputs.cpu().numpy())\n","    \n","    # ëª¨ë“  ë°°ì¹˜ì˜ ì˜ˆì¸¡ ê°’ì„ ì—°ê²°\n","    all_outputs = np.concatenate(all_outputs, axis=0)\n","    return all_outputs\n","\n","# TTAë¥¼ ì ìš©í•œ ì˜ˆì¸¡\n","all_outputs = tta_inference(tta_loader, model, device, tta_transforms)\n","preds_list = np.argmax(all_outputs, axis=1)\n","\n","# ì˜ˆì¸¡ ê²°ê³¼ë¥¼ ë°ì´í„°í”„ë ˆì„ìœ¼ë¡œ ì €ì¥\n","pred_df = pd.DataFrame(tta_dataset.df, columns=['ID', 'target'])\n","pred_df['target'] = preds_list\n","\n","# ì œì¶œ í˜•ì‹ íŒŒì¼ì„ ì½ì–´ì™€ ID ì—´ì´ ì¼ì¹˜í•˜ëŠ”ì§€ í™•ì¸\n","sample_submission_df = pd.read_csv(\"/data/ephemeral/home/datasets_fin/sample_submission.csv\")\n","assert (sample_submission_df['ID'] == pred_df['ID']).all()\n","\n","# ì˜ˆì¸¡ ê²°ê³¼ë¥¼ CSV íŒŒì¼ë¡œ ì €ì¥\n","pred_df.to_csv(f\"{model_name}_{pre_img_size}SIZE_{BATCH_SIZE}BATCH_{EPOCHS}EPOCH_0810_TTA_pred.csv\", index=False)\n","print(pred_df.head())\n"]},{"cell_type":"markdown","metadata":{},"source":["# ê¸°ë³¸"]},{"cell_type":"code","execution_count":21,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":12776,"status":"ok","timestamp":1700315185336,"user":{"displayName":"Ynot(ì†¡ì›í˜¸)","userId":"16271863862696372773"},"user_tz":-540},"id":"uRYe6jlPU_Om","outputId":"2a08690c-9ffe-418d-8679-eb9280147110"},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 393/393 [00:31<00:00, 12.36it/s]"]},{"name":"stdout","output_type":"stream","text":["                     ID  target  prob_class_0  prob_class_1  prob_class_2  \\\n","0  0008fdb22ddce0ce.jpg       2  4.766859e-15  3.384005e-13  1.000000e+00   \n","1  00091bffdffd83de.jpg      12  1.044518e-12  9.916116e-14  4.186410e-10   \n","2  00396fbc1f6cc21d.jpg       5  1.049912e-11  2.464661e-14  1.624842e-11   \n","3  00471f8038d9c4b6.jpg      12  5.382270e-11  1.996971e-12  1.053528e-09   \n","4  00901f504008d884.jpg       2  1.564134e-11  1.149906e-13  1.000000e+00   \n","\n","   prob_class_3  prob_class_4  prob_class_5  prob_class_6  prob_class_7  \\\n","0  3.235580e-10  1.328821e-12  1.868789e-10  6.139736e-14  8.582778e-11   \n","1  1.952860e-10  1.170990e-13  7.920273e-11  1.289312e-11  1.243196e-15   \n","2  9.912641e-13  4.954974e-14  1.000000e+00  1.223028e-08  3.500011e-14   \n","3  3.512946e-08  1.965280e-09  6.146787e-09  7.105433e-12  9.930785e-12   \n","4  7.879350e-11  1.485977e-15  7.895594e-12  2.091520e-13  2.060892e-11   \n","\n","   prob_class_8  prob_class_9  prob_class_10  prob_class_11  prob_class_12  \\\n","0  3.295609e-11  4.229628e-11   1.102311e-13   1.357595e-13   1.473029e-17   \n","1  4.504691e-11  3.512827e-13   1.549837e-09   2.288463e-16   1.000000e+00   \n","2  4.058532e-12  6.140948e-12   7.641075e-16   2.649696e-12   1.053594e-16   \n","3  7.823024e-10  1.708532e-10   1.401625e-11   5.453113e-12   9.999989e-01   \n","4  4.790274e-11  4.436024e-13   3.054688e-13   1.447578e-12   2.137790e-16   \n","\n","   prob_class_13  prob_class_14  prob_class_15  prob_class_16  \n","0   5.333748e-13   5.332741e-13   9.049057e-13   3.100361e-09  \n","1   1.878281e-09   8.377726e-14   2.351223e-10   4.754132e-10  \n","2   1.011928e-12   2.182986e-10   1.426141e-13   1.412500e-12  \n","3   9.065695e-07   3.021908e-09   7.053259e-08   2.945893e-09  \n","4   2.973281e-12   8.398193e-13   1.891732e-12   4.333655e-08  \n"]},{"name":"stderr","output_type":"stream","text":["\n"]}],"source":["import pandas as pd\n","import torch\n","from torch.nn.functional import softmax\n","from tqdm import tqdm\n","\n","# ëª¨ë¸ í‰ê°€ ëª¨ë“œ ì„¤ì •\n","model.eval()\n","preds_list = []\n","probs_list = []  # í™•ë¥ ì„ ì €ì¥í•  ë¦¬ìŠ¤íŠ¸\n","\n","# DataLoaderë¥¼ í†µí•´ ì´ë¯¸ì§€ë¥¼ ë°˜ë³µ ì²˜ë¦¬\n","for image, _ in tqdm(tst_loader):\n","    image = image.to(device)\n","    with torch.no_grad():\n","        preds = model(image)\n","        probs = softmax(preds, dim=1)  # Softmaxë¥¼ ì ìš©í•˜ì—¬ í™•ë¥  ê³„ì‚°\n","    preds_list.extend(preds.argmax(dim=1).detach().cpu().numpy())\n","    probs_list.extend(probs.detach().cpu().numpy())  # í™•ë¥  ê°’ ì €ì¥\n","\n","# ì˜ˆì¸¡ ê²°ê³¼ì™€ í™•ë¥ ì„ ë°ì´í„°í”„ë ˆì„ìœ¼ë¡œ ì €ì¥\n","pred_df = pd.DataFrame({\n","    'ID': tst_dataset.df['ID'],\n","    'target': preds_list\n","})\n","# í™•ë¥ ì„ ë°ì´í„°í”„ë ˆì„ì— ì¶”ê°€\n","for i in range(num_classes):\n","    pred_df[f'prob_class_{i}'] = [probs[i] for probs in probs_list]\n","\n","# ì œì¶œ í˜•ì‹ íŒŒì¼ì„ ì½ì–´ì™€ ID ì—´ì´ ì¼ì¹˜í•˜ëŠ”ì§€ í™•ì¸\n","sample_submission_df = pd.read_csv(\"/data/ephemeral/home/datasets_fin/sample_submission.csv\")\n","assert (sample_submission_df['ID'] == pred_df['ID']).all()\n","\n","# ì˜ˆì¸¡ ê²°ê³¼ì™€ í™•ë¥ ì„ CSV íŒŒì¼ë¡œ ì €ì¥\n","pred_df.to_csv(f\"{model_name}_{img_size}SIZE_{BATCH_SIZE}BATCH_{EPOCHS}EPOCH_0810_pred.csv\", index=False)\n","print(pred_df.head())\n"]},{"cell_type":"markdown","metadata":{},"source":["# 976 ì •ë‹µì§€ì™€ ë¹„êµ"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import pandas as pd\n","import matplotlib.pyplot as plt\n","from PIL import Image\n","import os\n","from sklearn.metrics import f1_score\n","import matplotlib.font_manager as fm\n","from matplotlib.font_manager import FontProperties\n","\n","\n","def load_predictions(file_paths):\n","    \"\"\"ì—¬ëŸ¬ ëª¨ë¸ì˜ ì˜ˆì¸¡ ê²°ê³¼ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤.\"\"\"\n","    predictions = {}\n","    for path in file_paths:\n","        model_name = os.path.basename(path).split('.')[0]\n","        df = pd.read_csv(path)\n","        # ID ì»¬ëŸ¼ ì¤‘ë³µ ì œê±° ë° ì˜ˆì¸¡ ì»¬ëŸ¼ ì´ë¦„ ë³€ê²½\n","        df = df[['ID', 'target']].rename(columns={'target': model_name})\n","        predictions[model_name] = df\n","    return predictions\n","\n","def find_different_predictions(predictions):\n","    \"\"\"ëª¨ë“  ëª¨ë¸ì—ì„œ ë‹¤ë¥´ê²Œ ì˜ˆì¸¡í•œ í•­ëª©ì„ ì°¾ìŠµë‹ˆë‹¤.\"\"\"\n","    # ëª¨ë“  ì˜ˆì¸¡ì„ í•˜ë‚˜ì˜ DataFrameìœ¼ë¡œ ë³‘í•©\n","    all_predictions = predictions[list(predictions.keys())[0]]\n","    for model, df in list(predictions.items())[1:]:\n","        all_predictions = pd.merge(all_predictions, df, on='ID', suffixes=('', f'_{model}'))\n","    \n","    # ì˜ˆì¸¡ ì»¬ëŸ¼ë§Œ ì„ íƒ\n","    prediction_columns = [col for col in all_predictions.columns if col != 'ID']\n","    \n","    # ì˜ˆì¸¡ì´ ë‹¤ë¥¸ í–‰ë§Œ ì„ íƒ\n","    different_predictions = all_predictions[all_predictions[prediction_columns].nunique(axis=1) > 1]\n","    return different_predictions\n","\n","def calculate_macro_f1(ground_truth, predictions):\n","    \"\"\"Macro F1 ì ìˆ˜ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.\"\"\"\n","    return f1_score(ground_truth, predictions, average='macro')\n","\n","def plot_error_distribution(ground_truth, predictions):\n","    \"\"\"ê° í´ë˜ìŠ¤ë³„ ì˜¤ë¥˜ ì˜ˆì¸¡ ê°œìˆ˜ë¥¼ bar plotìœ¼ë¡œ í‘œì‹œí•©ë‹ˆë‹¤.\"\"\"\n","    # í´ë˜ìŠ¤ ì´ë¦„ ì •ì˜\n","    class_names = {\n","        0: \"ê³„ì¢Œë²ˆí˜¸(ì†ê¸€ì”¨)\", 1: \"ì„ì‹ ì¶œì‚° ì§„ë£Œë¹„ ì§€ê¸‰ ì‹ ì²­ì„œ\", 2: \"ìë™ì°¨ ê³„ê¸°íŒ\", 3: \"ì…í‡´ì› í™•ì¸ì„œ\", 4: \"ì§„ë‹¨ì„œ\", \n","        5: \"ìš´ì „ë©´í—ˆì¦\", 6: \"ì§„ë£Œë¹„ì˜ìˆ˜ì¦\", 7: \"í†µì›/ì§„ë£Œ í™•ì¸ì„œ\", 8: \"ì£¼ë¯¼ë“±ë¡ì¦\", 9: \"ì—¬ê¶Œ\", \n","        10: \"ì§„ë£Œë¹„ ë‚©ì… í™•ì¸ì„œ\", 11: \"ì•½ì œë¹„ ì˜ìˆ˜ì¦\", 12: \"ì²˜ë°©ì „\", 13: \"ì´ë ¥ì„œ\", 14: \"ì†Œê²¬ì„œ\", \n","        15: \"ìë™ì°¨ ë“±ë¡ì¦\", 16: \"ìë™ì°¨ ë²ˆí˜¸íŒ\"\n","    }\n","\n","    error_counts = {i: 0 for i in range(len(class_names))}  # ëª¨ë“  í´ë˜ìŠ¤ì— ëŒ€í•´ ì´ˆê¸°í™”\n","    for gt, pred in zip(ground_truth, predictions):\n","        if gt != pred:\n","            error_counts[gt] += 1\n","    \n","    classes = sorted(error_counts.keys())\n","    counts = [error_counts[c] for c in classes]\n","    class_labels = [f\"{c}\\n{class_names[c]}\" for c in classes]\n","    \n","    # ë‚˜ëˆ” í°íŠ¸ ì„¤ì •\n","    # font_path = './font/NanumGothic.otf'\n","    # font_prop = FontProperties(fname=font_path)\n","    \n","    plt.figure(figsize=(20, 10))\n","    bars = plt.bar(class_labels, counts)\n","    plt.title(\"ì˜¤ë¥˜ ì˜ˆì¸¡ ê°œìˆ˜ (í´ë˜ìŠ¤ë³„)\", fontsize=16)# fontproperties=font_prop, \n","    plt.xlabel(\"í´ë˜ìŠ¤\", fontsize=14)# fontproperties=font_prop, \n","    plt.ylabel(\"ì˜¤ë¥˜ ê°œìˆ˜\", fontsize=14)# fontproperties=font_prop, \n","    plt.xticks(rotation=45, ha='center')\n","    \n","    # xì¶• ë ˆì´ë¸”ì— í°íŠ¸ ì ìš©\n","    ax = plt.gca()\n","    ax.set_xticklabels(class_labels, fontsize=10) # fontproperties=font_prop, \n","    \n","    plt.tight_layout()\n","    \n","    # ê° ë§‰ëŒ€ ìœ„ì— ê°’ í‘œì‹œ\n","    for bar in bars:\n","        height = bar.get_height()\n","        plt.text(bar.get_x() + bar.get_width()/2., height,\n","                 f'{height}',\n","                 ha='center', va='bottom')# fontproperties=font_prop, \n","    \n","    plt.show()  \n","    \n","    \n","def display_images_and_predictions(different_predictions, image_dir, ground_truth_name, prediction_name, class_filter=None, max_images_per_class=3):\n","    \"\"\"ë‹¤ë¥´ê²Œ ì˜ˆì¸¡ëœ í•­ëª©ì˜ ì´ë¯¸ì§€ì™€ ì˜ˆì¸¡ ê²°ê³¼ë¥¼ í‘œì‹œí•©ë‹ˆë‹¤.\"\"\"\n","    # ë‚˜ëˆ” í°íŠ¸ ì„¤ì •\n","    # font_path = './font/NanumGothic.otf'\n","    # font_prop = FontProperties(fname=font_path)\n","    class_names = {\n","        0: \"ê³„ì¢Œë²ˆí˜¸(ì†ê¸€ì”¨)\", 1: \"ì„ì‹ ì¶œì‚° ì§„ë£Œë¹„ ì§€ê¸‰ ì‹ ì²­ì„œ\", 2: \"ìë™ì°¨ ê³„ê¸°íŒ\", 3: \"ì…í‡´ì› í™•ì¸ì„œ\", 4: \"ì§„ë‹¨ì„œ\", \n","        5: \"ìš´ì „ë©´í—ˆì¦\", 6: \"ì§„ë£Œë¹„ì˜ìˆ˜ì¦\", 7: \"í†µì›/ì§„ë£Œ í™•ì¸ì„œ\", 8: \"ì£¼ë¯¼ë“±ë¡ì¦\", 9: \"ì—¬ê¶Œ\", \n","        10: \"ì§„ë£Œë¹„ ë‚©ì… í™•ì¸ì„œ\", 11: \"ì•½ì œë¹„ ì˜ìˆ˜ì¦\", 12: \"ì²˜ë°©ì „\", 13: \"ì´ë ¥ì„œ\", 14: \"ì†Œê²¬ì„œ\", \n","        15: \"ìë™ì°¨ ë“±ë¡ì¦\", 16: \"ìë™ì°¨ ë²ˆí˜¸íŒ\"\n","    }\n","    \n","    # í´ë˜ìŠ¤ë³„ë¡œ ì´ë¯¸ì§€ ë¶„ë¥˜\n","    class_images = {c: [] for c in range(len(class_names))}\n","    for idx, row in different_predictions.iterrows():\n","        ground_truth = row[ground_truth_name]\n","        if class_filter is None or ground_truth in class_filter:\n","            class_images[ground_truth].append(row)\n","    \n","    # í‘œì‹œí•  ì´ë¯¸ì§€ ì„ íƒ\n","    images_to_display = []\n","    for c, imgs in class_images.items():\n","        if class_filter is None or c in class_filter:\n","            images_to_display.extend(imgs[:max_images_per_class])\n","    \n","    # ì´ë¯¸ì§€ í‘œì‹œ\n","    num_images = len(images_to_display)\n","    rows = (num_images - 1) // 3 + 1\n","    fig, axs = plt.subplots(rows, 4, figsize=(20, 5*rows))\n","    \n","    for i, row in enumerate(images_to_display):\n","        ax = axs[i//4, i%4] if rows > 1 else axs[i%4]\n","        \n","        image_id = row['ID']\n","        ground_truth = row[ground_truth_name]\n","        prediction = row[prediction_name]\n","        image_path = os.path.join(image_dir, image_id)\n","        \n","        if os.path.exists(image_path):\n","            img = Image.open(image_path)\n","            img = img.resize((300, 300), Image.LANCZOS)\n","            ax.imshow(img)\n","            ax.axis('off')\n","            id = f\"ID: {image_id}\"\n","            comparison_text = f\"ì •ë‹µ: {ground_truth} ({class_names[ground_truth]})\\nì˜ˆì¸¡: {prediction} ({class_names[prediction]})\"\n","            ax.set_title(f\"{id}\\n{comparison_text}\", fontsize=10) # fontproperties=font_prop, \n","        else:\n","            ax.text(0.5, 0.5, f\"Image not found\\nfor ID: {image_id}\", ha='center', va='center')\n","    \n","    # ë¹ˆ ì„œë¸Œí”Œë¡¯ ì œê±°\n","    for i in range(num_images, rows*4):\n","        ax = axs[i//4, i%4] if rows > 1 else axs[i%4]\n","        ax.axis('off')\n","    \n","    plt.tight_layout()\n","    plt.show()\n","    \n","    print(f\"ì´ {num_images}ê°œì˜ ì´ë¯¸ì§€ê°€ í‘œì‹œë˜ì—ˆìŠµë‹ˆë‹¤.\")    \n","\n","# ì˜ˆì¸¡ íŒŒì¼ ê²½ë¡œ\n","file_paths = [\n","    \"/data/ephemeral/home/notebook/output_9760.csv\",\n","    # \"/data/ephemeral/home/notebook/efficientnet_b2.ra_in1k_384SIZE_32BATCH_2EPOCH_100_0806_PADOS_pred.csv\", # ìˆ˜ì •í•´ì•¼í•  ë¶€ë¶„\n","    \"/data/ephemeral/home/notebook/efficientnet_b4_512SIZE_8BATCH_5EPOCH_0810_TTA_pred.csv\"\n","]\n","\n","\n","# ì´ë¯¸ì§€ ë””ë ‰í† ë¦¬ ê²½ë¡œ\n","image_dir = \"/data/ephemeral/home/datasets_fin/test\"\n","\n","# ì˜ˆì¸¡ ë¡œë“œ\n","predictions = load_predictions(file_paths)\n","\n","# ë‹¤ë¥¸ ì˜ˆì¸¡ ì°¾ê¸°\n","different_predictions = find_different_predictions(predictions)\n","\n","# # ê²°ê³¼ ì¶œë ¥\n","# print(different_predictions)\n","\n","# Macro F1 ì ìˆ˜ ê³„ì‚°\n","ground_truth_name = os.path.basename(file_paths[0]).split('.')[0]\n","prediction_name = os.path.basename(file_paths[-1]).split('.')[0]\n","ground_truth = predictions[ground_truth_name][ground_truth_name]\n","prediction = predictions[prediction_name][prediction_name]\n","macro_f1 = calculate_macro_f1(ground_truth, prediction)\n","print(f\"Macro F1 Score: {macro_f1}\")\n","\n","# ì˜¤ë¥˜ ë¶„í¬ ê·¸ë˜í”„ í‘œì‹œ\n","plot_error_distribution(ground_truth, prediction)\n","\n","\n","# íŠ¹ì • í´ë˜ìŠ¤(ì˜ˆ: 0, 1, 2)ì— ëŒ€í•´ í´ë˜ìŠ¤ë‹¹ ìµœëŒ€ 3ê°œì”© ì´ë¯¸ì§€ í‘œì‹œ\n","display_images_and_predictions(different_predictions, image_dir, ground_truth_name, prediction_name, class_filter=[1,2], max_images_per_class=3)"]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":0}
