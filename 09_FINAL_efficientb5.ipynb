{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{"id":"OliaDaX_lwou"},"source":["# **📄 Document type classification baseline code**\n","> 문서 타입 분류 대회에 오신 여러분 환영합니다! 🎉     \n","> 아래 baseline에서는 ResNet 모델을 로드하여, 모델을 학습 및 예측 파일 생성하는 프로세스에 대해 알아보겠습니다.\n","\n","## Contents\n","- Prepare Environments\n","- Import Library & Define Functions\n","- Hyper-parameters\n","- Load Data\n","- Train Model\n","- Inference & Save File\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import zipfile\n","import os\n","\n","# 압축 파일 경로와 압축 해제할 디렉토리 설정\n","zip_file_path = '/data/ephemeral/home/datasets_fin/train_wdnx4.zip'\n","extract_dir = '/data/ephemeral/home/datasets_fin/train_wdnx4'\n","\n","# 압축 파일 열기\n","with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n","    # 모든 파일 압축 해제\n","    zip_ref.extractall(extract_dir)\n","\n","print(f'Files extracted to {extract_dir}')\n"]},{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":9396,"status":"ok","timestamp":1700314592802,"user":{"displayName":"Ynot(송원호)","userId":"16271863862696372773"},"user_tz":-540},"id":"3BaoIkv5Xwa0"},"outputs":[],"source":["import os\n","import time\n","import random\n","\n","import timm\n","import torch\n","import albumentations as A\n","import pandas as pd\n","import numpy as np\n","import torch.nn as nn\n","from albumentations.pytorch import ToTensorV2\n","from torch.optim import Adam, AdamW\n","from torchvision import transforms\n","from torch.utils.data import Dataset, DataLoader\n","from PIL import Image\n","from tqdm import tqdm\n","from sklearn.metrics import accuracy_score, f1_score"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["import json\n","import wandb\n","from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay # sklearn 내 confusion matrix 계산 함수\n","import matplotlib.pyplot as plt # 시각화를 위한 라이브러리\n","import torchvision.transforms as T # 이미지 변환을 위한 모듈\n","from torch.optim.lr_scheduler import ReduceLROnPlateau\n","from PIL import Image, ImageEnhance, ImageOps\n","import numpy as np\n","import cv2\n","import albumentations as A\n","from albumentations.pytorch import ToTensorV2\n","from torch.cuda.amp import autocast, GradScaler\n","from sklearn.metrics import accuracy_score, f1_score\n"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["# 시드를 고정합니다.\n","SEED = 42\n","os.environ['PYTHONHASHSEED'] = str(SEED)\n","random.seed(SEED)\n","np.random.seed(SEED)\n","torch.manual_seed(SEED)\n","torch.cuda.manual_seed(SEED)\n","torch.cuda.manual_seed_all(SEED)\n","torch.backends.cudnn.benchmark = True"]},{"cell_type":"code","execution_count":58,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["NanumGothic\n"]}],"source":["from matplotlib import font_manager, rc\n","import matplotlib.pyplot as plt\n","\n","font_path = '/usr/share/fonts/truetype/nanum/NanumGothic.ttf'\n","font_name = font_manager.FontProperties(fname=font_path).get_name()\n","print(font_name)\n","plt.rc('font', family=font_name)"]},{"cell_type":"markdown","metadata":{},"source":["# 0.97 submission 제출 파일을 가지고 test셋 분포 확인: train과 동일함"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import pandas as pd\n","\n","# CSV 파일 로드\n","meta = pd.read_csv(r'/data/ephemeral/home/datasets_fin/meta.csv')\n","data = pd.read_csv(r'/data/ephemeral/home/notebook/EASY_pred_ocr_0806_9514_34714.csv')\n","merged_data = pd.merge(data, meta, on='target', how='left')\n","\n","\n","# 'target' 컬럼의 값에 대한 빈도수 계산\n","class_counts = merged_data[['target','class_name_kr']].value_counts().sort_index()\n","\n","# 막대 그래프로 시각화\n","plt.figure(figsize=(10, 6))\n","class_counts.plot(kind='bar', color='skyblue')\n","plt.title('Class Distribution in Testset Data')\n","plt.xlabel('Class Label')\n","plt.ylabel('Frequency')\n","plt.xticks(rotation=90)  # 클래스 레이블이 수평으로 표시되도록 설정\n","plt.grid(axis='y', linestyle='--', alpha=0.7)\n","plt.show()\n"]},{"cell_type":"markdown","metadata":{},"source":["# 이미지 크기 확인"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import matplotlib.pyplot as plt\n","from collections import Counter\n","import os\n","from PIL import Image\n","\n","# 이미지 크기 추출 함수\n","def check_image_sizes(directory):\n","    sizes = []\n","    for filename in os.listdir(directory):\n","        if filename.endswith(('.jpg', '.jpeg', '.png')):\n","            filepath = os.path.join(directory, filename)\n","            with Image.open(filepath) as img:\n","                sizes.append(img.size)\n","    return sizes\n","\n","# 이미지 크기별 빈도 계산\n","directory = '/data/ephemeral/home/datasets_fin/combined_train/train'\n","sizes = check_image_sizes(directory)\n","size_counts = Counter(sizes)\n","\n","# 크기와 빈도 데이터 분리\n","sizes, counts = zip(*size_counts.items())\n","\n","# 시각화\n","plt.figure(figsize=(10, 6))\n","plt.barh(range(len(counts)), counts, tick_label=[f'{w}x{h}' for w, h in sizes])\n","plt.xlabel('Number of Images')\n","plt.ylabel('Image Size (width x height)')\n","plt.title('Image Size Distribution')\n","plt.show()\n","\n","\n","directory2 = '/data/ephemeral/home/datasets_fin/test'\n","sizes2 = check_image_sizes(directory2)\n","size_counts2 = Counter(sizes2)\n","\n","# 크기와 빈도 데이터 분리\n","sizes2, counts2 = zip(*size_counts2.items())\n","\n","# 시각화\n","plt.figure(figsize=(10, 6))\n","plt.barh(range(len(counts2)), counts2, tick_label=[f'{w}x{h}' for w, h in sizes2])\n","plt.xlabel('Number of Images')\n","plt.ylabel('Image Size (width x height)')\n","plt.title('Image Size Distribution')\n","plt.show()\n"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"Wjom43UvoXcx"},"source":["## 3. Hyper-parameters\n","* 학습 및 추론에 필요한 하이퍼파라미터들을 정의합니다."]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mhelloyoonjae\u001b[0m (\u001b[33mhelloyoonjae-\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"]},{"data":{"text/html":["wandb version 0.17.6 is available!  To upgrade, please run:\n"," $ pip install wandb --upgrade"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Tracking run with wandb version 0.17.5"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Run data is saved locally in <code>/data/ephemeral/home/notebook/wandb/run-20240810_221436-sxshmb5m</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Syncing run <strong><a href='https://wandb.ai/helloyoonjae-/helloyoonjae-/runs/sxshmb5m' target=\"_blank\">fastcampus_cv11</a></strong> to <a href='https://wandb.ai/helloyoonjae-/helloyoonjae-' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View project at <a href='https://wandb.ai/helloyoonjae-/helloyoonjae-' target=\"_blank\">https://wandb.ai/helloyoonjae-/helloyoonjae-</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View run at <a href='https://wandb.ai/helloyoonjae-/helloyoonjae-/runs/sxshmb5m' target=\"_blank\">https://wandb.ai/helloyoonjae-/helloyoonjae-/runs/sxshmb5m</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/helloyoonjae-/helloyoonjae-/runs/sxshmb5m?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"],"text/plain":["<wandb.sdk.wandb_run.Run at 0x7f4dff5955a0>"]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["wandb.init(project = 'helloyoonjae-', name = 'fastcampus_cv11')"]},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":436,"status":"ok","timestamp":1700315112439,"user":{"displayName":"Ynot(송원호)","userId":"16271863862696372773"},"user_tz":-540},"id":"KByfAeRmXwYk"},"outputs":[],"source":["# device\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","\n","# data config\n","data_path = '/data/ephemeral/home/datasets_fin/combined_train/train'\n","\n","# model config\n","model_name = 'tf_efficientnet_b5.ns_jft_in1k'#'tiny_vit_21m_384.dist_in22k_ft_in1k'#'efficientnetv2_rw_m.agc_in1k' #'vit_base_patch16_224.augreg_in1k' #'efficientnet_b3.ra2_in1k'#'densenet121.ra_in1k'# #'resnet101' #'resnet34' # 'resnet50' 'efficientnet-b0', ...\n","\n","# B0: 224 x 224\n","# B1: 240 x 240\n","# B2: 260 x 260\n","# B3: 300 x 300\n","# B4: 380 x 380\n","# B5: 456 x 456\n","# B6: 528 x 528\n","# B7: 600 x 600\n","\n","# training config\n","pre_img_size = 600\n","img_size = 456 #224 #256\n","LR = 5e-4\n","EPOCHS = 5\n","BATCH_SIZE = 8\n","num_workers = 4\n","early_stopping_patience = 5  # Early Stopping 설정\n","augment_ratio = 200"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"amum-FlIojc6"},"source":["## 4. Load Data\n","* 학습, 테스트 데이터셋과 로더를 정의합니다."]},{"cell_type":"code","execution_count":7,"metadata":{"executionInfo":{"elapsed":255,"status":"ok","timestamp":1700315066028,"user":{"displayName":"Ynot(송원호)","userId":"16271863862696372773"},"user_tz":-540},"id":"kTECBJfVTbdl"},"outputs":[],"source":["# one epoch 학습을 위한 함수입니다.\n","def train_one_epoch(loader, model, optimizer, loss_fn, device):\n","    model.train()\n","    train_loss = 0\n","    preds_list = []\n","    targets_list = []\n","\n","    pbar = tqdm(loader)\n","    for image, targets in pbar:\n","        image = image.to(device)\n","        targets = targets.to(device)\n","\n","        model.zero_grad(set_to_none=True)\n","\n","        preds = model(image)\n","        loss = loss_fn(preds, targets)\n","        loss.backward()\n","        optimizer.step()\n","\n","        train_loss += loss.item()\n","        preds_list.extend(preds.argmax(dim=1).detach().cpu().numpy())\n","        targets_list.extend(targets.detach().cpu().numpy())\n","\n","        pbar.set_description(f\"Loss: {loss.item():.4f}\")\n","\n","    train_loss /= len(loader)\n","    train_acc = accuracy_score(targets_list, preds_list)\n","    train_f1 = f1_score(targets_list, preds_list, average='macro')\n","\n","    ret = {\n","        \"train_loss\": train_loss,\n","        \"train_acc\": train_acc,\n","        \"train_f1\": train_f1,\n","    }\n","\n","    return ret"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[],"source":["# 데이터셋 클래스를 정의합니다.\n","class ImageDataset(Dataset):\n","    def __init__(self, csv, path, transform=None, oversample=False, augment_ratio=1):\n","        self.df = pd.read_csv(csv)\n","        self.path = path\n","        self.transform = transform\n","        self.oversample = oversample\n","        self.augment_ratio = augment_ratio\n","\n","        # 클래스간 불균형 해소를 위한 샘플 증식\n","        if self.oversample:\n","            # 각 클래스별로 데이터 수 계산\n","            class_counts = np.bincount(self.df.values[:, 1].astype(int))\n","\n","            # 각 클래스별로 증식할 횟수 설정 (이 예제에서는 최대 데이터 수에 맞춤)\n","            max_class_count = max(class_counts)\n","            oversample_factors = [max_class_count // count for count in class_counts]\n","            # Class 3, 7 가중치 2로 변경\n","            # oversample_factors[3] = 2\n","            # oversample_factors[7] = 2 \n","            # oversample_factors[14] = 3 \n","\n","            # 각 클래스별로 데이터를 증식한 새로운 데이터 프레임 생성\n","            oversampled_data = [self.df.values[self.df.values[:, 1] == cls].repeat(factor, axis=0) for cls, factor in enumerate(oversample_factors)]\n","            oversampled_data = np.vstack(oversampled_data)\n","\n","            self.df = pd.DataFrame(oversampled_data, columns=self.df.columns)\n","\n","    def __len__(self):\n","        return len(self.df) * self.augment_ratio\n","\n","    def __getitem__(self, idx):\n","        real_idx = idx % len(self.df)\n","        name, target = self.df.iloc[real_idx]\n","        img = np.array(Image.open(os.path.join(self.path, name)).convert(\"RGB\"))\n","        \n","        if self.transform:\n","            img = self.transform(image=img)['image']\n","        return img, target\n","    \n","    \n","# meta.csv 파일 읽기\n","meta_data = pd.read_csv('/data/ephemeral/home/datasets_fin/meta.csv')\n","label_to_class_name = dict(zip(meta_data['target'], meta_data['class_name']))"]},{"cell_type":"markdown","metadata":{},"source":["# 전처리"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[],"source":["import numpy as np\n","from PIL import Image\n","import albumentations as A\n","\n","class Crop(A.ImageOnlyTransform):\n","    def __init__(self, crop_position='top', always_apply=False, p=1.0):\n","        super(Crop, self).__init__(always_apply, p)\n","        self.crop_position = crop_position\n","\n","    def apply(self, img, **params):\n","        # 이미지를 PIL 이미지로 변환 (Albumentations은 NumPy 배열을 사용)\n","        if isinstance(img, np.ndarray):\n","            img = Image.fromarray(img)\n","\n","        width, height = img.size\n","\n","        if self.crop_position == 'top':\n","            img = img.crop((0, 0, width, height // 2))\n","        elif self.crop_position == 'bottom':\n","            img = img.crop((0, height // 2, width, height))\n","        else:\n","            raise ValueError(\"crop_position must be 'top' or 'bottom'\")\n","\n","        # PIL 이미지를 다시 NumPy 배열로 변환\n","        img = np.array(img)\n","        return img\n"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[],"source":["class Adjust(A.ImageOnlyTransform):\n","    def __init__(self, always_apply=False, p=1.0):\n","        super(Adjust, self).__init__(always_apply, p)\n","\n","    def apply(self, img, **params):\n","        \n","        # 이미지를 PIL 이미지로 변환 (Albumentations은 NumPy 배열을 사용)\n","        if isinstance(img, np.ndarray):\n","            # img = Image.fromarray(img)\n","            img = Image.fromarray(img.astype(np.uint8))\n","        # PIL 이미지를 다시 NumPy 배열로 변환\n","        img = np.array(img)               \n","                \n","        gray_image = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n","        average_brightness = np.mean(gray_image)\n","        # print(average_brightness)\n","        if average_brightness < 160:\n","            img_pil = Image.fromarray(img)\n","            # Adjust brightness\n","            enhancer = ImageEnhance.Brightness(img_pil)\n","            img_pil = enhancer.enhance(1.5)\n","            \n","            # Adjust contrast\n","            enhancer = ImageEnhance.Contrast(img_pil)\n","            img_pil = enhancer.enhance(2.0)\n","            \n","            img = np.array(img_pil)\n","        \n","        return img"]},{"cell_type":"markdown","metadata":{},"source":["# 정규화 수치를 기존것을 대체하여 train데이터에 대해 설정하기: 효과가 없어서 안하는 것으로 함"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import numpy as np\n","import torch\n","from torch.utils.data import DataLoader\n","from torchvision.datasets import ImageFolder\n","import albumentations as A\n","from albumentations.pytorch import ToTensorV2\n","from tqdm import tqdm\n","\n","# Albumentations transform 정의\n","pre_img_size = pre_img_size  # 사전 정의된 이미지 크기 (필요에 따라 수정)\n","img_size = pre_img_size\n","augment_ratio = augment_ratio\n","\n","transform = A.Compose([\n","    A.LongestMaxSize(max_size=pre_img_size, always_apply=True),\n","    A.PadIfNeeded(min_height=pre_img_size, min_width=pre_img_size, border_mode=0, value=(255, 255, 255)),\n","    A.Resize(height=img_size, width=img_size),\n","    \n","    A.OneOf([\n","        A.GaussNoise(var_limit=(10.0, 800.0), p=0.75),\n","        A.GaussianBlur(blur_limit=(1, 7), p=0.5)\n","    ], p=0.75),\n","    # A.RandomRotate90(p=0.5),\n","    A.HorizontalFlip(p=0.75),\n","    A.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2, p=0.5),\n","    A.ShiftScaleRotate(shift_limit=0.1, scale_limit=0.1, rotate_limit=30, p=0.25),\n","    # RotateBy30(p=0.75),\n","    A.CoarseDropout(max_holes=6, max_height=32, max_width=32, p=0.5),\n","    A.ElasticTransform(alpha=1, sigma=30, alpha_affine=30, p=0.5),\n","    A.HueSaturationValue(hue_shift_limit=20, sat_shift_limit=30, val_shift_limit=20, p=0.5),\n","    A.RandomBrightnessContrast(brightness_limit=0.2, contrast_limit=0.2, p=0.1),\n","    # A.Rotate(limit=30, p=0.75),\n","    A.CLAHE(clip_limit=2.0, tile_grid_size=(8, 8), p=0.5),\n","    A.MotionBlur(blur_limit=5, p=0.5),\n","    A.OpticalDistortion(p=0.5),\n","    A.Transpose(p=0.5),\n","    # A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n","    ToTensorV2(),\n","])\n","\n","class CustomDataset(ImageFolder):\n","    def __init__(self, root, transform=None, augment_ratio=1):\n","        super().__init__(root, transform=None)\n","        self.custom_transform = transform\n","        self.augment_ratio = augment_ratio\n","\n","    def __getitem__(self, index):\n","        path, target = self.samples[index % len(self.samples)]\n","        sample = self.loader(path).convert(\"RGB\")  # PIL 이미지로 변환\n","        if self.custom_transform is not None:\n","            sample = np.array(sample)  # Albumentations는 NumPy 배열을 사용\n","            sample = self.custom_transform(image=sample)[\"image\"]  # 변환 적용\n","        return sample, target\n","    \n","    def __len__(self):\n","        return len(self.samples) * self.augment_ratio\n","\n","# 데이터셋 루트 경로 설정 (필요에 따라 수정)\n","dataset_root = '/data/ephemeral/home/datasets_fin/combined_train/'\n","\n","# 데이터셋 로드\n","dataset = CustomDataset(root=dataset_root, transform=transform)\n","dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n","\n","# 채널별 평균과 표준 편차 계산\n","mean = 0.0\n","std = 0.0\n","nb_samples = 0.0\n","\n","for data, _ in tqdm(dataloader, desc=\"Calculating mean and std\", unit=\"batch\"):\n","    data = data.float()/ 255.0  # float 타입으로 변환하고 0-1 범위로 스케일링\n","    batch_samples = data.size(0)\n","    data = data.view(batch_samples, data.size(1), -1)\n","    mean += data.mean(2).sum(0)\n","    std += data.std(2).sum(0)\n","    nb_samples += batch_samples\n","\n","mean /= nb_samples\n","std /= nb_samples\n","\n","print(f'Mean: {mean}')\n","print(f'Std: {std}')\n","\n","# 계산된 값을 사용하여 정규화\n","custom_transform = transforms.Compose([\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean=mean, std=std),\n","])\n","\n","# 정규화된 데이터셋 로드\n","custom_dataset = CustomDataset(root=dataset_root, transform=custom_transform)\n"]},{"cell_type":"code","execution_count":11,"metadata":{"executionInfo":{"elapsed":1,"status":"ok","timestamp":1700315112439,"user":{"displayName":"Ynot(송원호)","userId":"16271863862696372773"},"user_tz":-540},"id":"llh5C7ZKoq2S"},"outputs":[],"source":["# https://demo.albumentations.ai/\n","import albumentations as A\n","from albumentations.pytorch import ToTensorV2\n","\n","# horizontal_flip, vertical_flip, double_flip, transpose 등 변환 정의\n","horizontal_flip = A.HorizontalFlip(p=1)\n","vertical_flip = A.VerticalFlip(p=1)\n","double_flip = A.Compose([\n","    A.HorizontalFlip(p=1),\n","    A.VerticalFlip(p=1),\n","])\n","transpose = A.Transpose(p=1)\n","transpose_hflip = A.Compose([\n","    A.Transpose(p=1), \n","    A.HorizontalFlip(p=1),\n","])\n","transpose_vflip = A.Compose([\n","    A.Transpose(p=1),\n","    A.VerticalFlip(p=1),\n","])\n","transpose_dflip = A.Compose([\n","    A.Transpose(p=1),  \n","    A.HorizontalFlip(p=1),\n","    A.VerticalFlip(p=1),\n","])\n","\n","# Augmentation을 위한 transform 코드\n","trn_transform = A.Compose([\n","    A.LongestMaxSize(max_size=pre_img_size, always_apply=True),\n","    A.PadIfNeeded(min_height=pre_img_size, min_width=pre_img_size, border_mode=0, value=(255, 255, 255)),\n","    A.OneOf([\n","        A.GaussNoise(var_limit=(10.0, 800.0), p=1),\n","        A.GaussianBlur(blur_limit=(1, 7), p=1),\n","        A.MotionBlur(blur_limit=(3, 7), p=1),\n","        A.MedianBlur(blur_limit=3, p=1)\n","    ], p=1),\n","    A.ShiftScaleRotate(shift_limit=0.1, scale_limit=0.1, rotate_limit=30, p=0.25),\n","    A.Rotate(limit=(0, 360), p=0.75),\n","    A.GridDistortion(always_apply=False, p=0.75, num_steps=6, distort_limit=(-0.3, 0.3), interpolation=0, border_mode=0, value=(0, 0, 0), mask_value=None, normalized=False),\n","    A.OneOf([\n","        horizontal_flip,\n","        vertical_flip,\n","        double_flip,\n","        transpose,\n","        transpose_hflip,\n","        transpose_vflip,\n","        transpose_dflip\n","    ], p=1.0),\n","    A.OneOf([\n","        A.OpticalDistortion(always_apply=False, p=1.0, distort_limit=(-0.3, 0.3), shift_limit=(-0.05, 0.09), interpolation=0, border_mode=0, value=(0, 0, 0), mask_value=None),\n","        A.MultiplicativeNoise(always_apply=False, p=1.0, multiplier=(0.93, 2.22), per_channel=True, elementwise=True),\n","        A.ISONoise(always_apply=False, p=1.0, intensity=(0.38, 1.0), color_shift=(0.18, 0.47)),\n","        A.RandomBrightnessContrast(always_apply=False, p=1.0, brightness_limit=(0.19, 0.62), contrast_limit=(-0.02, 0.62), brightness_by_max=True)\n","    ], p=0.75),\n","    \n","    A.Resize(height=img_size, width=img_size),\n","    A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n","    ToTensorV2(),\n","])\n","\n","# test image 변환을 위한 transform 코드\n","tst_transform = A.Compose([\n","    #Adjust(always_apply=True),\n","    A.LongestMaxSize(max_size=pre_img_size, always_apply=True),\n","    A.PadIfNeeded(min_height=pre_img_size, min_width=pre_img_size, border_mode=0, value=(255, 255, 255)),\n","    A.Resize(height=img_size, width=img_size),\n","    A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n","    ToTensorV2(),\n","])\n"]},{"cell_type":"code","execution_count":18,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Original training data count: 1666\n","Augmented training data count: 333200\n","Test data count: 3140\n"]}],"source":["# 데이터셋 및 데이터 로더 정의\n","trn_dataset = ImageDataset(\n","    \"/data/ephemeral/home/datasets_fin/train_labelupdate.csv\",\n","    \"/data/ephemeral/home/datasets_fin/combined_train/train\",\n","    transform=trn_transform,\n","    oversample=True,\n","    augment_ratio=augment_ratio\n",")\n","\n","\n","tst_dataset = ImageDataset(\n","    \"/data/ephemeral/home/datasets_fin/sample_submission.csv\",\n","    \"/data/ephemeral/home/datasets_fin/wdnx4/wdnx4\",\n","    transform=tst_transform,\n","    oversample=False,\n","    augment_ratio=1\n",")\n","\n","\n","ori_traindata_num = int(len(trn_dataset)/augment_ratio)\n","print(f\"Original training data count: {ori_traindata_num}\")\n","print(f\"Augmented training data count: {len(trn_dataset)}\")\n","print(f\"Test data count: {len(tst_dataset)}\")\n","# wandb config 업데이트\n","wandb.config.update({\n","    \"Trn_data\": len(trn_dataset)\n","})"]},{"cell_type":"markdown","metadata":{},"source":["# 기본 valid"]},{"cell_type":"code","execution_count":206,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Train dataset 개수: 266560\n","Validation dataset 개수: 66640\n","Test dataset 개수: 3140\n"]}],"source":["# 데이터 셋을 학습 데이터 셋과 검증 데이터 셋으로 분리\n","import copy\n","# validation config\n","VALID_RATIO = 0.8\n","\n","total_size = len(trn_dataset)\n","train_num, valid_num = int(total_size * VALID_RATIO), total_size - int(total_size * VALID_RATIO)\n","\n","# train - valid set 나누기\n","generator = torch.Generator().manual_seed(SEED)\n","train_dataset, valid_dataset = torch.utils.data.random_split(trn_dataset, [train_num, valid_num], generator = generator)\n","\n","valid_data = copy.deepcopy(valid_dataset)\n","valid_data.dataset.transform = tst_transform\n","\n","print(f'Train dataset 개수: {len(train_dataset)}')\n","print(f'Validation dataset 개수: {len(valid_dataset)}')\n","print(f'Test dataset 개수: {len(tst_dataset)}')"]},{"cell_type":"markdown","metadata":{},"source":["# stratified valid"]},{"cell_type":"code","execution_count":19,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Train dataset 개수: 251200\n","Validation dataset 개수: 62800\n","Test dataset 개수: 3140\n"]}],"source":["import pandas as pd\n","from sklearn.model_selection import StratifiedShuffleSplit\n","import torch\n","import numpy as np\n","import copy\n","\n","VALID_RATIO = 0.8\n","\n","# 원본 CSV 파일 읽기\n","original_df = pd.read_csv(\"/data/ephemeral/home/datasets_fin/train_labelupdate.csv\")\n","\n","# 원본 데이터의 라벨 추출\n","labels = original_df['target'].values\n","\n","# StratifiedShuffleSplit 사용하여 원본 데이터셋을 학습 및 검증으로 분할\n","sss = StratifiedShuffleSplit(n_splits=1, test_size=1-VALID_RATIO, random_state=SEED)\n","train_index, valid_index = next(sss.split(np.zeros(len(labels)), labels))\n","\n","# 학습 및 검증 데이터셋을 원본 데이터프레임에서 분리\n","train_df = original_df.iloc[train_index]\n","valid_df = original_df.iloc[valid_index]\n","\n","# 증강된 데이터셋을 원래 데이터셋에 대한 인덱스를 기반으로 분리\n","# train_df와 valid_df의 인덱스를 기반으로 증강된 데이터셋에 적용\n","train_indices = np.hstack([np.arange(i * augment_ratio, (i + 1) * augment_ratio) for i in train_index])\n","valid_indices = np.hstack([np.arange(i * augment_ratio, (i + 1) * augment_ratio) for i in valid_index])\n","\n","train_dataset = torch.utils.data.Subset(trn_dataset, train_indices)\n","valid_dataset = torch.utils.data.Subset(trn_dataset, valid_indices)\n","\n","# 검증 데이터에 tst_transform 적용\n","valid_data = copy.deepcopy(valid_dataset)\n","valid_data.dataset.transform = tst_transform\n","\n","print(f'Train dataset 개수: {len(train_dataset)}')\n","print(f'Validation dataset 개수: {len(valid_dataset)}')\n","print(f'Test dataset 개수: {len(tst_dataset)}')\n"]},{"cell_type":"code","execution_count":20,"metadata":{"executionInfo":{"elapsed":1,"status":"ok","timestamp":1700315112808,"user":{"displayName":"Ynot(송원호)","userId":"16271863862696372773"},"user_tz":-540},"id":"_sO03fWaQj1h"},"outputs":[],"source":["# DataLoader 정의\n","trn_loader = DataLoader(\n","    trn_dataset,\n","    batch_size=BATCH_SIZE,\n","    shuffle=True,\n","    num_workers=num_workers,\n","    pin_memory=True,\n","    drop_last=False\n",")\n","tst_loader = DataLoader(\n","    tst_dataset,\n","    batch_size=BATCH_SIZE,\n","    shuffle=False,\n","    num_workers=0,\n","    pin_memory=True\n",")\n","valid_loader = DataLoader(\n","    valid_dataset, \n","    batch_size = BATCH_SIZE, \n","    shuffle = False,\n","    num_workers=0,\n","    pin_memory=True\n","    )\n"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"Nmm5h3J-pXNV"},"source":["## 5. Train Model\n","* 모델을 로드하고, 학습을 진행합니다."]},{"cell_type":"markdown","metadata":{},"source":["# timm 모델"]},{"cell_type":"code","execution_count":21,"metadata":{"executionInfo":{"elapsed":870,"status":"ok","timestamp":1700315114067,"user":{"displayName":"Ynot(송원호)","userId":"16271863862696372773"},"user_tz":-540},"id":"FbBgFPsLT-CO"},"outputs":[],"source":["torch.cuda.empty_cache()\n","\n","# 모델을 불러옵니다.\n","model = timm.create_model(\n","    model_name,\n","    pretrained=True,\n","    num_classes=17,\n","    drop_rate=0.2\n",").to(device)\n","\n","# 손실 함수를 정의합니다.\n","loss_fn = nn.CrossEntropyLoss()\n","\n","# 옵티마이저를 정의합니다.\n","optimizer = AdamW(model.parameters(), lr=LR)\n","\n","# Learning Rate Scheduler를 정의합니다.\n","scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=3, verbose=True)\n","\n","# Early Stopping을 위한 변수 초기화\n","best_loss = float('inf')\n","early_stopping_counter = 0"]},{"cell_type":"code","execution_count":22,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8778,"status":"ok","timestamp":1700315122843,"user":{"displayName":"Ynot(송원호)","userId":"16271863862696372773"},"user_tz":-540},"id":"OvIVcSRgUPtS","outputId":"88230bf2-976f-45f6-b3b7-1a2d0ad00548"},"outputs":[],"source":["# 한 에폭(epoch) 동안 모델을 학습시키는 함수입니다.\n","def train_one_epoch(loader, model, optimizer, loss_fn, device, log_interval=1000):\n","    model.train()\n","    train_loss = 0\n","    preds_list = []\n","    targets_list = []\n","\n","    pbar = tqdm(loader)\n","    for batch_idx, (image, targets) in enumerate(pbar):\n","        if image is None or targets is None:\n","            continue\n","        \n","        image = image.to(device)\n","        targets = targets.to(device)\n","\n","        optimizer.zero_grad(set_to_none=True)\n","\n","        preds = model(image)\n","        loss = loss_fn(preds, targets)\n","        loss.backward()\n","        optimizer.step()\n","\n","        train_loss += loss.item()\n","        preds_list.extend(preds.argmax(dim=1).detach().cpu().numpy())\n","        targets_list.extend(targets.detach().cpu().numpy())\n","\n","        pbar.set_description(f\"Loss: {loss.item():.4f}\")\n","\n","        # 지정된 간격으로만 wandb에 로그를 남김\n","        if batch_idx % log_interval == 0:\n","            wandb.log({\n","                \"batch_loss\": loss.item(),\n","                \"cumulative_loss\": train_loss / (batch_idx + 1)\n","            })\n","\n","    train_loss /= len(loader)\n","    train_acc = accuracy_score(targets_list, preds_list)\n","    train_f1 = f1_score(targets_list, preds_list, average='macro')\n","\n","    ret = {\n","        \"train_loss\": train_loss,\n","        \"train_acc\": train_acc,\n","        \"train_f1\": train_f1,\n","    }\n","\n","    # 에폭 단위로 wandb에 학습 과정 로그\n","    wandb.log({\"train_loss\": train_loss, \"train_acc\": train_acc, \"train_f1\": train_f1})\n","\n","    return ret"]},{"cell_type":"code","execution_count":24,"metadata":{},"outputs":[],"source":["import torch\n","from sklearn.metrics import accuracy_score, f1_score\n","from tqdm import tqdm\n","\n","def training(model, dataloader, criterion, optimizer, device, epoch, num_epochs):\n","    model.train()\n","    train_loss = 0\n","    preds_list = []\n","    targets_list = []\n","\n","    pbar = tqdm(dataloader)\n","    for images, labels in pbar:\n","        images = images.to(device)\n","        labels = labels.to(device)\n","\n","        model.zero_grad(set_to_none=True)\n","\n","        preds = model(images)\n","        loss = criterion(preds, labels)\n","        loss.backward()\n","        optimizer.step()\n","\n","        train_loss += loss.item()\n","        preds_list.extend(preds.argmax(dim=1).detach().cpu().numpy())\n","        targets_list.extend(labels.detach().cpu().numpy())\n","\n","        pbar.set_description(f\"Epoch [{epoch+1}/{num_epochs}] - Train Loss: {loss.item()}\")\n","        \n","    train_loss /= len(dataloader)\n","    train_acc = accuracy_score(targets_list, preds_list)    \n","    train_f1 = f1_score(targets_list, preds_list, average='macro')\n","\n","    return model, train_loss, train_acc, train_f1\n","\n","def evaluation(model, dataloader, criterion, device, epoch, num_epochs):\n","    model.eval()  # 모델을 평가 모드로 설정\n","    valid_loss = 0.0\n","    preds_list = []\n","    targets_list = []\n","\n","    with torch.no_grad():  # model의 업데이트 막기\n","        tbar = tqdm(dataloader)\n","        for images, labels in tbar:\n","            images = images.to(device)\n","            labels = labels.to(device)\n","\n","            preds = model(images)\n","            loss = criterion(preds, labels)\n","\n","            valid_loss += loss.item()\n","            preds_list.extend(preds.argmax(dim=1).detach().cpu().numpy())\n","            targets_list.extend(labels.detach().cpu().numpy())\n","\n","            tbar.set_description(f\"Epoch [{epoch+1}/{num_epochs}] - Valid Loss: {loss.item()}\")\n","\n","    valid_loss = valid_loss / len(dataloader)\n","    valid_acc = accuracy_score(targets_list, preds_list)  \n","    valid_f1 = f1_score(targets_list, preds_list, average='macro')\n","\n","    return valid_loss, valid_acc, valid_f1\n","\n","def training_loop(model, train_dataloader, valid_dataloader, criterion, optimizer, device, num_epochs, early_stopping_patience, model_name, run):\n","    best_valid_loss = float('inf')\n","    early_stop_counter = 0\n","    valid_max_accuracy = -1\n","    best_model = None\n","\n","    for epoch in range(num_epochs):\n","        model, train_loss, train_acc, train_f1 = training(model, train_dataloader, criterion, optimizer, device, epoch, num_epochs)\n","        valid_loss, valid_acc, valid_f1 = evaluation(model, valid_dataloader, criterion, device, epoch, num_epochs)\n","\n","        monitoring_value = {'train_loss': train_loss, 'train_accuracy': train_acc, 'train_f1': train_f1, \n","                            'valid_loss': valid_loss, 'valid_accuracy': valid_acc, 'valid_f1': valid_f1}\n","        \n","        run.log(monitoring_value, step=epoch)\n","        \n","        print(f'''Epoch [{epoch + 1}/{num_epochs}] Finished\n","        Train Loss: {train_loss}, Train Accuracy: {train_acc}, Train F1: {train_f1}\n","        Valid Loss: {valid_loss}, Valid Accuracy: {valid_acc}, Valid F1: {valid_f1}''')\n","\n","        # Save the model after every epoch\n","        epoch_model_name = f\"{model_name}_epoch_{epoch + 1}.pt\"\n","        torch.save(model.state_dict(), f\"./{epoch_model_name}\")\n","        print(f'Model saved as {epoch_model_name}')\n","\n","        if valid_acc > valid_max_accuracy:\n","            valid_max_accuracy = valid_acc\n","\n","        if valid_loss < best_valid_loss:\n","            best_valid_loss = valid_loss\n","            best_model = model\n","            early_stop_counter = 0\n","        else:\n","            early_stop_counter += 1\n","\n","        if early_stop_counter >= early_stopping_patience:\n","            print(\"Early stopping\")\n","            break\n","\n","    return best_model, valid_max_accuracy"]},{"cell_type":"code","execution_count":25,"metadata":{},"outputs":[{"data":{"text/html":["Finishing last run (ID:sxshmb5m) before initializing another..."],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"9ba86deeedbe424caaa29cf697009310","version_major":2,"version_minor":0},"text/plain":["VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<style>\n","    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n","    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n","    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n","    </style>\n","<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>batch_loss</td><td>▇█▂▃▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>cumulative_loss</td><td>▇█▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁</td></tr><tr><td>train_f1</td><td>▁</td></tr><tr><td>train_loss</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>batch_loss</td><td>0.00015</td></tr><tr><td>cumulative_loss</td><td>0.04455</td></tr><tr><td>train_acc</td><td>0.95854</td></tr><tr><td>train_f1</td><td>0.95869</td></tr><tr><td>train_loss</td><td>0.11829</td></tr></table><br/></div></div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View run <strong style=\"color:#cdcd00\">fastcampus_cv11</strong> at: <a href='https://wandb.ai/helloyoonjae-/helloyoonjae-/runs/sxshmb5m' target=\"_blank\">https://wandb.ai/helloyoonjae-/helloyoonjae-/runs/sxshmb5m</a><br/> View project at: <a href='https://wandb.ai/helloyoonjae-/helloyoonjae-' target=\"_blank\">https://wandb.ai/helloyoonjae-/helloyoonjae-</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Find logs at: <code>./wandb/run-20240810_221436-sxshmb5m/logs</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Successfully finished last run (ID:sxshmb5m). Initializing new run:<br/>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"13cfc7b6c7b54465839eb558e16cf131","version_major":2,"version_minor":0},"text/plain":["VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.01111227994163831, max=1.0)…"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["wandb version 0.17.6 is available!  To upgrade, please run:\n"," $ pip install wandb --upgrade"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Tracking run with wandb version 0.17.5"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Run data is saved locally in <code>/data/ephemeral/home/notebook/wandb/run-20240811_101801-rjx6b26g</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Syncing run <strong><a href='https://wandb.ai/helloyoonjae-/helloyoonjae-/runs/rjx6b26g' target=\"_blank\">fastcampus_cv11</a></strong> to <a href='https://wandb.ai/helloyoonjae-/helloyoonjae-' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View project at <a href='https://wandb.ai/helloyoonjae-/helloyoonjae-' target=\"_blank\">https://wandb.ai/helloyoonjae-/helloyoonjae-</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View run at <a href='https://wandb.ai/helloyoonjae-/helloyoonjae-/runs/rjx6b26g' target=\"_blank\">https://wandb.ai/helloyoonjae-/helloyoonjae-/runs/rjx6b26g</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: logging graph, to disable use `wandb.watch(log_graph=False)`\n","Epoch [1/5] - Train Loss: 0.003721291199326515: 100%|██████████| 41650/41650 [2:03:21<00:00,  5.63it/s]    \n","Epoch [1/5] - Valid Loss: 0.06029784306883812: 100%|██████████| 7850/7850 [33:20<00:00,  3.92it/s]   \n"]},{"name":"stdout","output_type":"stream","text":["Epoch [1/5] Finished\n","        Train Loss: 0.036345140690544425, Train Accuracy: 0.9877971188475391, Train F1: 0.9878459099534824\n","        Valid Loss: 0.033467487800746654, Valid Accuracy: 0.9897770700636943, Valid F1: 0.9897963070793717\n","Model saved as tf_efficientnet_b5.ns_jft_in1k_epoch_1.pt\n"]},{"name":"stderr","output_type":"stream","text":["Epoch [2/5] - Train Loss: 0.00010965180263156071: 100%|██████████| 41650/41650 [2:02:35<00:00,  5.66it/s]  \n","Epoch [2/5] - Valid Loss: 0.5089395642280579: 100%|██████████| 7850/7850 [32:53<00:00,  3.98it/s]    \n"]},{"name":"stdout","output_type":"stream","text":["Epoch [2/5] Finished\n","        Train Loss: 0.027852874407122686, Train Accuracy: 0.9907082833133253, Train F1: 0.9907223764386129\n","        Valid Loss: 0.04782558159980957, Valid Accuracy: 0.988328025477707, Valid F1: 0.9885298437305631\n","Model saved as tf_efficientnet_b5.ns_jft_in1k_epoch_2.pt\n"]},{"name":"stderr","output_type":"stream","text":["Epoch [3/5] - Train Loss: 7.375933819275815e-06: 100%|██████████| 41650/41650 [2:04:45<00:00,  5.56it/s]   \n","Epoch [3/5] - Valid Loss: 0.0006924738408997655: 100%|██████████| 7850/7850 [32:51<00:00,  3.98it/s] \n"]},{"name":"stdout","output_type":"stream","text":["Epoch [3/5] Finished\n","        Train Loss: 0.02459358372356609, Train Accuracy: 0.9919327731092437, Train F1: 0.9919484721322268\n","        Valid Loss: 0.015319352624171256, Valid Accuracy: 0.9950796178343949, Valid F1: 0.9949040383399208\n","Model saved as tf_efficientnet_b5.ns_jft_in1k_epoch_3.pt\n"]},{"name":"stderr","output_type":"stream","text":["Epoch [4/5] - Train Loss: 0.03986269608139992: 100%|██████████| 41650/41650 [2:05:31<00:00,  5.53it/s]     \n","Epoch [4/5] - Valid Loss: 0.049288392066955566: 100%|██████████| 7850/7850 [32:53<00:00,  3.98it/s]  \n"]},{"name":"stdout","output_type":"stream","text":["Epoch [4/5] Finished\n","        Train Loss: 0.023005343230493396, Train Accuracy: 0.9926110444177672, Train F1: 0.992610674497882\n","        Valid Loss: 0.01393400842562387, Valid Accuracy: 0.9957165605095541, Valid F1: 0.9957402208435003\n","Model saved as tf_efficientnet_b5.ns_jft_in1k_epoch_4.pt\n"]},{"name":"stderr","output_type":"stream","text":["Epoch [5/5] - Train Loss: 7.610771717736498e-05: 100%|██████████| 41650/41650 [2:02:23<00:00,  5.67it/s]   \n","Epoch [5/5] - Valid Loss: 8.300280023831874e-05: 100%|██████████| 7850/7850 [32:44<00:00,  4.00it/s] \n"]},{"name":"stdout","output_type":"stream","text":["Epoch [5/5] Finished\n","        Train Loss: 0.020965055139964755, Train Accuracy: 0.9930792316926771, Train F1: 0.9930848339978455\n","        Valid Loss: 0.01111240493865205, Valid Accuracy: 0.9964171974522293, Valid F1: 0.9963570469635774\n","Model saved as tf_efficientnet_b5.ns_jft_in1k_epoch_5.pt\n","Valid Max Accuracy: 0.9964171974522293\n"]}],"source":["# wandb에 모델의 weight & bias, graident 시각화\n","run = wandb.init(project = 'helloyoonjae-', name = 'fastcampus_cv11')\n","run.watch(model, loss_fn, log = 'all', log_graph = True)\n","\n","model, valid_max_accuracy = training_loop(model, trn_loader, valid_loader, loss_fn, optimizer, device, EPOCHS, early_stopping_patience, model_name, run)\n","print(f'Valid Max Accuracy: {valid_max_accuracy}')"]},{"cell_type":"markdown","metadata":{},"source":["# custom attention 모델"]},{"cell_type":"code","execution_count":18,"metadata":{},"outputs":[],"source":["# 기본코드\n","import torch\n","import torch.nn as nn\n","import timm\n","import torch.nn.functional as F\n","\n","class AttentionModule(nn.Module):\n","    def __init__(self, in_features, out_features):\n","        super(AttentionModule, self).__init__()\n","        self.attention = nn.Sequential(\n","            nn.Linear(in_features, out_features),\n","            nn.Sigmoid()\n","        )\n","\n","    def forward(self, x):\n","        attention_weights = self.attention(x)\n","        return x * attention_weights\n","    \n","\n","class CustomEfficientNetB7(nn.Module):\n","    def __init__(self, num_classes, attention_size=1792):\n","        super(CustomEfficientNetB7, self).__init__()\n","        self.base_model = timm.create_model('efficientnet_b7', pretrained=True)\n","        \n","        # Remove the existing classifier\n","        self.base_model.reset_classifier(0, '')\n","\n","        # Add SE attention mechanism\n","        self.attention = AttentionModule(attention_size, attention_size)\n","\n","        # Add dropout layer with 0.2 ratio\n","        self.dropout = nn.Dropout(p=0.2)\n","        # drop_out 비율 20%\n","\n","        # New classifier with attention\n","        self.classifier = nn.Linear(attention_size, num_classes)\n","        \n","    def forward(self, x):\n","        x = self.base_model(x)\n","        \n","        # Global average pooling\n","        x = x.mean([2, 3])\n","\n","        # Apply attention mechanism\n","        x = self.attention(x)\n","\n","        # Apply dropout with 0.2 ratio\n","        x = self.dropout(x)\n","\n","        # Final classification\n","        x = self.classifier(x)\n","\n","        return x\n"]},{"cell_type":"code","execution_count":19,"metadata":{},"outputs":[],"source":["# 개선한 코드\n","import torch\n","import torch.nn as nn\n","import timm\n","import torch.nn.functional as F\n","\n","class AttentionModule(nn.Module):\n","    def __init__(self, in_features, out_features):\n","        super(AttentionModule, self).__init__()\n","        self.attention = nn.Sequential(\n","            nn.Linear(in_features, out_features),\n","            # nn.Sigmoid()\n","            nn.Softmax(dim=1)\n","        )\n","\n","    def forward(self, x):\n","        attention_weights = self.attention(x)\n","        return x * attention_weights\n","    \n","\n","class CustomEfficientNetB7(nn.Module):\n","    def __init__(self, num_classes, attention_size=1792):\n","        super(CustomEfficientNetB7, self).__init__()\n","        self.base_model = timm.create_model('efficientnet_b7', pretrained=True)\n","        \n","        # Remove the existing classifier\n","        self.base_model.reset_classifier(0, '')\n","\n","        # Add SE attention mechanism\n","        self.attention = AttentionModule(attention_size, attention_size)\n","\n","        # Add dropout layer with 0.2 ratio\n","        self.dropout = nn.Dropout(p=0.2)\n","        \n","        # Batch normalization after dropout\n","        self.batch_norm = nn.BatchNorm1d(attention_size)\n","\n","        # New classifier with attention\n","        self.classifier = nn.Linear(attention_size, num_classes)\n","        \n","    def forward(self, x):\n","        x = self.base_model(x)\n","        \n","        # Global average pooling\n","        x = F.adaptive_avg_pool2d(x, 1).view(x.size(0), -1)  # Adaptive pooling\n","\n","        # Apply attention mechanism\n","        attention_output = self.attention(x)\n","        \n","        # Residual Connection\n","        x = x + attention_output\n","\n","        # Apply dropout with 0.2 ratio\n","        x = self.dropout(x)\n","\n","        # Apply BatchNorm\n","        x = self.batch_norm(x)\n","\n","        # Final classification\n","        x = self.classifier(x)\n","\n","        return x\n"]},{"cell_type":"code","execution_count":20,"metadata":{},"outputs":[],"source":["import torch\n","from sklearn.metrics import accuracy_score, f1_score\n","from tqdm import tqdm\n","\n","def training(model, dataloader, criterion, optimizer, device, epoch, num_epochs):\n","    model.train()\n","    train_loss = 0\n","    preds_list = []\n","    targets_list = []\n","\n","    pbar = tqdm(dataloader)\n","    for images, labels in pbar:\n","        images = images.to(device)\n","        labels = labels.to(device)\n","\n","        model.zero_grad(set_to_none=True)\n","\n","        preds = model(images)\n","        loss = criterion(preds, labels)\n","        loss.backward()\n","        optimizer.step()\n","\n","        train_loss += loss.item()\n","        preds_list.extend(preds.argmax(dim=1).detach().cpu().numpy())\n","        targets_list.extend(labels.detach().cpu().numpy())\n","\n","        pbar.set_description(f\"Epoch [{epoch+1}/{num_epochs}] - Train Loss: {loss.item()}\")\n","        \n","    train_loss /= len(dataloader)\n","    train_acc = accuracy_score(targets_list, preds_list)    \n","    train_f1 = f1_score(targets_list, preds_list, average='macro')\n","\n","    return model, train_loss, train_acc, train_f1\n","\n","def evaluation(model, dataloader, criterion, device, epoch, num_epochs):\n","    model.eval()  # 모델을 평가 모드로 설정\n","    valid_loss = 0.0\n","    preds_list = []\n","    targets_list = []\n","\n","    with torch.no_grad():  # model의 업데이트 막기\n","        tbar = tqdm(dataloader)\n","        for images, labels in tbar:\n","            images = images.to(device)\n","            labels = labels.to(device)\n","\n","            preds = model(images)\n","            loss = criterion(preds, labels)\n","\n","            valid_loss += loss.item()\n","            preds_list.extend(preds.argmax(dim=1).detach().cpu().numpy())\n","            targets_list.extend(labels.detach().cpu().numpy())\n","\n","            tbar.set_description(f\"Epoch [{epoch+1}/{num_epochs}] - Valid Loss: {loss.item()}\")\n","\n","    valid_loss = valid_loss / len(dataloader)\n","    valid_acc = accuracy_score(targets_list, preds_list)  \n","    valid_f1 = f1_score(targets_list, preds_list, average='macro')\n","\n","    return valid_loss, valid_acc, valid_f1\n","\n","def training_loop(model, train_dataloader, valid_dataloader, criterion, optimizer, device, num_epochs, early_stopping_patience, model_name, run):\n","    best_valid_loss = float('inf')\n","    early_stop_counter = 0\n","    valid_max_accuracy = -1\n","    best_model = None\n","\n","    for epoch in range(num_epochs):\n","        model, train_loss, train_acc, train_f1 = training(model, train_dataloader, criterion, optimizer, device, epoch, num_epochs)\n","        valid_loss, valid_acc, valid_f1 = evaluation(model, valid_dataloader, criterion, device, epoch, num_epochs)\n","\n","        monitoring_value = {'train_loss': train_loss, 'train_accuracy': train_acc, 'train_f1': train_f1, \n","                            'valid_loss': valid_loss, 'valid_accuracy': valid_acc, 'valid_f1': valid_f1}\n","        \n","        run.log(monitoring_value, step=epoch)\n","        \n","        print(f'''Epoch [{epoch + 1}/{num_epochs}] Finished\n","        Train Loss: {train_loss}, Train Accuracy: {train_acc}, Train F1: {train_f1}\n","        Valid Loss: {valid_loss}, Valid Accuracy: {valid_acc}, Valid F1: {valid_f1}''')\n","\n","        # Save the model after every epoch\n","        epoch_model_name = f\"{model_name}_epoch_{epoch + 1}.pt\"\n","        torch.save(model.state_dict(), f\"./{epoch_model_name}\")\n","        print(f'Model saved as {epoch_model_name}')\n","\n","        if valid_acc > valid_max_accuracy:\n","            valid_max_accuracy = valid_acc\n","\n","        if valid_loss < best_valid_loss:\n","            best_valid_loss = valid_loss\n","            best_model = model\n","            early_stop_counter = 0\n","        else:\n","            early_stop_counter += 1\n","\n","        if early_stop_counter >= early_stopping_patience:\n","            print(\"Early stopping\")\n","            break\n","\n","    return best_model, valid_max_accuracy"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# 모델 생성\n","torch.cuda.empty_cache()\n","\n","num_classes = 17\n","\n","model = CustomEfficientNetB7(num_classes).to(device)\n","loss_fn = nn.CrossEntropyLoss()\n","optimizer = AdamW(model.parameters(), lr=LR)"]},{"cell_type":"code","execution_count":20,"metadata":{},"outputs":[{"data":{"text/html":["Finishing last run (ID:x4ps850u) before initializing another..."],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"36e8a02111b44299afc19b659fcd48b0","version_major":2,"version_minor":0},"text/plain":["VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View run <strong style=\"color:#cdcd00\">fastcampus_cv11</strong> at: <a href='https://wandb.ai/helloyoonjae-/helloyoonjae-/runs/x4ps850u' target=\"_blank\">https://wandb.ai/helloyoonjae-/helloyoonjae-/runs/x4ps850u</a><br/> View project at: <a href='https://wandb.ai/helloyoonjae-/helloyoonjae-' target=\"_blank\">https://wandb.ai/helloyoonjae-/helloyoonjae-</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Find logs at: <code>./wandb/run-20240809_233826-x4ps850u/logs</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Successfully finished last run (ID:x4ps850u). Initializing new run:<br/>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"3fc13bd15c0d4dc4a3236a278f24905c","version_major":2,"version_minor":0},"text/plain":["VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011112341657280922, max=1.0…"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["wandb version 0.17.6 is available!  To upgrade, please run:\n"," $ pip install wandb --upgrade"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Tracking run with wandb version 0.17.5"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Run data is saved locally in <code>/data/ephemeral/home/notebook/wandb/run-20240809_233923-t3hy1n02</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Syncing run <strong><a href='https://wandb.ai/helloyoonjae-/helloyoonjae-/runs/t3hy1n02' target=\"_blank\">fastcampus_cv11</a></strong> to <a href='https://wandb.ai/helloyoonjae-/helloyoonjae-' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View project at <a href='https://wandb.ai/helloyoonjae-/helloyoonjae-' target=\"_blank\">https://wandb.ai/helloyoonjae-/helloyoonjae-</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View run at <a href='https://wandb.ai/helloyoonjae-/helloyoonjae-/runs/t3hy1n02' target=\"_blank\">https://wandb.ai/helloyoonjae-/helloyoonjae-/runs/t3hy1n02</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: logging graph, to disable use `wandb.watch(log_graph=False)`\n","Epoch [1/5] - Train Loss: 0.0043776435777544975: 100%|██████████| 41650/41650 [1:04:03<00:00, 10.84it/s] \n","Epoch [1/5] - Valid Loss: 0.022932710126042366: 100%|██████████| 7850/7850 [40:19<00:00,  3.24it/s]  \n"]},{"name":"stdout","output_type":"stream","text":["Epoch [1/5] Finished\n","        Train Loss: 0.1506204385015825, Train Accuracy: 0.95046818727491, Train F1: 0.9505705695440249\n","        Valid Loss: 0.050231976082785325, Valid Accuracy: 0.9847611464968152, Valid F1: 0.9849932413814098\n","Model saved as efficientnet_b4_epoch_1.pt\n"]},{"name":"stderr","output_type":"stream","text":["Epoch [2/5] - Train Loss: 0.008261856622993946: 100%|██████████| 41650/41650 [1:03:53<00:00, 10.86it/s]  \n","Epoch [2/5] - Valid Loss: 0.0:  59%|█████▉    | 4656/7850 [24:00<17:49,  2.99it/s]                   "]}],"source":["# wandb에 모델의 weight & bias, graident 시각화\n","run = wandb.init(project = 'helloyoonjae-', name = 'fastcampus_cv11')\n","run.watch(model, loss_fn, log = 'all', log_graph = True)\n","\n","model, valid_max_accuracy = training_loop(model, trn_loader, valid_loader, loss_fn, optimizer, device, EPOCHS, early_stopping_patience, model_name, run)\n","print(f'Valid Max Accuracy: {valid_max_accuracy}')"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"lkwxRXoBpbaX"},"source":["# 6. Inference & Save File\n","* 테스트 이미지에 대한 추론을 진행하고, 결과 파일을 저장합니다."]},{"cell_type":"markdown","metadata":{},"source":["# pt 파일 불러오기"]},{"cell_type":"code","execution_count":19,"metadata":{},"outputs":[{"data":{"text/plain":["<All keys matched successfully>"]},"execution_count":19,"metadata":{},"output_type":"execute_result"}],"source":["# 모델 인스턴스 생성\n","model_name = 'tf_efficientnet_b5.ns_jft_in1k'#'tiny_vit_21m_384.dist_in22k_ft_in1k'#'efficientnetv2_rw_m.agc_in1k' #'vit_base_patch16_224.augreg_in1k' #'efficientnet_b3.ra2_in1k'#'densenet121.ra_in1k'# #'resnet101' #'resnet34' # 'resnet50' 'efficientnet-b0', ...\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","model = timm.create_model(\n","    model_name,\n","    pretrained=True,\n","    num_classes=17,\n","    drop_rate=0.2\n",").to(device)\n","\n","# 가중치 로드\n","model.load_state_dict(torch.load(r'/data/ephemeral/home/notebook/tf_efficientnet_b5.ns_jft_in1k_epoch_3.pt'))\n"]},{"cell_type":"markdown","metadata":{},"source":["# TTA (Test-Time Augmentation) "]},{"cell_type":"code","execution_count":54,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["  0%|          | 0/393 [00:00<?, ?it/s]"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 393/393 [03:02<00:00,  2.15it/s]"]},{"name":"stdout","output_type":"stream","text":["                     ID  target\n","0  0008fdb22ddce0ce.jpg       2\n","1  00091bffdffd83de.jpg      12\n","2  00396fbc1f6cc21d.jpg       5\n","3  00471f8038d9c4b6.jpg      12\n","4  00901f504008d884.jpg       2\n"]},{"name":"stderr","output_type":"stream","text":["\n"]}],"source":["import numpy as np\n","import torch\n","from torch.utils.data import DataLoader, Dataset\n","import albumentations as A\n","from albumentations.pytorch import ToTensorV2\n","from tqdm import tqdm\n","from PIL import Image, ImageEnhance\n","import pandas as pd\n","import os\n","import cv2\n","from torch.nn.functional import softmax\n","\n","# Albumentations transform 정의\n","pre_img_size = 512  # 사전 정의된 이미지 크기 (필요에 따라 수정)\n","\n","transform = A.Compose([\n","    A.LongestMaxSize(max_size=pre_img_size, always_apply=True),\n","    A.PadIfNeeded(min_height=pre_img_size, min_width=pre_img_size, border_mode=0, value=(255, 255, 255)),\n","    # A.Resize(height=pre_img_size, width=pre_img_size),  # 모든 이미지를 같은 크기로 리사이징\n","    #A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]), 여기서 normalize넣으면 완전히 잘못됨 후에 또 하기 때문\n","    ToTensorV2(),\n","])\n","\n","\n","class ImageDataset2(Dataset):\n","    def __init__(self, csv_file, path, transform=None):\n","        self.df = pd.read_csv(csv_file)\n","        self.path = path\n","        self.transform = transform\n","\n","    def __len__(self):\n","        return len(self.df)\n","\n","    def __getitem__(self, idx):\n","        real_idx = idx % len(self.df)\n","        name, target = self.df.iloc[real_idx]\n","        img_path = os.path.join(self.path, name)\n","        \n","        try:\n","            img = np.array(Image.open(img_path).convert(\"RGB\"))\n","        except Exception as e:\n","            print(f\"Error loading image {img_path}: {e}\")\n","            img = np.zeros((pre_img_size, pre_img_size, 3), dtype=np.uint8)  # 빈 이미지로 대체\n","        \n","        if self.transform:\n","            img = self.transform(image=img)['image']\n","        return img, target\n","\n","# TTA 변환 정의\n","tta_transforms = [\n","    A.Compose([\n","        # A.GaussNoise(var_limit=(10.0, 800.0), p=0.75),\n","        # A.GaussianBlur(blur_limit=(1, 7), p=0.5),\n","        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n","        ToTensorV2()\n","    ]),\n","    A.Compose([\n","        # A.GaussNoise(var_limit=(10.0, 800.0), p=0.75),\n","        # A.GaussianBlur(blur_limit=(1, 7), p=0.5),\n","        A.HorizontalFlip(p=1.0),\n","        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n","        ToTensorV2()\n","    ]),\n","    A.Compose([\n","        # A.GaussNoise(var_limit=(10.0, 800.0), p=0.75),\n","        # A.GaussianBlur(blur_limit=(1, 7), p=0.5),\n","        A.VerticalFlip(p=1.0),\n","        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n","        ToTensorV2()\n","    ]),\n","    A.Compose([\n","        # A.GaussNoise(var_limit=(10.0, 800.0), p=0.75),\n","        # A.GaussianBlur(blur_limit=(1, 7), p=0.5),\n","        A.Transpose(p=1.0),\n","        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n","        ToTensorV2()\n","    ]),\n","    A.Compose([\n","        A.Transpose(p=1.0),\n","        A.HorizontalFlip(p=1.0),\n","        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n","        ToTensorV2()\n","    ]),\n","    A.Compose([\n","        # A.GaussNoise(var_limit=(10.0, 800.0), p=0.75),\n","        # A.GaussianBlur(blur_limit=(1, 7), p=0.5),\n","        A.Transpose(p=1.0),\n","        A.VerticalFlip(p=1.0),\n","        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n","        ToTensorV2()\n","    ]),\n","    A.Compose([\n","        # A.GaussNoise(var_limit=(10.0, 800.0), p=0.75),\n","        # A.GaussianBlur(blur_limit=(1, 7), p=0.5),\n","        A.Transpose(p=1.0),\n","        A.HorizontalFlip(p=1.0),\n","        A.VerticalFlip(p=1.0),\n","        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n","        ToTensorV2()\n","    ]),\n","    # A.Compose([\n","    #     A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n","    #     ToTensorV2()\n","    # ])  # 원본 이미지\n","]\n","\n","tta_dataset = ImageDataset2(\n","    \"/data/ephemeral/home/datasets_fin/sample_submission.csv\",\n","    \"/data/ephemeral/home/datasets_fin/test/\",\n","    transform=transform\n",")\n","\n","tta_loader = DataLoader(\n","    tta_dataset,\n","    batch_size=BATCH_SIZE,\n","    shuffle=False,\n","    num_workers=0,\n","    pin_memory=True\n",")\n","\n","# TTA inference function\n","def tta_inference(loader, model, device, tta_transforms):\n","    model.eval()\n","    all_outputs = []\n","    \n","    weights = [0.1, 0.1, 0.1, 0.1, 0.2, 0.2, 0.2]  # 각 변환에 대한 가중치 설정\n","    \n","    for images, _ in tqdm(loader):\n","        images = images.to(device).float()  # 이미지 텐서를 float 형식으로 변환\n","        batch_outputs = torch.zeros(images.size(0), 17).to(device)  # 17은 클래스 수\n","        \n","        for weight, tta_transform in zip(weights, tta_transforms):\n","            tta_images = []\n","            for image in images:\n","                tta_image = tta_transform(image=image.permute(1, 2, 0).cpu().numpy().astype(np.float32))['image']\n","                tta_images.append(tta_image.to(device).float())\n","            tta_images = torch.stack(tta_images)\n","            with torch.no_grad():\n","                preds = model(tta_images)\n","                batch_outputs += weight * preds  # 가중치 적용\n","        \n","        # TTA 평균 내기 (Soft Voting)\n","        batch_outputs /= len(tta_transforms)\n","        \n","        # 최종 예측 값을 리스트에 저장\n","        all_outputs.append(batch_outputs.cpu().numpy())\n","    \n","    # 모든 배치의 예측 값을 연결\n","    all_outputs = np.concatenate(all_outputs, axis=0)\n","    return all_outputs\n","\n","# TTA를 적용한 예측\n","all_outputs = tta_inference(tta_loader, model, device, tta_transforms)\n","preds_list = np.argmax(all_outputs, axis=1)\n","\n","# 예측 결과를 데이터프레임으로 저장\n","pred_df = pd.DataFrame(tta_dataset.df, columns=['ID', 'target'])\n","pred_df['target'] = preds_list\n","\n","# 제출 형식 파일을 읽어와 ID 열이 일치하는지 확인\n","sample_submission_df = pd.read_csv(\"/data/ephemeral/home/datasets_fin/sample_submission.csv\")\n","assert (sample_submission_df['ID'] == pred_df['ID']).all()\n","\n","# 예측 결과를 CSV 파일로 저장\n","pred_df.to_csv(f\"{model_name}_{pre_img_size}SIZE_{BATCH_SIZE}BATCH_{EPOCHS}EPOCH_0810_TTA_pred.csv\", index=False)\n","print(pred_df.head())\n"]},{"cell_type":"markdown","metadata":{},"source":["# 기본"]},{"cell_type":"code","execution_count":21,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":12776,"status":"ok","timestamp":1700315185336,"user":{"displayName":"Ynot(송원호)","userId":"16271863862696372773"},"user_tz":-540},"id":"uRYe6jlPU_Om","outputId":"2a08690c-9ffe-418d-8679-eb9280147110"},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|██████████| 393/393 [00:31<00:00, 12.36it/s]"]},{"name":"stdout","output_type":"stream","text":["                     ID  target  prob_class_0  prob_class_1  prob_class_2  \\\n","0  0008fdb22ddce0ce.jpg       2  4.766859e-15  3.384005e-13  1.000000e+00   \n","1  00091bffdffd83de.jpg      12  1.044518e-12  9.916116e-14  4.186410e-10   \n","2  00396fbc1f6cc21d.jpg       5  1.049912e-11  2.464661e-14  1.624842e-11   \n","3  00471f8038d9c4b6.jpg      12  5.382270e-11  1.996971e-12  1.053528e-09   \n","4  00901f504008d884.jpg       2  1.564134e-11  1.149906e-13  1.000000e+00   \n","\n","   prob_class_3  prob_class_4  prob_class_5  prob_class_6  prob_class_7  \\\n","0  3.235580e-10  1.328821e-12  1.868789e-10  6.139736e-14  8.582778e-11   \n","1  1.952860e-10  1.170990e-13  7.920273e-11  1.289312e-11  1.243196e-15   \n","2  9.912641e-13  4.954974e-14  1.000000e+00  1.223028e-08  3.500011e-14   \n","3  3.512946e-08  1.965280e-09  6.146787e-09  7.105433e-12  9.930785e-12   \n","4  7.879350e-11  1.485977e-15  7.895594e-12  2.091520e-13  2.060892e-11   \n","\n","   prob_class_8  prob_class_9  prob_class_10  prob_class_11  prob_class_12  \\\n","0  3.295609e-11  4.229628e-11   1.102311e-13   1.357595e-13   1.473029e-17   \n","1  4.504691e-11  3.512827e-13   1.549837e-09   2.288463e-16   1.000000e+00   \n","2  4.058532e-12  6.140948e-12   7.641075e-16   2.649696e-12   1.053594e-16   \n","3  7.823024e-10  1.708532e-10   1.401625e-11   5.453113e-12   9.999989e-01   \n","4  4.790274e-11  4.436024e-13   3.054688e-13   1.447578e-12   2.137790e-16   \n","\n","   prob_class_13  prob_class_14  prob_class_15  prob_class_16  \n","0   5.333748e-13   5.332741e-13   9.049057e-13   3.100361e-09  \n","1   1.878281e-09   8.377726e-14   2.351223e-10   4.754132e-10  \n","2   1.011928e-12   2.182986e-10   1.426141e-13   1.412500e-12  \n","3   9.065695e-07   3.021908e-09   7.053259e-08   2.945893e-09  \n","4   2.973281e-12   8.398193e-13   1.891732e-12   4.333655e-08  \n"]},{"name":"stderr","output_type":"stream","text":["\n"]}],"source":["import pandas as pd\n","import torch\n","from torch.nn.functional import softmax\n","from tqdm import tqdm\n","\n","# 모델 평가 모드 설정\n","model.eval()\n","preds_list = []\n","probs_list = []  # 확률을 저장할 리스트\n","\n","# DataLoader를 통해 이미지를 반복 처리\n","for image, _ in tqdm(tst_loader):\n","    image = image.to(device)\n","    with torch.no_grad():\n","        preds = model(image)\n","        probs = softmax(preds, dim=1)  # Softmax를 적용하여 확률 계산\n","    preds_list.extend(preds.argmax(dim=1).detach().cpu().numpy())\n","    probs_list.extend(probs.detach().cpu().numpy())  # 확률 값 저장\n","\n","# 예측 결과와 확률을 데이터프레임으로 저장\n","pred_df = pd.DataFrame({\n","    'ID': tst_dataset.df['ID'],\n","    'target': preds_list\n","})\n","# 확률을 데이터프레임에 추가\n","for i in range(num_classes):\n","    pred_df[f'prob_class_{i}'] = [probs[i] for probs in probs_list]\n","\n","# 제출 형식 파일을 읽어와 ID 열이 일치하는지 확인\n","sample_submission_df = pd.read_csv(\"/data/ephemeral/home/datasets_fin/sample_submission.csv\")\n","assert (sample_submission_df['ID'] == pred_df['ID']).all()\n","\n","# 예측 결과와 확률을 CSV 파일로 저장\n","pred_df.to_csv(f\"{model_name}_{img_size}SIZE_{BATCH_SIZE}BATCH_{EPOCHS}EPOCH_0810_pred.csv\", index=False)\n","print(pred_df.head())\n"]},{"cell_type":"markdown","metadata":{},"source":["# 976 정답지와 비교"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import pandas as pd\n","import matplotlib.pyplot as plt\n","from PIL import Image\n","import os\n","from sklearn.metrics import f1_score\n","import matplotlib.font_manager as fm\n","from matplotlib.font_manager import FontProperties\n","\n","\n","def load_predictions(file_paths):\n","    \"\"\"여러 모델의 예측 결과를 로드합니다.\"\"\"\n","    predictions = {}\n","    for path in file_paths:\n","        model_name = os.path.basename(path).split('.')[0]\n","        df = pd.read_csv(path)\n","        # ID 컬럼 중복 제거 및 예측 컬럼 이름 변경\n","        df = df[['ID', 'target']].rename(columns={'target': model_name})\n","        predictions[model_name] = df\n","    return predictions\n","\n","def find_different_predictions(predictions):\n","    \"\"\"모든 모델에서 다르게 예측한 항목을 찾습니다.\"\"\"\n","    # 모든 예측을 하나의 DataFrame으로 병합\n","    all_predictions = predictions[list(predictions.keys())[0]]\n","    for model, df in list(predictions.items())[1:]:\n","        all_predictions = pd.merge(all_predictions, df, on='ID', suffixes=('', f'_{model}'))\n","    \n","    # 예측 컬럼만 선택\n","    prediction_columns = [col for col in all_predictions.columns if col != 'ID']\n","    \n","    # 예측이 다른 행만 선택\n","    different_predictions = all_predictions[all_predictions[prediction_columns].nunique(axis=1) > 1]\n","    return different_predictions\n","\n","def calculate_macro_f1(ground_truth, predictions):\n","    \"\"\"Macro F1 점수를 계산합니다.\"\"\"\n","    return f1_score(ground_truth, predictions, average='macro')\n","\n","def plot_error_distribution(ground_truth, predictions):\n","    \"\"\"각 클래스별 오류 예측 개수를 bar plot으로 표시합니다.\"\"\"\n","    # 클래스 이름 정의\n","    class_names = {\n","        0: \"계좌번호(손글씨)\", 1: \"임신출산 진료비 지급 신청서\", 2: \"자동차 계기판\", 3: \"입퇴원 확인서\", 4: \"진단서\", \n","        5: \"운전면허증\", 6: \"진료비영수증\", 7: \"통원/진료 확인서\", 8: \"주민등록증\", 9: \"여권\", \n","        10: \"진료비 납입 확인서\", 11: \"약제비 영수증\", 12: \"처방전\", 13: \"이력서\", 14: \"소견서\", \n","        15: \"자동차 등록증\", 16: \"자동차 번호판\"\n","    }\n","\n","    error_counts = {i: 0 for i in range(len(class_names))}  # 모든 클래스에 대해 초기화\n","    for gt, pred in zip(ground_truth, predictions):\n","        if gt != pred:\n","            error_counts[gt] += 1\n","    \n","    classes = sorted(error_counts.keys())\n","    counts = [error_counts[c] for c in classes]\n","    class_labels = [f\"{c}\\n{class_names[c]}\" for c in classes]\n","    \n","    # 나눔 폰트 설정\n","    # font_path = './font/NanumGothic.otf'\n","    # font_prop = FontProperties(fname=font_path)\n","    \n","    plt.figure(figsize=(20, 10))\n","    bars = plt.bar(class_labels, counts)\n","    plt.title(\"오류 예측 개수 (클래스별)\", fontsize=16)# fontproperties=font_prop, \n","    plt.xlabel(\"클래스\", fontsize=14)# fontproperties=font_prop, \n","    plt.ylabel(\"오류 개수\", fontsize=14)# fontproperties=font_prop, \n","    plt.xticks(rotation=45, ha='center')\n","    \n","    # x축 레이블에 폰트 적용\n","    ax = plt.gca()\n","    ax.set_xticklabels(class_labels, fontsize=10) # fontproperties=font_prop, \n","    \n","    plt.tight_layout()\n","    \n","    # 각 막대 위에 값 표시\n","    for bar in bars:\n","        height = bar.get_height()\n","        plt.text(bar.get_x() + bar.get_width()/2., height,\n","                 f'{height}',\n","                 ha='center', va='bottom')# fontproperties=font_prop, \n","    \n","    plt.show()  \n","    \n","    \n","def display_images_and_predictions(different_predictions, image_dir, ground_truth_name, prediction_name, class_filter=None, max_images_per_class=3):\n","    \"\"\"다르게 예측된 항목의 이미지와 예측 결과를 표시합니다.\"\"\"\n","    # 나눔 폰트 설정\n","    # font_path = './font/NanumGothic.otf'\n","    # font_prop = FontProperties(fname=font_path)\n","    class_names = {\n","        0: \"계좌번호(손글씨)\", 1: \"임신출산 진료비 지급 신청서\", 2: \"자동차 계기판\", 3: \"입퇴원 확인서\", 4: \"진단서\", \n","        5: \"운전면허증\", 6: \"진료비영수증\", 7: \"통원/진료 확인서\", 8: \"주민등록증\", 9: \"여권\", \n","        10: \"진료비 납입 확인서\", 11: \"약제비 영수증\", 12: \"처방전\", 13: \"이력서\", 14: \"소견서\", \n","        15: \"자동차 등록증\", 16: \"자동차 번호판\"\n","    }\n","    \n","    # 클래스별로 이미지 분류\n","    class_images = {c: [] for c in range(len(class_names))}\n","    for idx, row in different_predictions.iterrows():\n","        ground_truth = row[ground_truth_name]\n","        if class_filter is None or ground_truth in class_filter:\n","            class_images[ground_truth].append(row)\n","    \n","    # 표시할 이미지 선택\n","    images_to_display = []\n","    for c, imgs in class_images.items():\n","        if class_filter is None or c in class_filter:\n","            images_to_display.extend(imgs[:max_images_per_class])\n","    \n","    # 이미지 표시\n","    num_images = len(images_to_display)\n","    rows = (num_images - 1) // 3 + 1\n","    fig, axs = plt.subplots(rows, 4, figsize=(20, 5*rows))\n","    \n","    for i, row in enumerate(images_to_display):\n","        ax = axs[i//4, i%4] if rows > 1 else axs[i%4]\n","        \n","        image_id = row['ID']\n","        ground_truth = row[ground_truth_name]\n","        prediction = row[prediction_name]\n","        image_path = os.path.join(image_dir, image_id)\n","        \n","        if os.path.exists(image_path):\n","            img = Image.open(image_path)\n","            img = img.resize((300, 300), Image.LANCZOS)\n","            ax.imshow(img)\n","            ax.axis('off')\n","            id = f\"ID: {image_id}\"\n","            comparison_text = f\"정답: {ground_truth} ({class_names[ground_truth]})\\n예측: {prediction} ({class_names[prediction]})\"\n","            ax.set_title(f\"{id}\\n{comparison_text}\", fontsize=10) # fontproperties=font_prop, \n","        else:\n","            ax.text(0.5, 0.5, f\"Image not found\\nfor ID: {image_id}\", ha='center', va='center')\n","    \n","    # 빈 서브플롯 제거\n","    for i in range(num_images, rows*4):\n","        ax = axs[i//4, i%4] if rows > 1 else axs[i%4]\n","        ax.axis('off')\n","    \n","    plt.tight_layout()\n","    plt.show()\n","    \n","    print(f\"총 {num_images}개의 이미지가 표시되었습니다.\")    \n","\n","# 예측 파일 경로\n","file_paths = [\n","    \"/data/ephemeral/home/notebook/output_9760.csv\",\n","    # \"/data/ephemeral/home/notebook/efficientnet_b2.ra_in1k_384SIZE_32BATCH_2EPOCH_100_0806_PADOS_pred.csv\", # 수정해야할 부분\n","    \"/data/ephemeral/home/notebook/efficientnet_b4_512SIZE_8BATCH_5EPOCH_0810_TTA_pred.csv\"\n","]\n","\n","\n","# 이미지 디렉토리 경로\n","image_dir = \"/data/ephemeral/home/datasets_fin/test\"\n","\n","# 예측 로드\n","predictions = load_predictions(file_paths)\n","\n","# 다른 예측 찾기\n","different_predictions = find_different_predictions(predictions)\n","\n","# # 결과 출력\n","# print(different_predictions)\n","\n","# Macro F1 점수 계산\n","ground_truth_name = os.path.basename(file_paths[0]).split('.')[0]\n","prediction_name = os.path.basename(file_paths[-1]).split('.')[0]\n","ground_truth = predictions[ground_truth_name][ground_truth_name]\n","prediction = predictions[prediction_name][prediction_name]\n","macro_f1 = calculate_macro_f1(ground_truth, prediction)\n","print(f\"Macro F1 Score: {macro_f1}\")\n","\n","# 오류 분포 그래프 표시\n","plot_error_distribution(ground_truth, prediction)\n","\n","\n","# 특정 클래스(예: 0, 1, 2)에 대해 클래스당 최대 3개씩 이미지 표시\n","display_images_and_predictions(different_predictions, image_dir, ground_truth_name, prediction_name, class_filter=[1,2], max_images_per_class=3)"]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":0}
