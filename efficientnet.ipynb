{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "OliaDaX_lwou"
   },
   "source": [
    "## Contents\n",
    "- Import Library & Define Functions\n",
    "- Hyper-parameters\n",
    "- Load Data\n",
    "- Train Model\n",
    "- Inference & Save File\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "PXa_FPM73R9f"
   },
   "source": [
    "## Import Library & Define Functions\n",
    "* 학습 및 추론에 필요한 라이브러리를 로드합니다.\n",
    "* 학습 및 추론에 필요한 함수와 클래스를 정의합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 9396,
     "status": "ok",
     "timestamp": 1700314592802,
     "user": {
      "displayName": "Ynot(송원호)",
      "userId": "16271863862696372773"
     },
     "user_tz": -540
    },
    "id": "3BaoIkv5Xwa0"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "\n",
    "import timm\n",
    "import torch\n",
    "import albumentations as A\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from torch.optim import Adam\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from datetime import datetime\n",
    "import time\n",
    "from zoneinfo import ZoneInfo\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_time = datetime.fromtimestamp(time.time(), tz=ZoneInfo(\"Asia/Seoul\")).strftime(\"%Y%m%d-%H%M%S\")\n",
    "train_time\n",
    "\n",
    "wandb.init(project=\"document-classification\", name=f\"run-{train_time}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 시드를 고정합니다.\n",
    "SEED = 42\n",
    "os.environ['PYTHONHASHSEED'] = str(SEED)\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 241,
     "status": "ok",
     "timestamp": 1700314772722,
     "user": {
      "displayName": "Ynot(송원호)",
      "userId": "16271863862696372773"
     },
     "user_tz": -540
    },
    "id": "Hyl8oAy6TZAu"
   },
   "outputs": [],
   "source": [
    "# 데이터셋 클래스를 정의합니다.\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, csv_file, img_dir, transform=None, augraphy_pipeline=None):\n",
    "        self.data = pd.read_csv(csv_file)\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "        self.augraphy_pipeline = augraphy_pipeline\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = os.path.join(self.img_dir, self.data.iloc[idx, 0])\n",
    "        image = np.array(Image.open(img_name).convert('RGB'))\n",
    "        label = self.data.iloc[idx, 1]\n",
    "\n",
    "        if self.augraphy_pipeline:\n",
    "            image = self.augraphy_pipeline(image)\n",
    "\n",
    "        if self.transform:\n",
    "            augmented = self.transform(image=image)\n",
    "            image = augmented['image']\n",
    "\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 255,
     "status": "ok",
     "timestamp": 1700315066028,
     "user": {
      "displayName": "Ynot(송원호)",
      "userId": "16271863862696372773"
     },
     "user_tz": -540
    },
    "id": "kTECBJfVTbdl"
   },
   "outputs": [],
   "source": [
    "# one epoch 학습을 위한 함수입니다.\n",
    "def train_one_epoch(loader, model, optimizer, loss_fn, device):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    preds_list = []\n",
    "    targets_list = []\n",
    "\n",
    "    pbar = tqdm(loader)\n",
    "    for image, targets in pbar:\n",
    "        image = image.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        model.zero_grad(set_to_none=True)\n",
    "\n",
    "        preds = model(image)\n",
    "        loss = loss_fn(preds, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        preds_list.extend(preds.argmax(dim=1).detach().cpu().numpy())\n",
    "        targets_list.extend(targets.detach().cpu().numpy())\n",
    "\n",
    "        pbar.set_description(f\"Loss: {loss.item():.4f}\")\n",
    "\n",
    "    train_loss /= len(loader)\n",
    "    train_acc = accuracy_score(targets_list, preds_list)\n",
    "    train_f1 = f1_score(targets_list, preds_list, average='macro')\n",
    "\n",
    "    ret = {\n",
    "        \"train_loss\": train_loss,\n",
    "        \"train_acc\": train_acc,\n",
    "        \"train_f1\": train_f1,\n",
    "    }\n",
    "\n",
    "    # wandb에 훈련 메트릭 로깅\n",
    "    wandb.log(ret)\n",
    "    \n",
    "    return ret"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "Wjom43UvoXcx"
   },
   "source": [
    "## Hyper-parameters\n",
    "* 학습 및 추론에 필요한 하이퍼파라미터들을 정의합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 436,
     "status": "ok",
     "timestamp": 1700315112439,
     "user": {
      "displayName": "Ynot(송원호)",
      "userId": "16271863862696372773"
     },
     "user_tz": -540
    },
    "id": "KByfAeRmXwYk"
   },
   "outputs": [],
   "source": [
    "# device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# data config\n",
    "data_path = 'data/'\n",
    "\n",
    "# model config\n",
    "model_name = 'efficientnet_b0'\n",
    "\n",
    "# training config\n",
    "img_size = 224\n",
    "LR = 1e-3\n",
    "EPOCHS = 40\n",
    "BATCH_SIZE = 32\n",
    "num_workers = 4\n",
    "\n",
    "retrain_full_dataset = False # 최종 예측 시 전체 train 데이터로 재학습할지 여부\n",
    "reinitialize_model = False # 최종 예측 재학습 시 모델 초기화할지 여부\n",
    "\n",
    "# 설정 로깅\n",
    "wandb.config.update({\n",
    "    \"model\": model_name,\n",
    "    \"img_size\": img_size,\n",
    "    \"learning_rate\": LR,\n",
    "    \"epochs\": EPOCHS,\n",
    "    \"batch_size\": BATCH_SIZE,\n",
    "    \"num_workers\": num_workers,\n",
    "    \"retrain_full_dataset\": retrain_full_dataset,\n",
    "    \"reinitialize_model\": reinitialize_model\n",
    "})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "amum-FlIojc6"
   },
   "source": [
    "## Load Data\n",
    "* 학습, 테스트 데이터셋과 로더를 정의합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1700315112439,
     "user": {
      "displayName": "Ynot(송원호)",
      "userId": "16271863862696372773"
     },
     "user_tz": -540
    },
    "id": "llh5C7ZKoq2S"
   },
   "outputs": [],
   "source": [
    "import augraphy\n",
    "from augraphy import *\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "\n",
    "# augmentation을 위한 transform 코드\n",
    "def get_augraphy_pipeline():\n",
    "    return AugraphyPipeline([\n",
    "        BleedThrough(p=0.5),\n",
    "        DirtyRollers(p=0.5),\n",
    "        InkBleed(p=0.5),\n",
    "        Faxify(p=0.3),\n",
    "        NoiseTexturize(p=0.5),\n",
    "        Letterpress(p=0.5),\n",
    "        LowInkPeriodicLines(p=0.5),\n",
    "        LowInkRandomLines(p=0.5),\n",
    "        Folding(p=0.5),\n",
    "        Markup(p=0.3),  # PencilScribbles 대신\n",
    "        Stains(p=0.3),  # Watermark 대신\n",
    "        ])\n",
    "\n",
    "def get_train_transforms(height, width):\n",
    "    return A.Compose([\n",
    "        A.RandomResizedCrop(height=height, width=width, scale=(0.8, 1.0), ratio=(0.75, 1.3333333333333333)),\n",
    "        A.OneOf([\n",
    "            A.RandomRotate90(p=0.5),\n",
    "            A.Rotate(limit=180, p=0.5),\n",
    "        ], p=0.7),\n",
    "        A.Flip(p=0.5),\n",
    "        A.RandomBrightnessContrast(brightness_limit=(-0.2, 0.2), contrast_limit=(-0.2, 0.2), p=0.7),\n",
    "        A.GaussNoise(var_limit=(10.0, 150.0), p=0.5),\n",
    "        A.GaussianBlur(blur_limit=(3, 15), p=0.5),\n",
    "        A.OneOf([\n",
    "            A.OpticalDistortion(distort_limit=0.1, shift_limit=0.1, p=1.0),\n",
    "            A.GridDistortion(num_steps=5, distort_limit=0.1, p=1.0),\n",
    "        ], p=0.5),\n",
    "        A.ImageCompression(quality_lower=50, quality_upper=100, p=0.5),\n",
    "        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ToTensorV2(),\n",
    "    ])\n",
    "\n",
    "def get_pred_transforms(height, width):\n",
    "    return A.Compose([\n",
    "        A.Resize(height=height, width=width),\n",
    "        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ToTensorV2(),\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stratified_split_dataset(train_csv_path, img_dir, trn_transform, tst_transform, augraphy_pipeline=None, train_size=0.7, val_size=0.15, test_size=0.15, random_state=42):\n",
    "    # CSV 파일 읽기\n",
    "    train_df = pd.read_csv(train_csv_path)\n",
    "    \n",
    "    # 첫 번째 split: 훈련 세트와 나머지(검증+테스트) 세트로 분할\n",
    "    train_df, temp_df = train_test_split(\n",
    "        train_df, \n",
    "        train_size=train_size,\n",
    "        stratify=train_df['target'],\n",
    "        random_state=random_state\n",
    "    )\n",
    "    \n",
    "    # 두 번째 split: 나머지를 검증 세트와 테스트 세트로 분할\n",
    "    val_size_adjusted = val_size / (val_size + test_size)\n",
    "    val_df, test_df = train_test_split(\n",
    "        temp_df, \n",
    "        train_size=val_size_adjusted, \n",
    "        stratify=temp_df['target'],\n",
    "        random_state=random_state\n",
    "    )\n",
    "    \n",
    "    print(f\"훈련 세트: {len(train_df)} 샘플\")\n",
    "    print(f\"검증 세트: {len(val_df)} 샘플\")\n",
    "    print(f\"테스트 세트: {len(test_df)} 샘플\")\n",
    "    \n",
    "    # 각 데이터프레임을 임시 CSV 파일로 저장\n",
    "    train_df.to_csv('temp_train.csv', index=False)\n",
    "    val_df.to_csv('temp_val.csv', index=False)\n",
    "    test_df.to_csv('temp_test.csv', index=False)\n",
    "    \n",
    "    # ImageDataset 생성\n",
    "    train_dataset = ImageDataset('temp_train.csv', img_dir, transform=trn_transform, augraphy_pipeline=augraphy_pipeline)\n",
    "    val_dataset = ImageDataset('temp_val.csv', img_dir, transform=tst_transform)\n",
    "    test_dataset = ImageDataset('temp_test.csv', img_dir, transform=tst_transform)\n",
    "    \n",
    "    # 임시 파일 삭제\n",
    "    os.remove('temp_train.csv')\n",
    "    os.remove('temp_val.csv')\n",
    "    os.remove('temp_test.csv')\n",
    "    \n",
    "    return train_dataset, val_dataset, test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1700315112808,
     "user": {
      "displayName": "Ynot(송원호)",
      "userId": "16271863862696372773"
     },
     "user_tz": -540
    },
    "id": "INxdmsStop2L",
    "outputId": "49f0d412-8ce6-4d2f-ae78-d5cf3d056340"
   },
   "outputs": [],
   "source": [
    "# Dataset 정의\n",
    "train_dataset, val_dataset, test_dataset = stratified_split_dataset(\n",
    "    'data/train.csv',\n",
    "    'data/train/',\n",
    "    trn_transform=get_train_transforms(img_size, img_size),\n",
    "    tst_transform=get_pred_transforms(img_size, img_size),\n",
    "    augraphy_pipeline=get_augraphy_pipeline()\n",
    ")\n",
    "\n",
    "pred_dataset = ImageDataset(\n",
    "    \"data/sample_submission.csv\",\n",
    "    \"data/test/\",\n",
    "    transform=get_pred_transforms(img_size, img_size)\n",
    ")\n",
    "\n",
    "print(len(train_dataset), len(val_dataset), len(test_dataset), len(pred_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1700315112808,
     "user": {
      "displayName": "Ynot(송원호)",
      "userId": "16271863862696372773"
     },
     "user_tz": -540
    },
    "id": "_sO03fWaQj1h"
   },
   "outputs": [],
   "source": [
    "# DataLoader 정의\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=num_workers,\n",
    "    pin_memory=True,\n",
    "    drop_last=False\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=num_workers,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=num_workers,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "pred_loader = DataLoader(\n",
    "    pred_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=0,\n",
    "    pin_memory=True\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "Nmm5h3J-pXNV"
   },
   "source": [
    "## Train Model\n",
    "* 모델을 로드하고, 학습을 진행합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model\n",
    "model = timm.create_model(\n",
    "    model_name,\n",
    "    pretrained=True,\n",
    "    num_classes=17\n",
    ").to(device)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = Adam(model.parameters(), lr=LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(EPOCHS):\n",
    "    ret = train_one_epoch(train_loader, model, optimizer, loss_fn, device=device)\n",
    "    ret['epoch'] = epoch\n",
    "\n",
    "    # wandb에 에폭 로깅\n",
    "    wandb.log({\"epoch\": epoch})\n",
    "\n",
    "    log = \"\"\n",
    "    for k, v in ret.items():\n",
    "      log += f\"{k}: {v:.4f}\\n\"\n",
    "    print(log)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "def evaluate(loader, model, loss_fn, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for image, targets in tqdm(loader, desc=\"Evaluating\"):\n",
    "            image = image.to(device)\n",
    "            targets = targets.to(device)\n",
    "\n",
    "            preds = model(image)\n",
    "            loss = loss_fn(preds, targets)\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            all_preds.extend(preds.argmax(dim=1).cpu().numpy())\n",
    "            all_targets.extend(targets.cpu().numpy())\n",
    "\n",
    "    avg_loss = total_loss / len(loader)\n",
    "    accuracy = accuracy_score(all_targets, all_preds)\n",
    "    f1 = f1_score(all_targets, all_preds, average='macro')\n",
    "\n",
    "    # wandb에 평가 메트릭 로깅\n",
    "    results = {\n",
    "        \"loss\": avg_loss,\n",
    "        \"accuracy\": accuracy,\n",
    "        \"f1\": f1\n",
    "    }\n",
    "    wandb.log(results)\n",
    "\n",
    "    return avg_loss, accuracy, f1\n",
    "\n",
    "# 학습 후 각 데이터셋에 대한 평가\n",
    "model.to(device)\n",
    "train_results = evaluate(train_loader, model, loss_fn, device)\n",
    "valid_results = evaluate(val_loader, model, loss_fn, device)\n",
    "test_results = evaluate(test_loader, model, loss_fn, device)\n",
    "\n",
    "# 평가 결과 로깅\n",
    "wandb.log({\n",
    "    \"final_train_loss\": train_results[0],\n",
    "    \"final_train_accuracy\": train_results[1],\n",
    "    \"final_train_f1\": train_results[2],\n",
    "    \"final_valid_loss\": valid_results[0],\n",
    "    \"final_valid_accuracy\": valid_results[1],\n",
    "    \"final_valid_f1\": valid_results[2],\n",
    "    \"final_test_loss\": test_results[0],\n",
    "    \"final_test_accuracy\": test_results[1],\n",
    "    \"final_test_f1\": test_results[2]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpret_results(train_results, valid_results, test_results):\n",
    "    \"\"\"\n",
    "    훈련, 검증, 테스트 결과를 해석하는 함수\n",
    "    \n",
    "    :param train_results: (train_loss, train_acc, train_f1)\n",
    "    :param valid_results: (valid_loss, valid_acc, valid_f1)\n",
    "    :param test_results: (test_loss, test_acc, test_f1)\n",
    "    :return: 해석 문자열\n",
    "    \"\"\"\n",
    "    train_loss, train_acc, train_f1 = train_results\n",
    "    valid_loss, valid_acc, valid_f1 = valid_results\n",
    "    test_loss, test_acc, test_f1 = test_results\n",
    "    \n",
    "    interpretation = \"모델 성능 해석:\\n\\n\"\n",
    "    \n",
    "    # 각 세트의 성능 출력\n",
    "    interpretation += f\"훈련 세트 - Loss: {train_loss:.4f}, Accuracy: {train_acc:.4f}, F1: {train_f1:.4f}\\n\"\n",
    "    interpretation += f\"검증 세트 - Loss: {valid_loss:.4f}, Accuracy: {valid_acc:.4f}, F1: {valid_f1:.4f}\\n\"\n",
    "    interpretation += f\"테스트 세트 - Loss: {test_loss:.4f}, Accuracy: {test_acc:.4f}, F1: {test_f1:.4f}\\n\\n\"\n",
    "    \n",
    "    # 과적합 여부 확인\n",
    "    if train_acc - valid_acc > 0.05 and train_acc - test_acc > 0.05:\n",
    "        interpretation += \"과적합 징후가 있습니다. 훈련 세트의 성능이 검증 및 테스트 세트보다 현저히 높습니다.\\n\"\n",
    "    elif valid_acc - test_acc > 0.05:\n",
    "        interpretation += \"검증 세트에 과적합되었을 가능성이 있습니다. 테스트 세트의 성능이 상대적으로 낮습니다.\\n\"\n",
    "    else:\n",
    "        interpretation += \"과적합의 징후가 크지 않습니다. 세 세트의 성능이 비교적 일관적입니다.\\n\"\n",
    "    \n",
    "    # 전반적인 성능 평가\n",
    "    avg_acc = (train_acc + valid_acc + test_acc) / 3\n",
    "    if avg_acc < 0.3:\n",
    "        interpretation += \"전반적인 성능이 낮습니다. 모델 개선이 필요합니다.\\n\"\n",
    "    elif avg_acc < 0.6:\n",
    "        interpretation += \"모델이 어느 정도의 학습을 보이지만, 상당한 개선의 여지가 있습니다.\\n\"\n",
    "    else:\n",
    "        interpretation += \"모델이 비교적 좋은 성능을 보이고 있습니다. 미세 조정을 통해 더 개선할 수 있습니다.\\n\"\n",
    "    \n",
    "    # F1 점수 해석\n",
    "    if min(train_f1, valid_f1, test_f1) < 0.3:\n",
    "        interpretation += \"F1 점수가 낮습니다. 클래스 불균형 문제를 고려해야 할 수 있습니다.\\n\"\n",
    "    \n",
    "    # 개선 제안\n",
    "    interpretation += \"\\n개선을 위한 제안:\\n\"\n",
    "    if train_acc - valid_acc > 0.05:\n",
    "        interpretation += \"- 정규화 기법 (예: dropout, L2 정규화)을 적용해 보세요.\\n\"\n",
    "        interpretation += \"- 데이터 증강 기법을 강화해 보세요.\\n\"\n",
    "    if avg_acc < 0.5:\n",
    "        interpretation += \"- 더 복잡한 모델 아키텍처를 시도해 보세요.\\n\"\n",
    "        interpretation += \"- 학습률과 배치 크기를 조정해 보세요.\\n\"\n",
    "        interpretation += \"- 전이 학습을 고려해 보세요.\\n\"\n",
    "    if min(train_f1, valid_f1, test_f1) < 0.3:\n",
    "        interpretation += \"- 클래스 가중치 조정을 통해 불균형 문제를 해결해 보세요.\\n\"\n",
    "        interpretation += \"- 앙상블 기법을 시도해 보세요.\\n\"\n",
    "    \n",
    "    return interpretation\n",
    "\n",
    "interpret = interpret_results(train_results, valid_results, test_results)\n",
    "print(interpret)\n",
    "wandb.log({\"interpretation\": interpret})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def evaluate_and_visualize_errors(model, dataloader, device, num_samples=10):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    error_images = []\n",
    "    error_preds = []\n",
    "    error_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(dataloader, desc=\"Evaluating\"):\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "            # 오류 식별\n",
    "            errors = preds != labels\n",
    "            error_images.extend(images[errors].cpu())\n",
    "            error_preds.extend(preds[errors].cpu().numpy())\n",
    "            error_labels.extend(labels[errors].cpu().numpy())\n",
    "\n",
    "    # 정확도 계산\n",
    "    accuracy = np.mean(np.array(all_preds) == np.array(all_labels))\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "    # 오류 시각화\n",
    "    num_samples = min(num_samples, len(error_images))\n",
    "    fig, axes = plt.subplots(1, num_samples, figsize=(20, 4))\n",
    "    for i in range(num_samples):\n",
    "        img = error_images[i].permute(1, 2, 0).numpy()\n",
    "        axes[i].imshow(img)\n",
    "        axes[i].set_title(f\"Pred: {error_preds[i]}, True: {error_labels[i]}\")\n",
    "        axes[i].axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # 혼동 행렬 생성 및 시각화\n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.show()\n",
    "\n",
    "    return error_images, error_preds, error_labels\n",
    "\n",
    "# 검증 세트에 대한 평가 및 오류 시각화\n",
    "print(\"Validation Set Errors:\")\n",
    "val_errors = evaluate_and_visualize_errors(model, val_loader, device)\n",
    "\n",
    "# 테스트 세트에 대한 평가 및 오류 시각화\n",
    "print(\"Test Set Errors:\")\n",
    "test_errors = evaluate_and_visualize_errors(model, test_loader, device)\n",
    "\n",
    "# 오류 분석\n",
    "def analyze_errors(error_preds, error_labels):\n",
    "    error_pairs = list(zip(error_preds, error_labels))\n",
    "    error_counts = {}\n",
    "    for pred, true in error_pairs:\n",
    "        key = f\"Pred: {pred}, True: {true}\"\n",
    "        error_counts[key] = error_counts.get(key, 0) + 1\n",
    "    \n",
    "    print(\"Most common errors:\")\n",
    "    for error, count in sorted(error_counts.items(), key=lambda x: x[1], reverse=True)[:5]:\n",
    "        print(f\"{error}: {count} times\")\n",
    "\n",
    "print(\"Validation Set Error Analysis:\")\n",
    "analyze_errors(val_errors[1], val_errors[2])\n",
    "\n",
    "print(\"Test Set Error Analysis:\")\n",
    "analyze_errors(test_errors[1], test_errors[2])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "lkwxRXoBpbaX"
   },
   "source": [
    "# Inference & Save File\n",
    "* 테스트 이미지에 대한 추론을 진행하고, 결과 파일을 저장합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if retrain_full_dataset:\n",
    "    print(\"Starting final training on entire dataset for submission...\")\n",
    "\n",
    "    # 전체 데이터셋 생성\n",
    "    full_dataset = ImageDataset(\n",
    "        \"data/train.csv\",\n",
    "        \"data/train/\",\n",
    "        transform=train_transform\n",
    "    )\n",
    "\n",
    "    full_loader = DataLoader(\n",
    "        full_dataset,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=True,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True,\n",
    "        drop_last=False\n",
    "    )\n",
    "\n",
    "    # 모델 재초기화\n",
    "    if reinitialize_model:\n",
    "        model = timm.create_model(model_name, pretrained=True, num_classes=17, drop_rate=0.3).to(device)\n",
    "        optimizer = Adam(model.parameters(), lr=LR)\n",
    "\n",
    "    # 전체 데이터셋으로 재학습\n",
    "    for epoch in range(EPOCHS):\n",
    "        ret = train_one_epoch(full_loader, model, optimizer, loss_fn, device=device)\n",
    "        print(f\"Epoch {epoch+1}/{EPOCHS}\")\n",
    "        print(f\"Loss: {ret['train_loss']:.4f}, Accuracy: {ret['train_acc']:.4f}, F1: {ret['train_f1']:.4f}\")\n",
    "\n",
    "    print(\"Final training completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 12776,
     "status": "ok",
     "timestamp": 1700315185336,
     "user": {
      "displayName": "Ynot(송원호)",
      "userId": "16271863862696372773"
     },
     "user_tz": -540
    },
    "id": "uRYe6jlPU_Om",
    "outputId": "2a08690c-9ffe-418d-8679-eb9280147110"
   },
   "outputs": [],
   "source": [
    "print(\"Generating predictions for submission...\")\n",
    "preds_list = []\n",
    "\n",
    "model.eval()\n",
    "for image, _ in tqdm(pred_loader):\n",
    "    image = image.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        preds = model(image)\n",
    "    preds_list.extend(preds.argmax(dim=1).detach().cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 282,
     "status": "ok",
     "timestamp": 1700315216829,
     "user": {
      "displayName": "Ynot(송원호)",
      "userId": "16271863862696372773"
     },
     "user_tz": -540
    },
    "id": "aClN7Qi7VZoh"
   },
   "outputs": [],
   "source": [
    "pred_df = pd.DataFrame(pred_dataset.data, columns=['ID', 'target'])\n",
    "pred_df['target'] = preds_list "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1700315238836,
     "user": {
      "displayName": "Ynot(송원호)",
      "userId": "16271863862696372773"
     },
     "user_tz": -540
    },
    "id": "VDBXQqAzVvLY"
   },
   "outputs": [],
   "source": [
    "sample_submission_df = pd.read_csv(\"data/sample_submission.csv\")\n",
    "assert (sample_submission_df['ID'] == pred_df['ID']).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 317,
     "status": "ok",
     "timestamp": 1700315244710,
     "user": {
      "displayName": "Ynot(송원호)",
      "userId": "16271863862696372773"
     },
     "user_tz": -540
    },
    "id": "ePx2vCELVnuS"
   },
   "outputs": [],
   "source": [
    "submission_file_path = os.path.join('output', f'{train_time}.csv')\n",
    "pred_df.to_csv(submission_file_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "executionInfo": {
     "elapsed": 353,
     "status": "ok",
     "timestamp": 1700315247734,
     "user": {
      "displayName": "Ynot(송원호)",
      "userId": "16271863862696372773"
     },
     "user_tz": -540
    },
    "id": "9yMO8s6GqAwZ",
    "outputId": "9a30616f-f0ea-439f-a906-dd806737ce00"
   },
   "outputs": [],
   "source": [
    "pred_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
