{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "OliaDaX_lwou"
   },
   "source": [
    "# **π“„ Document type classification baseline code**\n",
    "> λ¬Έμ„ νƒ€μ… λ¶„λ¥ λ€νμ— μ¤μ‹  μ—¬λ¬λ¶„ ν™μν•©λ‹λ‹¤! π‰     \n",
    "> μ•„λ baselineμ—μ„λ” ResNet λ¨λΈμ„ λ΅λ“ν•μ—¬, λ¨λΈμ„ ν•™μµ λ° μμΈ΅ νμΌ μƒμ„±ν•λ” ν”„λ΅μ„Έμ¤μ— λ€ν•΄ μ•μ•„λ³΄κ² μµλ‹λ‹¤.\n",
    "\n",
    "## Contents\n",
    "- Import Library & Define Functions\n",
    "- Hyper-parameters\n",
    "- Load Data\n",
    "- Train Model\n",
    "- Inference & Save File\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "PXa_FPM73R9f"
   },
   "source": [
    "## Import Library & Define Functions\n",
    "* ν•™μµ λ° μ¶”λ΅ μ— ν•„μ”ν• λΌμ΄λΈλ¬λ¦¬λ¥Ό λ΅λ“ν•©λ‹λ‹¤.\n",
    "* ν•™μµ λ° μ¶”λ΅ μ— ν•„μ”ν• ν•¨μμ™€ ν΄λμ¤λ¥Ό μ •μν•©λ‹λ‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 9396,
     "status": "ok",
     "timestamp": 1700314592802,
     "user": {
      "displayName": "Ynot(μ†΅μ›νΈ)",
      "userId": "16271863862696372773"
     },
     "user_tz": -540
    },
    "id": "3BaoIkv5Xwa0"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "\n",
    "import timm\n",
    "import torch\n",
    "import albumentations as A\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from torch.optim import Adam\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from datetime import datetime\n",
    "import time\n",
    "from zoneinfo import ZoneInfo\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_time = datetime.fromtimestamp(time.time(), tz=ZoneInfo(\"Asia/Seoul\")).strftime(\"%Y%m%d-%H%M%S\")\n",
    "train_time\n",
    "\n",
    "wandb.init(project=\"document-classification\", name=f\"run-{train_time}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# μ‹λ“λ¥Ό κ³ μ •ν•©λ‹λ‹¤.\n",
    "SEED = 42\n",
    "os.environ['PYTHONHASHSEED'] = str(SEED)\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 241,
     "status": "ok",
     "timestamp": 1700314772722,
     "user": {
      "displayName": "Ynot(μ†΅μ›νΈ)",
      "userId": "16271863862696372773"
     },
     "user_tz": -540
    },
    "id": "Hyl8oAy6TZAu"
   },
   "outputs": [],
   "source": [
    "# λ°μ΄ν„°μ…‹ ν΄λμ¤λ¥Ό μ •μν•©λ‹λ‹¤.\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, csv, path, transform=None):\n",
    "        self.df = pd.read_csv(csv).values\n",
    "        self.path = path\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        name, target = self.df[idx]\n",
    "        img = np.array(Image.open(os.path.join(self.path, name)))\n",
    "        if self.transform:\n",
    "            img = self.transform(image=img)['image']\n",
    "        return img, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 255,
     "status": "ok",
     "timestamp": 1700315066028,
     "user": {
      "displayName": "Ynot(μ†΅μ›νΈ)",
      "userId": "16271863862696372773"
     },
     "user_tz": -540
    },
    "id": "kTECBJfVTbdl"
   },
   "outputs": [],
   "source": [
    "# one epoch ν•™μµμ„ μ„ν• ν•¨μμ…λ‹λ‹¤.\n",
    "def train_one_epoch(loader, model, optimizer, loss_fn, device):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    preds_list = []\n",
    "    targets_list = []\n",
    "\n",
    "    pbar = tqdm(loader)\n",
    "    for image, targets in pbar:\n",
    "        image = image.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        model.zero_grad(set_to_none=True)\n",
    "\n",
    "        preds = model(image)\n",
    "        loss = loss_fn(preds, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        preds_list.extend(preds.argmax(dim=1).detach().cpu().numpy())\n",
    "        targets_list.extend(targets.detach().cpu().numpy())\n",
    "\n",
    "        pbar.set_description(f\"Loss: {loss.item():.4f}\")\n",
    "\n",
    "    train_loss /= len(loader)\n",
    "    train_acc = accuracy_score(targets_list, preds_list)\n",
    "    train_f1 = f1_score(targets_list, preds_list, average='macro')\n",
    "\n",
    "    ret = {\n",
    "        \"train_loss\": train_loss,\n",
    "        \"train_acc\": train_acc,\n",
    "        \"train_f1\": train_f1,\n",
    "    }\n",
    "\n",
    "    # wandbμ— ν›λ ¨ λ©”νΈλ¦­ λ΅κΉ…\n",
    "    wandb.log(ret)\n",
    "    \n",
    "    return ret"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "Wjom43UvoXcx"
   },
   "source": [
    "## Hyper-parameters\n",
    "* ν•™μµ λ° μ¶”λ΅ μ— ν•„μ”ν• ν•μ΄νΌνλΌλ―Έν„°λ“¤μ„ μ •μν•©λ‹λ‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 436,
     "status": "ok",
     "timestamp": 1700315112439,
     "user": {
      "displayName": "Ynot(μ†΅μ›νΈ)",
      "userId": "16271863862696372773"
     },
     "user_tz": -540
    },
    "id": "KByfAeRmXwYk"
   },
   "outputs": [],
   "source": [
    "# device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# data config\n",
    "data_path = 'data/'\n",
    "\n",
    "# model config\n",
    "model_name = 'resnet34' # 'resnet50' 'efficientnet-b0', ...\n",
    "\n",
    "# training config\n",
    "img_size = 32\n",
    "LR = 1e-3\n",
    "EPOCHS = 1\n",
    "BATCH_SIZE = 32\n",
    "num_workers = 0\n",
    "\n",
    "# μ„¤μ • λ΅κΉ…\n",
    "wandb.config.update({\n",
    "    \"model\": model_name,\n",
    "    \"img_size\": img_size,\n",
    "    \"learning_rate\": LR,\n",
    "    \"epochs\": EPOCHS,\n",
    "    \"batch_size\": BATCH_SIZE,\n",
    "})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "amum-FlIojc6"
   },
   "source": [
    "## Load Data\n",
    "* ν•™μµ, ν…μ¤νΈ λ°μ΄ν„°μ…‹κ³Ό λ΅λ”λ¥Ό μ •μν•©λ‹λ‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1700315112439,
     "user": {
      "displayName": "Ynot(μ†΅μ›νΈ)",
      "userId": "16271863862696372773"
     },
     "user_tz": -540
    },
    "id": "llh5C7ZKoq2S"
   },
   "outputs": [],
   "source": [
    "# augmentationμ„ μ„ν• transform μ½”λ“\n",
    "train_transform = A.Compose([\n",
    "    # μ΄λ―Έμ§€ ν¬κΈ° μ΅°μ •\n",
    "    A.Resize(height=img_size, width=img_size),\n",
    "    # images normalization\n",
    "    A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    # numpy μ΄λ―Έμ§€λ‚ PIL μ΄λ―Έμ§€λ¥Ό PyTorch ν…μ„λ΅ λ³€ν™\n",
    "    ToTensorV2(),\n",
    "])\n",
    "\n",
    "# test image λ³€ν™μ„ μ„ν• transform μ½”λ“\n",
    "pred_transform = A.Compose([\n",
    "    A.Resize(height=img_size, width=img_size),\n",
    "    A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ToTensorV2(),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_dataset_split(train_csv_path, img_dir, trn_transform, tst_transform, train_size=0.7, val_size=0.15, test_size=0.15, random_state=42):\n",
    "    # CSV νμΌ μ½κΈ°\n",
    "    train_df = pd.read_csv(train_csv_path)\n",
    "    \n",
    "    # μ²« λ²μ§Έ split: ν›λ ¨ μ„ΈνΈμ™€ λ‚λ¨Έμ§€(κ²€μ¦+ν…μ¤νΈ) μ„ΈνΈλ΅ λ¶„ν• \n",
    "    train_df, temp_df = train_test_split(\n",
    "        train_df, \n",
    "        train_size=train_size, \n",
    "        random_state=random_state\n",
    "    )\n",
    "    \n",
    "    # λ‘ λ²μ§Έ split: λ‚λ¨Έμ§€λ¥Ό κ²€μ¦ μ„ΈνΈμ™€ ν…μ¤νΈ μ„ΈνΈλ΅ λ¶„ν• \n",
    "    val_size_adjusted = val_size / (val_size + test_size)\n",
    "    val_df, test_df = train_test_split(\n",
    "        temp_df, \n",
    "        train_size=val_size_adjusted, \n",
    "        random_state=random_state\n",
    "    )\n",
    "    \n",
    "    print(f\"ν›λ ¨ μ„ΈνΈ: {len(train_df)} μƒν”\")\n",
    "    print(f\"κ²€μ¦ μ„ΈνΈ: {len(val_df)} μƒν”\")\n",
    "    print(f\"ν…μ¤νΈ μ„ΈνΈ: {len(test_df)} μƒν”\")\n",
    "    \n",
    "    # κ° λ°μ΄ν„°ν”„λ μ„μ„ μ„μ‹ CSV νμΌλ΅ μ €μ¥\n",
    "    train_df.to_csv('temp_train.csv', index=False)\n",
    "    val_df.to_csv('temp_val.csv', index=False)\n",
    "    test_df.to_csv('temp_test.csv', index=False)\n",
    "    \n",
    "    # ImageDataset μƒμ„±\n",
    "    train_dataset = ImageDataset('temp_train.csv', img_dir, transform=trn_transform)\n",
    "    val_dataset = ImageDataset('temp_val.csv', img_dir, transform=tst_transform)\n",
    "    test_dataset = ImageDataset('temp_test.csv', img_dir, transform=tst_transform)\n",
    "    \n",
    "    # μ„μ‹ νμΌ μ‚­μ \n",
    "    os.remove('temp_train.csv')\n",
    "    os.remove('temp_val.csv')\n",
    "    os.remove('temp_test.csv')\n",
    "    \n",
    "    return train_dataset, val_dataset, test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1700315112808,
     "user": {
      "displayName": "Ynot(μ†΅μ›νΈ)",
      "userId": "16271863862696372773"
     },
     "user_tz": -540
    },
    "id": "INxdmsStop2L",
    "outputId": "49f0d412-8ce6-4d2f-ae78-d5cf3d056340"
   },
   "outputs": [],
   "source": [
    "# Dataset μ •μ\n",
    "train_dataset, val_dataset, test_dataset = random_dataset_split(\n",
    "    'data/train.csv',\n",
    "    'data/train/',\n",
    "    train_transform,\n",
    "    pred_transform\n",
    ")\n",
    "\n",
    "pred_dataset = ImageDataset(\n",
    "    \"data/sample_submission.csv\",\n",
    "    \"data/test/\",\n",
    "    transform=pred_transform\n",
    ")\n",
    "\n",
    "print(len(train_dataset), len(val_dataset), len(test_dataset), len(pred_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1700315112808,
     "user": {
      "displayName": "Ynot(μ†΅μ›νΈ)",
      "userId": "16271863862696372773"
     },
     "user_tz": -540
    },
    "id": "_sO03fWaQj1h"
   },
   "outputs": [],
   "source": [
    "# DataLoader μ •μ\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=num_workers,\n",
    "    pin_memory=True,\n",
    "    drop_last=False\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=num_workers,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=num_workers,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "pred_loader = DataLoader(\n",
    "    pred_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=0,\n",
    "    pin_memory=True\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "Nmm5h3J-pXNV"
   },
   "source": [
    "## Train Model\n",
    "* λ¨λΈμ„ λ΅λ“ν•κ³ , ν•™μµμ„ μ§„ν–‰ν•©λ‹λ‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model\n",
    "model = timm.create_model(\n",
    "    model_name,\n",
    "    pretrained=True,\n",
    "    num_classes=17\n",
    ").to(device)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = Adam(model.parameters(), lr=LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(EPOCHS):\n",
    "    ret = train_one_epoch(train_loader, model, optimizer, loss_fn, device=device)\n",
    "    ret['epoch'] = epoch\n",
    "\n",
    "    # wandbμ— μ—ν­ λ΅κΉ…\n",
    "    wandb.log({\"epoch\": epoch})\n",
    "\n",
    "    log = \"\"\n",
    "    for k, v in ret.items():\n",
    "      log += f\"{k}: {v:.4f}\\n\"\n",
    "    print(log)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ν‰κ°€"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "def evaluate(loader, model, loss_fn, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for image, targets in tqdm(loader, desc=\"Evaluating\"):\n",
    "            image = image.to(device)\n",
    "            targets = targets.to(device)\n",
    "\n",
    "            preds = model(image)\n",
    "            loss = loss_fn(preds, targets)\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            all_preds.extend(preds.argmax(dim=1).cpu().numpy())\n",
    "            all_targets.extend(targets.cpu().numpy())\n",
    "\n",
    "    avg_loss = total_loss / len(loader)\n",
    "    accuracy = accuracy_score(all_targets, all_preds)\n",
    "    f1 = f1_score(all_targets, all_preds, average='macro')\n",
    "\n",
    "    # wandbμ— ν‰κ°€ λ©”νΈλ¦­ λ΅κΉ…\n",
    "    results = {\n",
    "        \"loss\": avg_loss,\n",
    "        \"accuracy\": accuracy,\n",
    "        \"f1\": f1\n",
    "    }\n",
    "    wandb.log(results)\n",
    "\n",
    "    return avg_loss, accuracy, f1\n",
    "\n",
    "# ν•™μµ ν›„ κ° λ°μ΄ν„°μ…‹μ— λ€ν• ν‰κ°€\n",
    "model.to(device)\n",
    "train_results = evaluate(train_loader, model, loss_fn, device)\n",
    "valid_results = evaluate(val_loader, model, loss_fn, device)\n",
    "test_results = evaluate(test_loader, model, loss_fn, device)\n",
    "\n",
    "# ν‰κ°€ κ²°κ³Ό λ΅κΉ…\n",
    "wandb.log({\n",
    "    \"final_train_loss\": train_results[0],\n",
    "    \"final_train_accuracy\": train_results[1],\n",
    "    \"final_train_f1\": train_results[2],\n",
    "    \"final_valid_loss\": valid_results[0],\n",
    "    \"final_valid_accuracy\": valid_results[1],\n",
    "    \"final_valid_f1\": valid_results[2],\n",
    "    \"final_test_loss\": test_results[0],\n",
    "    \"final_test_accuracy\": test_results[1],\n",
    "    \"final_test_f1\": test_results[2]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpret_results(train_results, valid_results, test_results):\n",
    "    \"\"\"\n",
    "    ν›λ ¨, κ²€μ¦, ν…μ¤νΈ κ²°κ³Όλ¥Ό ν•΄μ„ν•λ” ν•¨μ\n",
    "    \n",
    "    :param train_results: (train_loss, train_acc, train_f1)\n",
    "    :param valid_results: (valid_loss, valid_acc, valid_f1)\n",
    "    :param test_results: (test_loss, test_acc, test_f1)\n",
    "    :return: ν•΄μ„ λ¬Έμμ—΄\n",
    "    \"\"\"\n",
    "    train_loss, train_acc, train_f1 = train_results\n",
    "    valid_loss, valid_acc, valid_f1 = valid_results\n",
    "    test_loss, test_acc, test_f1 = test_results\n",
    "    \n",
    "    interpretation = \"λ¨λΈ μ„±λ¥ ν•΄μ„:\\n\\n\"\n",
    "    \n",
    "    # κ° μ„ΈνΈμ μ„±λ¥ μ¶λ ¥\n",
    "    interpretation += f\"ν›λ ¨ μ„ΈνΈ - Loss: {train_loss:.4f}, Accuracy: {train_acc:.4f}, F1: {train_f1:.4f}\\n\"\n",
    "    interpretation += f\"κ²€μ¦ μ„ΈνΈ - Loss: {valid_loss:.4f}, Accuracy: {valid_acc:.4f}, F1: {valid_f1:.4f}\\n\"\n",
    "    interpretation += f\"ν…μ¤νΈ μ„ΈνΈ - Loss: {test_loss:.4f}, Accuracy: {test_acc:.4f}, F1: {test_f1:.4f}\\n\\n\"\n",
    "    \n",
    "    # κ³Όμ ν•© μ—¬λ¶€ ν™•μΈ\n",
    "    if train_acc - valid_acc > 0.05 and train_acc - test_acc > 0.05:\n",
    "        interpretation += \"κ³Όμ ν•© μ§•ν›„κ°€ μμµλ‹λ‹¤. ν›λ ¨ μ„ΈνΈμ μ„±λ¥μ΄ κ²€μ¦ λ° ν…μ¤νΈ μ„ΈνΈλ³΄λ‹¤ ν„μ €ν λ†’μµλ‹λ‹¤.\\n\"\n",
    "    elif valid_acc - test_acc > 0.05:\n",
    "        interpretation += \"κ²€μ¦ μ„ΈνΈμ— κ³Όμ ν•©λμ—μ„ κ°€λ¥μ„±μ΄ μμµλ‹λ‹¤. ν…μ¤νΈ μ„ΈνΈμ μ„±λ¥μ΄ μƒλ€μ μΌλ΅ λ‚®μµλ‹λ‹¤.\\n\"\n",
    "    else:\n",
    "        interpretation += \"κ³Όμ ν•©μ μ§•ν›„κ°€ ν¬μ§€ μ•μµλ‹λ‹¤. μ„Έ μ„ΈνΈμ μ„±λ¥μ΄ λΉ„κµμ  μΌκ΄€μ μ…λ‹λ‹¤.\\n\"\n",
    "    \n",
    "    # μ „λ°μ μΈ μ„±λ¥ ν‰κ°€\n",
    "    avg_acc = (train_acc + valid_acc + test_acc) / 3\n",
    "    if avg_acc < 0.3:\n",
    "        interpretation += \"μ „λ°μ μΈ μ„±λ¥μ΄ λ‚®μµλ‹λ‹¤. λ¨λΈ κ°μ„ μ΄ ν•„μ”ν•©λ‹λ‹¤.\\n\"\n",
    "    elif avg_acc < 0.6:\n",
    "        interpretation += \"λ¨λΈμ΄ μ–΄λ μ •λ„μ ν•™μµμ„ λ³΄μ΄μ§€λ§, μƒλ‹Ήν• κ°μ„ μ μ—¬μ§€κ°€ μμµλ‹λ‹¤.\\n\"\n",
    "    else:\n",
    "        interpretation += \"λ¨λΈμ΄ λΉ„κµμ  μΆ‹μ€ μ„±λ¥μ„ λ³΄μ΄κ³  μμµλ‹λ‹¤. λ―Έμ„Έ μ΅°μ •μ„ ν†µν•΄ λ” κ°μ„ ν•  μ μμµλ‹λ‹¤.\\n\"\n",
    "    \n",
    "    # F1 μ μ ν•΄μ„\n",
    "    if min(train_f1, valid_f1, test_f1) < 0.3:\n",
    "        interpretation += \"F1 μ μκ°€ λ‚®μµλ‹λ‹¤. ν΄λμ¤ λ¶κ· ν• λ¬Έμ λ¥Ό κ³ λ ¤ν•΄μ•Ό ν•  μ μμµλ‹λ‹¤.\\n\"\n",
    "    \n",
    "    # κ°μ„  μ μ•\n",
    "    interpretation += \"\\nκ°μ„ μ„ μ„ν• μ μ•:\\n\"\n",
    "    if train_acc - valid_acc > 0.05:\n",
    "        interpretation += \"- μ •κ·ν™” κΈ°λ²• (μ: dropout, L2 μ •κ·ν™”)μ„ μ μ©ν•΄ λ³΄μ„Έμ”.\\n\"\n",
    "        interpretation += \"- λ°μ΄ν„° μ¦κ°• κΈ°λ²•μ„ κ°•ν™”ν•΄ λ³΄μ„Έμ”.\\n\"\n",
    "    if avg_acc < 0.5:\n",
    "        interpretation += \"- λ” λ³µμ΅ν• λ¨λΈ μ•„ν‚¤ν…μ²λ¥Ό μ‹λ„ν•΄ λ³΄μ„Έμ”.\\n\"\n",
    "        interpretation += \"- ν•™μµλ¥ κ³Ό λ°°μΉ ν¬κΈ°λ¥Ό μ΅°μ •ν•΄ λ³΄μ„Έμ”.\\n\"\n",
    "        interpretation += \"- μ „μ΄ ν•™μµμ„ κ³ λ ¤ν•΄ λ³΄μ„Έμ”.\\n\"\n",
    "    if min(train_f1, valid_f1, test_f1) < 0.3:\n",
    "        interpretation += \"- ν΄λμ¤ κ°€μ¤‘μΉ μ΅°μ •μ„ ν†µν•΄ λ¶κ· ν• λ¬Έμ λ¥Ό ν•΄κ²°ν•΄ λ³΄μ„Έμ”.\\n\"\n",
    "        interpretation += \"- μ•™μƒλΈ” κΈ°λ²•μ„ μ‹λ„ν•΄ λ³΄μ„Έμ”.\\n\"\n",
    "    \n",
    "    return interpretation\n",
    "\n",
    "interpret = interpret_results(train_results, valid_results, test_results)\n",
    "print(interpret)\n",
    "wandb.log({\"interpretation\": interpret})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "lkwxRXoBpbaX"
   },
   "source": [
    "# Inference & Save File\n",
    "* ν…μ¤νΈ μ΄λ―Έμ§€μ— λ€ν• μ¶”λ΅ μ„ μ§„ν–‰ν•κ³ , κ²°κ³Ό νμΌμ„ μ €μ¥ν•©λ‹λ‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 12776,
     "status": "ok",
     "timestamp": 1700315185336,
     "user": {
      "displayName": "Ynot(μ†΅μ›νΈ)",
      "userId": "16271863862696372773"
     },
     "user_tz": -540
    },
    "id": "uRYe6jlPU_Om",
    "outputId": "2a08690c-9ffe-418d-8679-eb9280147110"
   },
   "outputs": [],
   "source": [
    "preds_list = []\n",
    "\n",
    "model.eval()\n",
    "for image, _ in tqdm(pred_loader):\n",
    "    image = image.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        preds = model(image)\n",
    "    preds_list.extend(preds.argmax(dim=1).detach().cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 282,
     "status": "ok",
     "timestamp": 1700315216829,
     "user": {
      "displayName": "Ynot(μ†΅μ›νΈ)",
      "userId": "16271863862696372773"
     },
     "user_tz": -540
    },
    "id": "aClN7Qi7VZoh"
   },
   "outputs": [],
   "source": [
    "pred_df = pd.DataFrame(pred_dataset.df, columns=['ID', 'target'])\n",
    "pred_df['target'] = preds_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1700315238836,
     "user": {
      "displayName": "Ynot(μ†΅μ›νΈ)",
      "userId": "16271863862696372773"
     },
     "user_tz": -540
    },
    "id": "VDBXQqAzVvLY"
   },
   "outputs": [],
   "source": [
    "sample_submission_df = pd.read_csv(\"data/sample_submission.csv\")\n",
    "assert (sample_submission_df['ID'] == pred_df['ID']).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 317,
     "status": "ok",
     "timestamp": 1700315244710,
     "user": {
      "displayName": "Ynot(μ†΅μ›νΈ)",
      "userId": "16271863862696372773"
     },
     "user_tz": -540
    },
    "id": "ePx2vCELVnuS"
   },
   "outputs": [],
   "source": [
    "submission_file_path = os.path.join('output', f'{train_time}.csv')\n",
    "pred_df.to_csv(submission_file_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "executionInfo": {
     "elapsed": 353,
     "status": "ok",
     "timestamp": 1700315247734,
     "user": {
      "displayName": "Ynot(μ†΅μ›νΈ)",
      "userId": "16271863862696372773"
     },
     "user_tz": -540
    },
    "id": "9yMO8s6GqAwZ",
    "outputId": "9a30616f-f0ea-439f-a906-dd806737ce00"
   },
   "outputs": [],
   "source": [
    "pred_df.head()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
