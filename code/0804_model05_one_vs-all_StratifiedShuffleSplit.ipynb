{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import torch\n",
    "import albumentations as A\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "import random\n",
    "\n",
    "import wandb\n",
    "\n",
    "import timm\n",
    "from dotenv import load_dotenv\n",
    "from datetime import datetime\n",
    "from zoneinfo import ZoneInfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 사용할 수 있는 모델 목록 확인\n",
    "model_names = timm.list_models()\n",
    "print(model_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "1. **Lightweight MobileNets and GhostNets**:\n",
    "   - `mobilenetv2_035`, `mobilenetv2_050`, `mobilenetv2_075`, `mobilenetv2_100`, `mobilenetv3_small_050`, `mobilenetv3_small_075`, `mobilenetv3_small_100`, `ghostnet_050`, `ghostnet_100`, `ghostnetv2_100`, `ghostnetv2_130`, `ghostnetv2_160`\n",
    "\n",
    "2. **EfficientNets (Lite models)**:\n",
    "   - `efficientnet_lite0`, `efficientnet_lite1`, `efficientnet_lite2`, `efficientnet_lite3`, `efficientnet_lite4`\n",
    "\n",
    "3. **Tiny and Small ViT Models**:\n",
    "   - `vit_tiny_patch16_224`, `vit_tiny_patch16_384`, `vit_small_patch16_224`, `vit_small_patch16_384`\n",
    "\n",
    "4. **Small EfficientNets and MobileViTs**:\n",
    "   - `efficientnet_b0`, `efficientnet_b1`, `efficientnet_b2`, `efficientnet_b3`, `efficientnetv2_rw_s`, `mobilevit_xs`, `mobilevit_s`\n",
    "\n",
    "5. **CoaT Models (Lite variants)**:\n",
    "   - `coat_lite_tiny`, `coat_lite_mini`, `coat_lite_small`, `coat_lite_medium`\n",
    "\n",
    "6. **Cait and ResNet Variants**:\n",
    "   - `cait_xxs24_224`, `cait_xxs24_384`, `resnet18`, `resnet34`\n",
    "\n",
    "7. **MixNet and EfficientNets (Base models)**:\n",
    "   - `mixnet_s`, `mixnet_m`, `mixnet_l`, `efficientnet_b4`, `efficientnet_b5`\n",
    "\n",
    "8. **DLA Models**:\n",
    "   - `dla34`, `dla46_c`, `dla46x_c`, `dla60`, `dla60_res2net`, `dla60_res2next`, `dla60x`, `dla60x_c`\n",
    "\n",
    "9. **RegNet and RexNet Models**:\n",
    "   - `regnetx_002`, `regnetx_004`, `regnetx_006`, `rexnet_100`, `rexnet_130`, `rexnetr_100`, `rexnetr_130`\n",
    "\n",
    "10. **ViT Base and Larger EfficientNets**:\n",
    "    - `vit_base_patch16_224`, `vit_base_patch16_384`, `efficientnet_b6`, `efficientnet_b7`\n",
    "\n",
    "11. **Larger and Deeper Networks**:\n",
    "    - `resnet50`, `resnet101`, `resnext50_32x4d`, `resnext101_32x4d`, `densenet121`, `densenet169`\n",
    "\n",
    "12. **Heavier and State-of-the-Art Models**:\n",
    "    - `beit_large_patch16_224`, `beit_large_patch16_384`, `beit_large_patch16_512`, `swin_large_patch4_window12_384`, `swinv2_base_window12to24_192to384`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 시드를 고정합니다.\n",
    "SEED = 42\n",
    "os.environ['PYTHONHASHSEED'] = str(SEED)\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습에 사용할 장치를 설정합니다.\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# 데이터 설정\n",
    "data_path = '../data/'\n",
    "\n",
    "# 모델 설정\n",
    "model_name = 'efficientnet_b5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 설정\n",
    "img_size = 256  \n",
    "LR = 1e-5\n",
    "EPOCHS = 1\n",
    "BATCH_SIZE = 4\n",
    "num_workers = 4  # 일단 0으로 설정하여 멀티프로세싱 비활성화\n",
    "early_stopping_patience = 5  # Early Stopping 설정\n",
    "augment_ratio = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # wandb 연동\n",
    "# load_dotenv()\n",
    "# api_key = os.getenv('WANDB_API_KEY')\n",
    "\n",
    "# wandb.login(key=api_key)\n",
    "\n",
    "# train_time = datetime.fromtimestamp(time.time(), tz=ZoneInfo(\"Asia/Seoul\")).strftime(\"%Y%m%d-%H%M%S\")\n",
    "# wandb.init(project=\"document_classification-cv\", name=f\"one_vs_all-{train_time}\")\n",
    "\n",
    "# print(train_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wandb 연동\n",
    "load_dotenv()\n",
    "api_key = os.getenv('WANDB_API_KEY')\n",
    "\n",
    "wandb.login(key=api_key)\n",
    "\n",
    "train_time = datetime.fromtimestamp(time.time(), tz=ZoneInfo(\"Asia/Seoul\")).strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "# wandb 프로젝트 초기화\n",
    "# wandb.init(\n",
    "#     project=\"document_classification-cv\",\n",
    "#     name=f\"one_vs_all-{train_time}\"\n",
    "#     config={\n",
    "#         \"learning_rate\": LR,\n",
    "#         \"architecture\": model_name,\n",
    "#         \"dataset\": \"custom-dataset\",\n",
    "#         \"epochs\": EPOCHS,\n",
    "#         \"batch_size\": BATCH_SIZE,\n",
    "#         \"image_size\": img_size,\n",
    "#         \"num_workers\" : num_workers,\n",
    "#         'augment_ratio' : augment_ratio\n",
    "#     }\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 변환 설정\n",
    "trn_transform = A.Compose([\n",
    "    A.Resize(height=img_size, width=img_size),\n",
    "    A.OneOf([\n",
    "        A.GaussNoise(var_limit=(10.0, 800.0), p=0.75),\n",
    "        A.GaussianBlur(blur_limit=(1, 7), p=0.5)\n",
    "    ], p=0.75),\n",
    "    A.RandomRotate90(p=0.5),\n",
    "    A.HorizontalFlip(p=0.75),\n",
    "    A.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2, p=0.5),\n",
    "    A.ShiftScaleRotate(shift_limit=0.1, scale_limit=0.1, rotate_limit=30, p=0.25),\n",
    "    A.CoarseDropout(max_holes=6, max_height=32, max_width=32, p=0.5),\n",
    "    A.ElasticTransform(alpha=1, sigma=30, alpha_affine=30, p=0.5),\n",
    "    A.HueSaturationValue(hue_shift_limit=20, sat_shift_limit=30, val_shift_limit=20, p=0.5),\n",
    "    A.RandomBrightnessContrast(brightness_limit=0.2, contrast_limit=0.2, p=0.5),\n",
    "    A.Rotate(limit=30, p=0.75),\n",
    "    A.CLAHE(clip_limit=2.0, tile_grid_size=(8, 8), p=0.5),\n",
    "    A.MotionBlur(blur_limit=5, p=0.5),\n",
    "    A.OpticalDistortion(p=0.5),\n",
    "    A.Transpose(p=0.5),\n",
    "    A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ToTensorV2(),\n",
    "])\n",
    "\n",
    "tst_transform = A.Compose([\n",
    "    A.Resize(height=img_size, width=img_size),\n",
    "    A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ToTensorV2(),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset 정의\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, df, path, transform=None):\n",
    "        self.df = df.values\n",
    "        self.path = path\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        name, target = self.df[idx]\n",
    "        img_path = os.path.join(self.path, name)\n",
    "\n",
    "        try:\n",
    "            img = np.array(Image.open(img_path))\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading image {img_path}: {e}\")\n",
    "            img = np.zeros((img_size, img_size, 3), dtype=np.uint8)  # 빈 이미지로 대체\n",
    "\n",
    "        if self.transform:\n",
    "            img = self.transform(image=img)['image']\n",
    "        return img, target\n",
    "\n",
    "# 데이터 로드\n",
    "train_df = pd.read_csv(f\"{data_path}train.csv\")\n",
    "\n",
    "# 데이터 증강\n",
    "augmented_data = []\n",
    "for _, row in tqdm(train_df.iterrows(), total=len(train_df), desc=\"Augmenting data\"):\n",
    "    img_path = os.path.join(f\"{data_path}train/\", row['ID'])\n",
    "    img = np.array(Image.open(img_path))\n",
    "    \n",
    "    for _ in range(augment_ratio):\n",
    "        augmented_img = trn_transform(image=img)['image']\n",
    "        augmented_data.append((row['ID'], row['target']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 증강된 데이터를 원본 데이터와 합침\n",
    "augmented_df = pd.DataFrame(augmented_data, columns=['ID', 'target'])\n",
    "full_train_df = pd.concat([train_df, augmented_df], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# StratifiedShuffleSplit을 사용하여 데이터 분할\n",
    "sss = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 단일 분할 수행\n",
    "for train_index, val_index in sss.split(full_train_df, full_train_df['target']):\n",
    "    train_df_split = full_train_df.iloc[train_index]\n",
    "    val_df = full_train_df.iloc[val_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset 및 DataLoader 생성\n",
    "train_dataset = ImageDataset(train_df_split, f\"{data_path}train/\", transform=trn_transform)\n",
    "val_dataset = ImageDataset(val_df, f\"{data_path}train/\", transform=tst_transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=num_workers, pin_memory=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=num_workers, pin_memory=True)\n",
    "\n",
    "# 테스트 데이터셋 생성\n",
    "tst_dataset = ImageDataset(\n",
    "    pd.read_csv(f\"{data_path}sample_submission.csv\"),\n",
    "    f\"{data_path}test/\",\n",
    "    transform=tst_transform\n",
    ")\n",
    "\n",
    "tst_loader = DataLoader(\n",
    "    tst_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=num_workers,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "# 클래스 비율 확인\n",
    "print(\"Train set class distribution:\")\n",
    "print(train_df_split['target'].value_counts(normalize=True))\n",
    "print(\"\\nValidation set class distribution:\")\n",
    "print(val_df['target'].value_counts(normalize=True))\n",
    "\n",
    "# 데이터 수 출력\n",
    "print(f\"Original training data count: {len(train_df)}\")\n",
    "print(f\"Augmented training data count: {len(train_df_split)}\")\n",
    "print(f\"Validation data count: {len(val_df)}\")\n",
    "print(f\"Test data count: {len(tst_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # 시각화 함수\n",
    "# def visualize_augmented_images(dataset, num_images=5, augmentations_per_image=4, data_gap=1570):\n",
    "#     fig, axes = plt.subplots(num_images, augmentations_per_image + 1, figsize=(20, num_images * 4))\n",
    "    \n",
    "#     for i in range(num_images):\n",
    "#         # 원본 이미지 인덱스\n",
    "#         original_img_index = random.randint(0,ori_traindata_num)\n",
    "        \n",
    "#         # 원본 이미지 로드\n",
    "#         original_img_path = os.path.join(dataset.path, dataset.df[original_img_index][0])\n",
    "#         original_img = np.array(Image.open(original_img_path))\n",
    "        \n",
    "#         # 원본 이미지 표시\n",
    "#         axes[i, 0].imshow(original_img)\n",
    "#         axes[i, 0].set_title(\"Original Image\")\n",
    "#         axes[i, 0].axis(\"off\")\n",
    "        \n",
    "#         # 증강된 이미지 최대 4개 표시\n",
    "#         for j in range(1, augmentations_per_image + 1):\n",
    "#             # 증강된 이미지를 올바르게 가져오기 위해, 증강 비율을 사용하여 인덱스를 계산\n",
    "#             augmented_idx = original_img_index + data_gap * (j - 1)\n",
    "#             augmented_img, _ = dataset[augmented_idx]\n",
    "#             augmented_img = augmented_img.permute(1, 2, 0).numpy()  # CHW -> HWC\n",
    "            \n",
    "#             # Normalize 된 이미지를 다시 원래 범위로 변환\n",
    "#             mean = np.array([0.485, 0.456, 0.406])\n",
    "#             std = np.array([0.229, 0.224, 0.225])\n",
    "#             augmented_img = std * augmented_img + mean\n",
    "#             augmented_img = np.clip(augmented_img, 0, 1)\n",
    "            \n",
    "#             axes[i, j].imshow(augmented_img)\n",
    "#             axes[i, j].set_title(f\"Augmented Image {j}\")\n",
    "#             axes[i, j].axis(\"off\")\n",
    "    \n",
    "#     plt.tight_layout()\n",
    "#     plt.show()\n",
    "\n",
    "# # 트레이닝 데이터 시각화\n",
    "# visualize_augmented_images(trn_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# trn_loader = DataLoader(\n",
    "#     trn_dataset,\n",
    "#     batch_size=BATCH_SIZE,\n",
    "#     shuffle=True,\n",
    "#     num_workers=num_workers,\n",
    "#     pin_memory=True,\n",
    "#     drop_last=False\n",
    "# )\n",
    "\n",
    "# tst_loader = DataLoader(\n",
    "#     tst_dataset,\n",
    "#     batch_size=BATCH_SIZE,\n",
    "#     shuffle=False,\n",
    "#     num_workers=num_workers,\n",
    "#     pin_memory=True\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # 모델을 불러옵니다.\n",
    "# model = timm.create_model(\n",
    "#     model_name,\n",
    "#     pretrained=True,\n",
    "#     num_classes=17,\n",
    "#     drop_rate=0.2  # Dropout 설정\n",
    "# ).to(device)\n",
    "\n",
    "# # 손실 함수를 정의합니다.\n",
    "# loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# # 옵티마이저를 정의합니다.\n",
    "# optimizer = AdamW(model.parameters(), lr=LR)\n",
    "\n",
    "# # Learning Rate Scheduler를 정의합니다.\n",
    "# scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=3, verbose=True)\n",
    "\n",
    "# # Early Stopping을 위한 변수 초기화\n",
    "# best_loss = float('inf')\n",
    "# early_stopping_counter = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_for_class(class_num):\n",
    "    wandb.init(\n",
    "        project=f\"{train_time}_one_vs_all\",\n",
    "        name=f\"{train_time}_class_{class_num}\",\n",
    "        config={\n",
    "            \"learning_rate\": LR,\n",
    "            \"architecture\": model_name,\n",
    "            \"dataset\": \"custom-dataset\",\n",
    "            \"epochs\": EPOCHS,\n",
    "            \"batch_size\": BATCH_SIZE,\n",
    "            \"image_size\": img_size,\n",
    "            \"num_workers\": num_workers,\n",
    "            'augment_ratio': augment_ratio,\n",
    "            \"class_num\": class_num\n",
    "        }\n",
    "    )\n",
    "\n",
    "    model = timm.create_model(model_name, pretrained=True, num_classes=2, drop_rate=0.2).to(device)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    optimizer = AdamW(model.parameters(), lr=LR)\n",
    "    scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=3, verbose=True)\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    early_stopping_counter = 0\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        # 학습\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        train_preds, train_targets = [], []\n",
    "\n",
    "        for image, targets in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{EPOCHS} - Training\"):\n",
    "            image = image.to(device)\n",
    "            targets = (targets == class_num).long().to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            preds = model(image)\n",
    "            loss = loss_fn(preds, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            train_preds.extend(preds.argmax(dim=1).cpu().numpy())\n",
    "            train_targets.extend(targets.cpu().numpy())\n",
    "\n",
    "        train_loss /= len(train_loader)\n",
    "        train_acc = accuracy_score(train_targets, train_preds)\n",
    "        train_f1 = f1_score(train_targets, train_preds, average='binary')\n",
    "\n",
    "        # 검증\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        val_preds, val_targets = [], []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for image, targets in tqdm(val_loader, desc=f\"Epoch {epoch+1}/{EPOCHS} - Validation\"):\n",
    "                image = image.to(device)\n",
    "                targets = (targets == class_num).long().to(device)\n",
    "\n",
    "                preds = model(image)\n",
    "                loss = loss_fn(preds, targets)\n",
    "\n",
    "                val_loss += loss.item()\n",
    "                val_preds.extend(preds.argmax(dim=1).cpu().numpy())\n",
    "                val_targets.extend(targets.cpu().numpy())\n",
    "\n",
    "        val_loss /= len(val_loader)\n",
    "        val_acc = accuracy_score(val_targets, val_preds)\n",
    "        val_f1 = f1_score(val_targets, val_preds, average='binary')\n",
    "\n",
    "        print(f\"Class {class_num} - Epoch {epoch+1}/{EPOCHS}\")\n",
    "        print(f\"Train Loss: {train_loss:.4f}, Acc: {train_acc:.4f}, F1: {train_f1:.4f}\")\n",
    "        print(f\"Val Loss: {val_loss:.4f}, Acc: {val_acc:.4f}, F1: {val_f1:.4f}\")\n",
    "\n",
    "        wandb.log({\n",
    "            \"epoch\": epoch + 1,\n",
    "            \"train_loss\": train_loss,\n",
    "            \"train_accuracy\": train_acc,\n",
    "            \"train_f1_score\": train_f1,\n",
    "            \"val_loss\": val_loss,\n",
    "            \"val_accuracy\": val_acc,\n",
    "            \"val_f1_score\": val_f1\n",
    "        })\n",
    "\n",
    "        scheduler.step(val_loss)\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            early_stopping_counter = 0\n",
    "            torch.save(model.state_dict(), f\"{model_name}_class{class_num}_best_model.pth\")\n",
    "        else:\n",
    "            early_stopping_counter += 1\n",
    "            if early_stopping_counter >= early_stopping_patience:\n",
    "                print(f\"Early stopping triggered for class {class_num} after {epoch+1} epochs.\")\n",
    "                break\n",
    "\n",
    "    wandb.finish()\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모든 클래스에 대해 모델 학습\n",
    "models = []\n",
    "for class_num in range(17):\n",
    "    print(f\"Training model for class {class_num}\")\n",
    "    model = train_model_for_class(class_num)\n",
    "    models.append(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 앙상블 예측 함수\n",
    "def ensemble_predict(models, loader):\n",
    "    predictions = []\n",
    "    for image, _ in tqdm(loader):\n",
    "        image = image.to(device)\n",
    "        class_predictions = []\n",
    "        for model in models:\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                preds = model(image)\n",
    "                class_predictions.append(preds[:, 1].cpu().numpy())  # 클래스에 속할 확률\n",
    "        class_predictions = np.array(class_predictions).T\n",
    "        predictions.extend(np.argmax(class_predictions, axis=1))\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 테스트 데이터에 대한 앙상블 예측 수행\n",
    "ensemble_preds = ensemble_predict(models, tst_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 예측 결과를 데이터프레임으로 저장\n",
    "pred_df = pd.DataFrame(tst_dataset.df, columns=['ID', 'target'])\n",
    "pred_df['target'] = ensemble_preds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 제출 형식 파일을 읽어와 ID 열이 일치하는지 확인\n",
    "sample_submission_df = pd.read_csv(f\"{data_path}sample_submission.csv\")\n",
    "assert (sample_submission_df['ID'] == pred_df['ID']).all()\n",
    "\n",
    "# 예측 결과를 CSV 파일로 저장\n",
    "pred_df.to_csv(f\"{model_name}_ensemble_pred.csv\", index=False)\n",
    "print(pred_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
